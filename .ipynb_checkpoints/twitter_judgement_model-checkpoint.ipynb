{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 極性判定とDoc２Vecを使ったTwitterネガポジ予測\n",
    "＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝\n",
    "### 【このnotebookについて】\n",
    "2019年7〜10月までフルタイムで通っていたスクールの卒業課題テーマを、機械学習の勉強のために発展させたものです<br>\n",
    "卒業発表スライド　https://www.slideshare.net/secret/y0m7g1nZdxpVYP<br>\n",
    "＊当初は炎上予測がテーマだったので、このnotebookの内容とはややズレます<br>\n",
    "＊表紙スライドの字が見えない場合は２枚目から戻ると見えます<br>\n",
    "\n",
    "＊ちなみに…<br>\n",
    "スクールで取り組んだ課題のリポジトリ \n",
    "https://github.com/kaorisugi/diveintocode-ml<br>\n",
    "論文読解課題のスライドシェア \n",
    "https://www.slideshare.net/secret/qGmdiwl4uGS20O<br>\n",
    "\n",
    "### 【ゴール】\n",
    "これからツイートする予定の文章に対し、過去の類似ツイートを探し、反応のネガポジスコア付きで上位１０位まで提示する。<br>\n",
    "### 【モデルの仕組み】\n",
    "１）ツイートデータセットを取得<br>\n",
    "　・TwitterAPIを使ってツイートを取得<br>\n",
    "　・各ツイートに対する反応ツイート（リプライ、引用RT）を取得<br>\n",
    "　・反応ツイートの極性表現数をカウントしてネガポジスコアとpositive/negative/fire!!!判定を得る<br>\n",
    " 　（positive/negativeの判定基準：極性表現数が多い方、fire!!!(炎上）の判定基準：極性表現の７０％以上がnegative）\n",
    "２）データセットの前処理<br>\n",
    "　・正規表現、ストップワード除去など<br>\n",
    "３）予測モデルを生成<br>\n",
    "　・データセットをDoc２vecで学習<br>\n",
    "４）ツイート予定文章のネガポジ予測を返す<br>\n",
    "　・データセットから、ツイート予定文書と似ている文書を探す<br>\n",
    " ・ネガポジスコア付きで、類似ツイート上位１０個を返す\n",
    "### 【結果】\n",
    "類似度確認用にデータセット内にあるものと同じ文を入力したところ、類似度1位で返ってきた。また、２位、３位にもマスクに関する似た話題のツイートが提示されたので類似ツイートの抽出は成功。ネガポジスコアもデータズレなどなく正確に表示され、目的は達成できた。<br>\n",
    "ツイッターAPI制限により、まだサンプルが少ない（完成時２００件程度）が、データを蓄積できる仕様にしているので、ツイート文のバリエーションを増やしていけば、様々な入力文に対応できるようになると思う。<br>\n",
    "ネガポジ判定については、ネガティブなテーマへの言及に共感したコメントでネガ判定が出ているケースも多く、必ずしもツイート主へのネガ感情ではないことに注意が必要。<br>\n",
    "\n",
    "### 【その他試みたこと】\n",
    "１）文章ベクトルを特徴量としたネガポジ予測モデル<br>\n",
    "　・文章ベクトルとフォロワー数を特徴量X、ネガポジスコアを目的変数yとしたデータを学習<br>\n",
    "　・文章ベクトルはDoc２vecとTf-idfの２種を作成<br>\n",
    "　・ツイート予定文書を入力してネガポジスコアを予測する<br>\n",
    "　・試した予測モデル<br>\n",
    "　・MultiOutputRegressor、SVRのrbf と　SVRの線形、lightgbm、ランダムフォレスト<br>\n",
    "　  　→精度が低すぎて断念<br>\n",
    "２）ツイッターAPI制限への挑戦（データセットの拡大）<br>\n",
    "　・古いツイートを大量取得できるパッケージを発見（通常は１週間程度しか遡れない）<br>\n",
    "　　　→取得データから反応ツイートの取得を試みたができなかった<br>\n",
    "   \n",
    "### 【利用するには】\n",
    "・config.py ファイルにツイッターAPIトークンを記入<br>\n",
    "・インストールが必要なツールは、notebook内にマジックコマンドにて記載してあります<br>\n",
    "・import MeCab で詰まる場合は、MeCabバインディングのnatto-pyからimportするとうまくいきそうです。<br>\n",
    "（$ pip install natto-pyでインストールした上でfrom natto import MeCab) <br>\n",
    "＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## １）ツイートデータセットを取得\n",
    "・TwitterAPIでツイートを取得<br>\n",
    "・各ツイートに対するリプライ、引用RTを取得<br>\n",
    "・極性表現数をカウントしてネガポジスコアを得る<br>\n",
    "\n",
    "#### 参考サイト\n",
    "【Python】tweepyでTwitterのツイートを検索して取得<br>\n",
    "https://vatchlog.com/tweepy-search/<br>\n",
    "【Python】tweepyで期間指定してツイートを検索する<br>\n",
    "https://vatchlog.com/tweepy-search-time/<br>\n",
    "バズったツイートへのリアクションを感情分析してみる<br>【Google Natural Language API / Python】<br>\n",
    "https://qiita.com/matsuri0828/items/029b4d0d510dcfb5c5dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /opt/conda/lib/python3.6/site-packages (19.3.1)\n",
      "Requirement already satisfied: tweepy in /opt/conda/lib/python3.6/site-packages (3.8.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in /opt/conda/lib/python3.6/site-packages (from tweepy) (2.18.4)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /opt/conda/lib/python3.6/site-packages (from tweepy) (1.6.7)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tweepy) (1.11.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.11.1->tweepy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.11.1->tweepy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.11.1->tweepy) (2018.1.18)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: oseti in /opt/conda/lib/python3.6/site-packages (0.2)\n",
      "Requirement already satisfied: sengiri in /opt/conda/lib/python3.6/site-packages (from oseti) (0.2.1)\n",
      "Requirement already satisfied: mecab-python3 in /opt/conda/lib/python3.6/site-packages (from oseti) (0.7)\n",
      "Requirement already satisfied: neologdn in /opt/conda/lib/python3.6/site-packages (from oseti) (0.2.1)\n",
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.6/site-packages (from sengiri->oseti) (0.5.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (2.18.4)\n",
      "Requirement already satisfied: requests_oauthlib in /opt/conda/lib/python3.6/site-packages (1.3.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests) (2018.1.18)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from requests_oauthlib) (3.1.0)\n",
      "Requirement already satisfied: sengiri in /opt/conda/lib/python3.6/site-packages (0.2.1)\n",
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.6/site-packages (from sengiri) (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "#必要なツールをインストール（初回のみ実行）\n",
    "! pip install --upgrade pip\n",
    "! pip install tweepy\n",
    "! pip install oseti\n",
    "! pip install requests requests_oauthlib\n",
    "! pip install sengiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import re\n",
    "import emoji\n",
    "import oseti\n",
    "from datetime import datetime, date, timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import config\n",
    "\n",
    "class Get_Twitter():\n",
    "\n",
    "    def __init__(self, day, reload, print_rep = False, exclud_words = \"配信スタート ＃キャンペーン　リツイートキャンペーン\", RT_count = 5000):\n",
    "        self.oseti_analyzer = oseti.Analyzer()  #極性判定\n",
    "        self.CK = config.CONSUMER_KEY\n",
    "        self.CS = config.CONSUMER_SECRET\n",
    "        self.AT = config.ACCESS_TOKEN\n",
    "        self.AS = config.ACCESS_TOKEN_SECRET\n",
    "        self.ew = exclud_words\n",
    "        self.print_rep = print_rep\n",
    "        self.rt = str(RT_count)\n",
    "        self.columns = [\n",
    "            \"Id\", \"Date\", \"Name\", \"Full_text\",\n",
    "            \"Judge\", \"Posi_score\", \"Nega_score\", \"Followers\", \"link\"\n",
    "        ]\n",
    "        self.posi_pd = pd.DataFrame([], columns = self.columns)\n",
    "        self.nega_pd = pd.DataFrame([], columns = self.columns)\n",
    "        self.fire_pd = pd.DataFrame([], columns = self.columns)\n",
    "        self.wait = 0\n",
    "        self.reload = reload\n",
    "        day = datetime.strptime(day, '%Y-%m-%d')\n",
    "        self.day = day.strftime('%Y-%m-%d')\n",
    "\n",
    "    def main(self):\n",
    "        self._Make_Dir() # データ格納ファイルの準備\n",
    "\n",
    "        #ツイートを取得、センチメント判定\n",
    "        try:\n",
    "            status = self.Get_Buzz() #バズったツイート取得\n",
    "            for i in status:            \n",
    "                if self.wait == 10:\n",
    "                    print(\"10回待機したため終了\")\n",
    "                    break\n",
    "                self.Status(i)\n",
    "                if self.Exclude_Word(self.buzz_full_text) == True:# 除外ワードを含むツイートは除外\n",
    "                    continue\n",
    "                if self.Text_Count() == True: #30W以下のツイートは除外\n",
    "                    continue\n",
    "                self.Get_Rep() #リプライを取得\n",
    "                self.Get_RT() #RTコメントを取得\n",
    "                if self.Min_Rep() == False: # コメントが少ないツイートは除外\n",
    "                    continue\n",
    "                self.Get_Senti() #コメントをセンチメント判定\n",
    "                self._Get_Analysis() #ツイートをセンチメント判定\n",
    "        #エラー時はスキップして次のツイート取得\n",
    "        except (ValueError,  KeyError, TypeError, tweepy.TweepError) as e:\n",
    "            pass\n",
    "        #リクエスト回数が上限に達した場合はリセット時間まで待機して継続\n",
    "        except tweepy.RateLimitError as e:\n",
    "            if self.reload:\n",
    "                self.wait += 1\n",
    "                print(\"==========\")\n",
    "                print('get_buzzのリクエスト回数が上限に達しました。リセット時間まで待機')\n",
    "                print('Wait 15min...')\n",
    "                print()\n",
    "                for _ in tqdm(range(15 * 60)):\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        #生成したデータをprint\n",
    "        print()\n",
    "        print(\"↓↓↓positiveサンプル↓↓↓\")\n",
    "        display(self.posi_pd.head())\n",
    "        print()            \n",
    "        print(\"↓↓↓negativeサンプル↓↓↓\")\n",
    "        display(self.nega_pd.head())\n",
    "        print()\n",
    "        print(\"↓↓↓fire_tweetサンプル↓↓↓\")\n",
    "        display(self.fire_pd.head())\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        #生成したPandasDataFrameをcsvで書き出す\n",
    "        total_pd = pd.concat([self.posi_pd, self.nega_pd, self.fire_pd], ignore_index=True)\n",
    "        buzz_old = pd.read_csv('./output/buzz_tweet.csv')\n",
    "        buzz_new = pd.concat([buzz_old, total_pd])#既存データと連結\n",
    "        buzz_new.drop_duplicates(subset=\"Id\",inplace=True)#重複ID行を削除            \n",
    "        buzz_new.to_csv('./output/buzz_tweet.csv', index = False, header = True)\n",
    "        print(\"csvへの書き出しが完了しました。新規データ数{}、全データ数：{}\".format(len(buzz_new) - len(buzz_old), len(buzz_new)))\n",
    "        print(\"サンプルが0件の場合は、15分後に再度実行すると取得できる場合があります。\") \n",
    "        print(\"fire_tweetは出現率が非常に低いです。\")\n",
    "\n",
    "    #Api認証\n",
    "    def _Auth(self):\n",
    "        auth = tweepy.OAuthHandler(self.CK, self.CS)\n",
    "        auth.set_access_token(self.AT, self.AS)\n",
    "        api = tweepy.API(auth)\n",
    "        return api\n",
    "\n",
    "    #出力用ディレクトリとcsvファイルを作成（存在しない場合のみ）\n",
    "    def _Make_Dir(self):\n",
    "        new_dir_path = 'output'\n",
    "        try:\n",
    "            os.makedirs(new_dir_path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        if (os.path.isfile('./output/buzz_tweet.csv')) == False:\n",
    "            self.posi_pd.to_csv('./output/buzz_tweet.csv', index = False)  \n",
    "\n",
    "    #絵文字削除\n",
    "    def _remove_emoji(self, text):\n",
    "        return ''.join(c for c in text if c not in emoji.UNICODE_EMOJI)\n",
    "\n",
    "    #テキストを正規表現処理、絵文字削除\n",
    "    def _format_text(self, text):\n",
    "        text=re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", text)\n",
    "        text=re.sub('\\n', \"\", text)\n",
    "        text=re.sub(r'@?[!-~]+', \"\", text)\n",
    "        text=self._remove_emoji(text)\n",
    "        return text\n",
    "    \n",
    "    #　日付表記を整える、日本時間に修正\n",
    "    def _date_format(self, date):\n",
    "        date = datetime.strptime(str(date), '%a %b %d %H:%M:%S %z %Y')\n",
    "        date = date + timedelta(hours=9)\n",
    "        return datetime.strftime(date, '%Y-%m-%d %H:%M')\n",
    "\n",
    "    def Status(self, status): \n",
    "        self.buzz_id = status._json['id']\n",
    "        self.buzz_id_str = status._json['id_str']\n",
    "        self.buzz_name = status._json['user']['screen_name']\n",
    "        self.buzz_full_text = status._json['full_text']\n",
    "        self.date = status._json['created_at']\n",
    "        self.date = self._date_format(self.date)\n",
    "        self.favo = status._json['favorite_count']\n",
    "        self.rt_count = status._json['retweet_count']\n",
    "        api = self._Auth()\n",
    "        self.followers = status._json['user']['followers_count']\n",
    "        #self.followers = len(api.followers(status._json['user']['screen_name']))\n",
    "    \n",
    "    #除外ワード\n",
    "    def Exclude_Word(self, text):                        \n",
    "        if self.ew in str(text):\n",
    "            print(\"==========\")\n",
    "            print(\"除外ワード\")\n",
    "            print()\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    #ツイート内にリンクがあれば分割\n",
    "    def Text_Count(self):\n",
    "        if re.search(\"(https://t.co/\\w+)\", self.buzz_full_text) == None:\n",
    "            self.link = None\n",
    "        else:                   \n",
    "            self.buzz_full_text = re.split(\"(https://t.co/\\w+)\", self.buzz_full_text)\n",
    "            self.link = self.buzz_full_text[1]\n",
    "            self.buzz_full_text = self.buzz_full_text[0]\n",
    "        if len(self.buzz_full_text) < 30:\n",
    "            return True\n",
    "\n",
    "    #リプライ＋引用RTコメントが100未満のツイートは除外\n",
    "    def Min_Rep(self):\n",
    "        reply_texts_rows = []\n",
    "        if self.rep_cnt + self.RTcomme_cnt > 100:\n",
    "            reply_texts_rows.append(self.rep_row)\n",
    "            reply_texts_rows.append(self.rt_row)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    #sentiment_listを一次元にし、ツイートごとの極性表現の総和の辞書にする\n",
    "    def Get_Senti(self):\n",
    "        self.sentiment_list = sum(self.sentiment_list, [])#１次元にする\n",
    "        self.sentiment = dict((key, sum(d[key] for d in self.sentiment_list)) for key in self.sentiment_list[0])\n",
    "\n",
    "    #バズったツイートを取得(デフォルト：5000RT以上)\n",
    "    def Get_Buzz(self):\n",
    "        api = self._Auth()\n",
    "        try:       \n",
    "            status = api.search(q = 'filter:safe min_retweets:' + self.rt + ' exclude:retweets until:' + self.day,\n",
    "                lang ='ja', count =100, tweet_mode = 'extended', result_type = 'recent')\n",
    "            return status\n",
    "        #エラー時はスキップして次のツイート取得\n",
    "        except (ValueError,  KeyError) as e:\n",
    "            pass\n",
    "        #リクエスト回数が上限に達した場合はリセット時間まで待機して継続\n",
    "        except (tweepy.RateLimitError, tweepy.TweepError) as e:\n",
    "            if self.reload:\n",
    "                self.wait += 1\n",
    "                print(\"==========\")\n",
    "                print('get_buzzのリクエスト回数が上限に達しました。リセット時間まで待機')\n",
    "                print('Wait 15min...')\n",
    "                print()\n",
    "                for _ in tqdm(range(15 * 60)):\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                pass\n",
    "        #return status\n",
    "    \n",
    "    #リプライを取得\n",
    "    def Get_Rep(self):\n",
    "        api = self._Auth()     \n",
    "        query_reply = '@' + self.buzz_name + ' exclude:retweets'\n",
    "        self.rep_row = []\n",
    "        self.sentiment_list = []\n",
    "        self.rep_cnt =0\n",
    "        wait_cnt = 0\n",
    "        try:\n",
    "            for status_reply in api.search(q=query_reply, lang='ja', count=100):\n",
    "                if status_reply._json['in_reply_to_status_id_str'] == self.buzz_id_str:\n",
    "                    row = self._format_text(status_reply._json['text'])\n",
    "                    #極性判定\n",
    "                    sentiment_score = self.oseti_analyzer.count_polarity(str(row))#strにする\n",
    "                    self.sentiment_list.append(sentiment_score)\n",
    "                    self.rep_row.append(row)\n",
    "                    self.rep_cnt += 1\n",
    "                else:\n",
    "                    pass\n",
    "        #エラーはスキップして次のツイート取得\n",
    "        except (ValueError,  KeyError, tweepy.TweepError) as e:\n",
    "            pass\n",
    "        #リクエスト回数が上限に達した場合はリセット時間まで待機して継続\n",
    "        except tweepy.RateLimitError as e:\n",
    "            self.wait += 1\n",
    "            if self.reload:\n",
    "                print(\"==========\")\n",
    "                print('get_repのリクエスト回数が上限に達しました。リセット時間まで待機')\n",
    "                print('Wait 15min...')\n",
    "                print()\n",
    "                for _ in tqdm(range(15 * 60)):\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    # 引用RTを取得\n",
    "    def Get_RT(self):\n",
    "        api = self._Auth()\n",
    "        query_quote = self.buzz_id_str + ' exclude:retweets'\n",
    "        self.RTcomme_cnt = 0\n",
    "        self.rt_row = []\n",
    "        try:\n",
    "            for status_quote in api.search(q=query_quote, lang='ja', count=100):\n",
    "                if status_quote._json['id_str'] == self.buzz_id_str:\n",
    "                    continue\n",
    "                else:\n",
    "                    row = self._format_text(status_quote._json['text'])\n",
    "                #極性判定\n",
    "                sentiment_score = self.oseti_analyzer.count_polarity(str(row))#strにする\n",
    "                self.sentiment_list.append(sentiment_score)\n",
    "                self.rt_row.append(row)\n",
    "                self.RTcomme_cnt += 1\n",
    "        #エラーはスキップして次のツイート取得\n",
    "        except (ValueError,  KeyError, tweepy.TweepError) as e:\n",
    "            pass\n",
    "        #リクエスト回数が上限に達した場合はリセット時間まで待機して継続\n",
    "        except tweepy.RateLimitError as e:\n",
    "            self.wait += 1\n",
    "            if self.reload:\n",
    "                print(\"==========\")\n",
    "                print('get_rtのリクエスト回数が上限に達しました。リセット時間まで待機')\n",
    "                print('Wait 15min...')\n",
    "                print()\n",
    "                for _ in tqdm(range(15 * 60)):\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                pass        \n",
    "\n",
    "    #取得したTweetをprint\n",
    "    def _Print(self):\n",
    "        print(\"name：\", self.buzz_name, \"／フォロワー数：\", self.followers)\n",
    "        print(\"date：\", self.date, \"／ツイートID：\", self.buzz_id_str)\n",
    "        print(\"RT数：\", self.rt_count, \"／favorite数：\", self.favo)\n",
    "        print(\"リプライ数：\", self.rep_cnt, \"／RTコメント数(上限１００）：\", self.RTcomme_cnt)\n",
    "        if self.print_rep == True:\n",
    "            print(\"リプライ\\n\", self.rep_row)\n",
    "            print(\"RTコメント\\n\", self.rt_row)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    #センチメント判定結果を取得\n",
    "    def _Get_Analysis(self):\n",
    "        total = self.sentiment[\"positive\"] + self.sentiment[\"negative\"]\n",
    "        if self.sentiment[\"positive\"] >= self.sentiment[\"negative\"]:\n",
    "            print(\"==========\")\n",
    "            print(self.buzz_full_text)\n",
    "            print()\n",
    "            print(\"【判定:positive】　　極性表現数\", self.sentiment)\n",
    "            self._Print()\n",
    "            s = pd.Series([self.buzz_id, self.date, self.buzz_name, self.buzz_full_text, \"positive\", self.sentiment[\"positive\"], self.sentiment[\"negative\"], self.followers, self.link], index = self.columns)\n",
    "            self.posi_pd = self.posi_pd.append(s, ignore_index=True)\n",
    "        elif self.sentiment[\"negative\"]/total >= 0.7:\n",
    "            print(\"==========\")\n",
    "            print(self.buzz_full_text)\n",
    "            print()\n",
    "            print(\"【判定:fire!!!】　　極性表現数\", self.sentiment)\n",
    "            print(\"ネガ表現の割合{:.3g}\".format(self.sentiment[\"negative\"]/total))\n",
    "            self._Print()\n",
    "            s = pd.Series([self.buzz_id, self.date, self.buzz_name, self.buzz_full_text, \"fire\", self.sentiment[\"positive\"], self.sentiment[\"negative\"], self.followers, self.link], index = self.columns)\n",
    "            self.fire_pd = self.fire_pd.append(s, ignore_index=True)\n",
    "        else:\n",
    "            print(\"==========\")\n",
    "            print(self.buzz_full_text)\n",
    "            print()\n",
    "            print(\"【判定:negative】　　極性表現数\", self.sentiment)\n",
    "            self._Print()\n",
    "            s = pd.Series([self.buzz_id, self.date, self.buzz_name, self.buzz_full_text, \"negative\", self.sentiment[\"positive\"], self.sentiment[\"negative\"], self.followers, self.link], index = self.columns)\n",
    "            self.nega_pd = self.nega_pd.append(s, ignore_index=True)\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ツイートデータセット取得　実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "【鬼滅の刃コラボ中！】\n",
      "ローソン国際展示場駅前店では1日限定で「鬼滅の刃」コラボを実施中です！\n",
      "\n",
      "キャラクタースタンディやポスターの展示、また入店音が炭治郎・禰豆子・善逸・伊之助のボイス（ランダム）となっておりますのでぜひチェックしてください！\n",
      "\n",
      "#鬼滅の刃 \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 20, 'negative': 18}\n",
      "name： kimetsu_off ／フォロワー数： 961791\n",
      "date： 2019-12-28 08:56 ／ツイートID： 1210710961771859968\n",
      "RT数： 6942 ／favorite数： 38985\n",
      "リプライ数： 2 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "どなたかが呟かれてましたけど、成人のADHDでは「何か思いつくと、それをタスクの一番最後に追加するのではなく、現在行っているタスクの一番上に常に置いてしまい、それに注意を占有されてしまうことから、その結果スケジュールが崩壊して死ぬ」という状態になる人が多い気がしますな。\n",
      "\n",
      "【判定:negative】　　極性表現数 {'positive': 46, 'negative': 55}\n",
      "name： noooooooorth ／フォロワー数： 15670\n",
      "date： 2019-12-28 08:45 ／ツイートID： 1210708419159609344\n",
      "RT数： 5253 ／favorite数： 18970\n",
      "リプライ数： 19 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "除夜の鐘が煩い、お祭りが煩い、花火が煩い、風鈴が煩い、園児が煩い…\n",
      "少数の煩いクレームにどんどん静かにさせられる寂しい日本。\n",
      "次の煩いは蝉の鳴き声？鳥の声？人の声かな… \n",
      "\n",
      "【判定:negative】　　極性表現数 {'positive': 92, 'negative': 135}\n",
      "name： takeshi_tsuruno ／フォロワー数： 615857\n",
      "date： 2019-12-28 08:43 ／ツイートID： 1210707713971212288\n",
      "RT数： 6682 ／favorite数： 18275\n",
      "リプライ数： 75 ／RTコメント数(上限１００）： 51\n",
      "\n",
      "==========\n",
      "若月健矢選手、立花理香さん\n",
      "\n",
      "ご結婚おめでとうございます！\n",
      "末長くお幸せに✨\n",
      "\n",
      "バファローズ㊗️ポンタ\n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 71, 'negative': 3}\n",
      "name： bs_ponta ／フォロワー数： 288636\n",
      "date： 2019-12-28 08:34 ／ツイートID： 1210705650491113472\n",
      "RT数： 6955 ／favorite数： 13325\n",
      "リプライ数： 59 ／RTコメント数(上限１００）： 52\n",
      "\n",
      "==========\n",
      "【ご報告】この度、みなさまにご報告したいことができました。ぜひご覧いただけますと幸いです。 \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 145, 'negative': 19}\n",
      "name： RiccaTachibana ／フォロワー数： 175344\n",
      "date： 2019-12-28 08:22 ／ツイートID： 1210702646509588480\n",
      "RT数： 45084 ／favorite数： 83417\n",
      "リプライ数： 84 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "【#本日12月28日は竈門禰豆子の誕生日!!】\n",
      "本日は、鬼でありながら鬼殺隊に所属する\n",
      "炭治郎自慢の妹・竈門禰豆子の誕生日です！\n",
      "\n",
      "この特別な日を祝して、\n",
      "禰豆子の魅力が詰まったヘッダーをプレゼント！\n",
      "\n",
      "人を守るために鬼の力を使う\n",
      "禰豆子のヘッダー、ぜひご活用ください！ \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 82, 'negative': 10}\n",
      "name： kimetsu_off ／フォロワー数： 961791\n",
      "date： 2019-12-28 08:00 ／ツイートID： 1210696997839110144\n",
      "RT数： 38622 ／favorite数： 129417\n",
      "リプライ数： 22 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "【注意喚起！】寝ている人から財布等を抜き取る犯罪が発生しているという連絡が警察からありました。待機列などで、皆さん声を掛け合って、注意してください。残念ながらコミケでは、4会期連続でスリの現行犯逮捕が起きています。お宝資金はしっかりと管理を。#C97\n",
      "\n",
      "【判定:negative】　　極性表現数 {'positive': 59, 'negative': 111}\n",
      "name： comiketofficial ／フォロワー数： 221597\n",
      "date： 2019-12-28 07:27 ／ツイートID： 1210688744660975616\n",
      "RT数： 10010 ／favorite数： 7296\n",
      "リプライ数： 5 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "「鬼滅の刃」グッズプレゼント🎉\n",
      "\n",
      "大好評のため第2段\n",
      "シリーズ累計2500万部記念\n",
      "鬼滅の刃 全18巻セット\n",
      "を10名様にプレゼント🎁\n",
      "\n",
      "応募方法\n",
      "・フォロー&amp;リツィート\n",
      "・「応募」とリプ\n",
      "\n",
      "応募期間\n",
      "・12月31日\n",
      "\n",
      "当選した方にはDMにてお知らせ！ \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 23, 'negative': 14}\n",
      "name： kimetsu_goods ／フォロワー数： 14631\n",
      "date： 2019-12-28 07:10 ／ツイートID： 1210684401157206016\n",
      "RT数： 7443 ／favorite数： 4607\n",
      "リプライ数： 99 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "アニメーター始めてから初めてキャベツを描いた、妙なプレッシャーがあった\n",
      "#炎炎ノ消防隊 \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 77, 'negative': 38}\n",
      "name： varon666 ／フォロワー数： 3581\n",
      "date： 2019-12-28 02:49 ／ツイートID： 1210618724517990400\n",
      "RT数： 19202 ／favorite数： 57937\n",
      "リプライ数： 72 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "🔥“弐ノ章”制作＆2020年夏放送決定🔥\n",
      "\n",
      "壱ノ章の続編となるTVアニメ『炎炎ノ消防隊 弐ノ章』の制作が決定しました！\n",
      "\n",
      "さらに“弐ノ章ティザービジュアル”も発表！\n",
      "\n",
      "2020年も炎炎の炎はますます燃え続けます🔥🔥\n",
      "\n",
      "『炎炎ノ消防隊 弐ノ章』ご期待ください！！\n",
      "\n",
      "#fireforce #炎炎ノ消防隊 #弐ノ章 \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 98, 'negative': 33}\n",
      "name： FireForce_PR ／フォロワー数： 79950\n",
      "date： 2019-12-28 02:07 ／ツイートID： 1210608126434500608\n",
      "RT数： 8290 ／favorite数： 19035\n",
      "リプライ数： 58 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "【祝】12月28日は「竈門禰豆子の誕生日」\n",
      "\n",
      "『鬼滅の刃』の登場キャラクターで、主人公・竈門炭治郎の妹。物語開始時12歳→14歳。家族と慎ましくも幸せな生活を送っていたが、突然の惨劇により鬼に変貌してしまう。人を喰らう鬼としての衝動を抑え込みながら、炭治郎や仲間のために行動する。 \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 76, 'negative': 11}\n",
      "name： livedoornews ／フォロワー数： 992978\n",
      "date： 2019-12-28 00:15 ／ツイートID： 1210579841025789952\n",
      "RT数： 6916 ／favorite数： 29036\n",
      "リプライ数： 4 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "コミケ会場で「プライスカードを忘れた😭」となった場合、落ち着いて机の上にあるチラシ類を見てみて下さい。\n",
      "全てのサークルさんの机の上に、プライスカードを置いてきましたから😋\n",
      "\n",
      "＊不要でしたらごめんなさい \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 141, 'negative': 41}\n",
      "name： ShimayaTokyo ／フォロワー数： 7788\n",
      "date： 2019-12-27 22:54 ／ツイートID： 1210559641823793152\n",
      "RT数： 17337 ／favorite数： 23301\n",
      "リプライ数： 55 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "チュートリアルでお世話になったキャラが実は最強の裏ボスっていうゲーム \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 59, 'negative': 34}\n",
      "name： black_sabasu ／フォロワー数： 86499\n",
      "date： 2019-12-27 22:26 ／ツイートID： 1210552542595178498\n",
      "RT数： 6773 ／favorite数： 27010\n",
      "リプライ数： 64 ／RTコメント数(上限１００）： 47\n",
      "\n",
      "==========\n",
      "Mステ #ウルトラSUPERLIVE \n",
      "11時間生放送中🎤\n",
      "\n",
      "女々しくての10周年のお祝いに駆けつけてくれたのはなんと\n",
      "#ボボボーボ・ボーボボ\n",
      "でした‼️\n",
      "\n",
      "#ボボボーボ\n",
      "#ボーボボ\n",
      "#ウルトラタモリ\n",
      "#Mステ \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 32, 'negative': 22}\n",
      "name： Mst_com ／フォロワー数： 1247765\n",
      "date： 2019-12-27 22:12 ／ツイートID： 1210549021443379200\n",
      "RT数： 12384 ／favorite数： 20286\n",
      "リプライ数： 4 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "クソワロ\n",
      "\n",
      "女々しくて発売10年で人気キャラが駆けつける・・・\n",
      "\n",
      "ボボボーボ・ボーボボｗｗｗｗｗｗｗｗ\n",
      "\n",
      "#Mステ\n",
      "#金爆 \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 31, 'negative': 18}\n",
      "name： k3po7gouki ／フォロワー数： 7116\n",
      "date： 2019-12-27 22:04 ／ツイートID： 1210547002594840577\n",
      "RT数： 5602 ／favorite数： 13121\n",
      "リプライ数： 14 ／RTコメント数(上限１００）： 92\n",
      "\n",
      "==========\n",
      "なにわ男子「2019年ありがとうございました！」\n",
      "アオハルツアー愛知公演でしたー！\n",
      "2019年のコンサートを締めくくることができました！\n",
      "\n",
      "なにふぁむのみなさんへ\n",
      "\n",
      "｢最高な1年をありがとう♡｣\n",
      "\n",
      "なにわ男子からの報告事も！\n",
      "\n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 137, 'negative': 38}\n",
      "name： islandtv_up ／フォロワー数： 192219\n",
      "date： 2019-12-27 21:50 ／ツイートID： 1210543508886441985\n",
      "RT数： 8489 ／favorite数： 19730\n",
      "リプライ数： 2 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "今日で仕事納めの方が多いみたいなので良かったらこれ見て疲れを癒してください。 \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 81, 'negative': 14}\n",
      "name： jirosan77 ／フォロワー数： 203202\n",
      "date： 2019-12-27 21:46 ／ツイートID： 1210542380153065472\n",
      "RT数： 5883 ／favorite数： 29950\n",
      "リプライ数： 65 ／RTコメント数(上限１００）： 36\n",
      "\n",
      "==========\n",
      "昨日は恩師であり、恩人であり、ホンモノの小栗旬さんのお誕生日だったので、夜中に正装で会いに行きました。\n",
      "\n",
      "おめでとうございまーきのっ！ \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 53, 'negative': 15}\n",
      "name： hinode_obt ／フォロワー数： 173229\n",
      "date： 2019-12-27 21:36 ／ツイートID： 1210539839268913152\n",
      "RT数： 6265 ／favorite数： 97003\n",
      "リプライ数： 71 ／RTコメント数(上限１００）： 59\n",
      "\n",
      "==========\n",
      "＃嵐 松潤さんに、\n",
      "\n",
      "「ゴーちゃんってオス？」\n",
      "\n",
      "とも聞かれたブイ！\n",
      "\n",
      "松潤さんが思うほうでOKブイ\n",
      "\n",
      "＃ウルトラSUPERLIVE   \n",
      "＃Mステ\n",
      "＃ウルトラゴーちゃん\n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 84, 'negative': 24}\n",
      "name： gochan_V ／フォロワー数： 73943\n",
      "date： 2019-12-27 21:11 ／ツイートID： 1210533697167867906\n",
      "RT数： 8337 ／favorite数： 31012\n",
      "リプライ数： 3 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "東京スカイツリー駅近くにあるお店「まんまる」の、お好みできなことあんこと黒蜜をつけて食べる「あったか白玉」✨\n",
      "\n",
      "白玉が6個ついて500円とリーズナブルで、味も抜群の美味しさです！ \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 47, 'negative': 12}\n",
      "name： sweetroad5 ／フォロワー数： 355605\n",
      "date： 2019-12-27 21:08 ／ツイートID： 1210532793383411713\n",
      "RT数： 5502 ／favorite数： 26400\n",
      "リプライ数： 6 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "『ノンストップ・ストーリー』ライブキービジュアル用にホロライブメンバー23名を描かせていただきました。おるだんさんデザインの衣装が素敵すぎる😭✨ライブ今から楽しみだ～！ \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 119, 'negative': 20}\n",
      "name： naru_quadrille ／フォロワー数： 25871\n",
      "date： 2019-12-27 21:06 ／ツイートID： 1210532392407977984\n",
      "RT数： 5730 ／favorite数： 14769\n",
      "リプライ数： 82 ／RTコメント数(上限１００）： 55\n",
      "\n",
      "==========\n",
      "＃嵐 松潤さんは、\n",
      "\n",
      "「いつも同じ人が入ってる？」\n",
      "「何人かいるの？」\n",
      "「暑くない？」\n",
      "\n",
      "とか話しかけてくれたブイ！正直質問の意味はよく分からなかったけど、とにかくお話できて本当にうれしかったブイ！\n",
      "\n",
      "＃ウルトラSUPERLIVE   \n",
      "＃Mステ\n",
      "＃ウルトラゴーちゃん\n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 90, 'negative': 10}\n",
      "name： gochan_V ／フォロワー数： 73943\n",
      "date： 2019-12-27 21:00 ／ツイートID： 1210530972006936576\n",
      "RT数： 14112 ／favorite数： 49020\n",
      "リプライ数： 4 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "【🎉 重 大 発 表 🎉】\n",
      "\n",
      "「hololive 1st fes.『ノンストップ・ストーリー』」\n",
      "出演タレント23名のアイドル衣装＆メインヴィジュアル解禁✨\n",
      "\n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 64, 'negative': 24}\n",
      "name： hololivetv ／フォロワー数： 107923\n",
      "date： 2019-12-27 21:00 ／ツイートID： 1210530770474950657\n",
      "RT数： 5154 ／favorite数： 9639\n",
      "リプライ数： 2 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "King &amp; Princeのみなさんが #Mステカメラ 📹初登場～🤣\n",
      "\n",
      "今日のテーマは\n",
      "『今年一番盛り上がったこと』です😎Ⓜ\n",
      "”あるナゾナゾ”からメンバーを巻き込んだ一大事に😂\n",
      "答えは「森」なので「木が3本」でした🌲🌲🌲\n",
      "\n",
      "今日は『koi-wazurai』を披露してくれます😍😍\n",
      "#ウルトラSUPERLIVE #Mステ \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 124, 'negative': 24}\n",
      "name： Mst_com ／フォロワー数： 1247765\n",
      "date： 2019-12-27 20:23 ／ツイートID： 1210521643577040896\n",
      "RT数： 17031 ／favorite数： 53736\n",
      "リプライ数： 4 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "可愛い可愛い\n",
      "かずま。ふじお。\n",
      "お疲れちゃん。\n",
      "\n",
      "#HiGH_LOW_THE_WORST\n",
      "#ハイロー\n",
      "#THERAMPAGE \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 135, 'negative': 30}\n",
      "name： jun_shison0305 ／フォロワー数： 483902\n",
      "date： 2019-12-27 20:13 ／ツイートID： 1210519062096138240\n",
      "RT数： 8173 ／favorite数： 42790\n",
      "リプライ数： 73 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "✨ディズニー限定デザインPRESENT✨\n",
      "\n",
      "@lovemaybelline をフォロー&amp;\n",
      "この投稿をリツイートで、\n",
      "\n",
      "❤ラッシュニスタ N\n",
      "❤ハイパーシャープ ライナー R\n",
      "❤パウダーインペンシル BR2\n",
      "\n",
      "のミッキーマウスデザイン３itemsが抽選で10名様に当たるチャンス！\n",
      "\n",
      "応募は12/31まで🆗\n",
      "\n",
      "#メイベリン\n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 193, 'negative': 19}\n",
      "name： LoveMaybelline ／フォロワー数： 205744\n",
      "date： 2019-12-27 20:00 ／ツイートID： 1210515669101338625\n",
      "RT数： 6964 ／favorite数： 1248\n",
      "リプライ数： 95 ／RTコメント数(上限１００）： 9\n",
      "\n",
      "==========\n",
      "V6のみなさんが #Mステカメラ 📹初登場～🤣\n",
      "\n",
      "今日のテーマは\n",
      "『今年一番盛り上がったこと』です😎Ⓜ\n",
      "”ある後輩グループ\"の話から乱入に発展!?😠\n",
      "方向性を改める!?V6の来年は期待大です🤣\n",
      "\n",
      "今日は『ある日願いが叶ったんだ』を披露してくださいました🕺\n",
      "#ウルトラSUPERLIVE #Mステ \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 100, 'negative': 26}\n",
      "name： Mst_com ／フォロワー数： 1247765\n",
      "date： 2019-12-27 19:43 ／ツイートID： 1210511532645158913\n",
      "RT数： 13717 ／favorite数： 36595\n",
      "リプライ数： 3 ／RTコメント数(上限１００）： 100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "「鬼の手」袋を編みました。\n",
      "\n",
      "・左手にジャストフィット！\n",
      "・厚手の生地であったかい！\n",
      "・妖怪退治にも使える！\n",
      "・今冬のマストアイテム！ \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 68, 'negative': 19}\n",
      "name： bon_66 ／フォロワー数： 1294\n",
      "date： 2019-12-27 19:41 ／ツイートID： 1210511013507784705\n",
      "RT数： 25608 ／favorite数： 64036\n",
      "リプライ数： 80 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "\n",
      "↓↓↓positiveサンプル↓↓↓\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Full_text</th>\n",
       "      <th>Judge</th>\n",
       "      <th>Posi_score</th>\n",
       "      <th>Nega_score</th>\n",
       "      <th>Followers</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1210710961771859968</td>\n",
       "      <td>2019-12-28 08:56</td>\n",
       "      <td>kimetsu_off</td>\n",
       "      <td>【鬼滅の刃コラボ中！】\\nローソン国際展示場駅前店では1日限定で「鬼滅の刃」コラボを実施中で...</td>\n",
       "      <td>positive</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>961791</td>\n",
       "      <td>https://t.co/TgagYYCcAU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1210705650491113472</td>\n",
       "      <td>2019-12-28 08:34</td>\n",
       "      <td>bs_ponta</td>\n",
       "      <td>若月健矢選手、立花理香さん\\n\\nご結婚おめでとうございます！\\n末長くお幸せに✨\\n\\nバ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>288636</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1210702646509588480</td>\n",
       "      <td>2019-12-28 08:22</td>\n",
       "      <td>RiccaTachibana</td>\n",
       "      <td>【ご報告】この度、みなさまにご報告したいことができました。ぜひご覧いただけますと幸いです。</td>\n",
       "      <td>positive</td>\n",
       "      <td>145</td>\n",
       "      <td>19</td>\n",
       "      <td>175344</td>\n",
       "      <td>https://t.co/hR8m5IHoBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1210696997839110144</td>\n",
       "      <td>2019-12-28 08:00</td>\n",
       "      <td>kimetsu_off</td>\n",
       "      <td>【#本日12月28日は竈門禰豆子の誕生日!!】\\n本日は、鬼でありながら鬼殺隊に所属する\\n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>82</td>\n",
       "      <td>10</td>\n",
       "      <td>961791</td>\n",
       "      <td>https://t.co/gOF5Qp6woi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1210684401157206016</td>\n",
       "      <td>2019-12-28 07:10</td>\n",
       "      <td>kimetsu_goods</td>\n",
       "      <td>「鬼滅の刃」グッズプレゼント🎉\\n\\n大好評のため第2段\\nシリーズ累計2500万部記念\\n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>14631</td>\n",
       "      <td>https://t.co/xEvX9hhyVv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id              Date            Name  \\\n",
       "0  1210710961771859968  2019-12-28 08:56     kimetsu_off   \n",
       "1  1210705650491113472  2019-12-28 08:34        bs_ponta   \n",
       "2  1210702646509588480  2019-12-28 08:22  RiccaTachibana   \n",
       "3  1210696997839110144  2019-12-28 08:00     kimetsu_off   \n",
       "4  1210684401157206016  2019-12-28 07:10   kimetsu_goods   \n",
       "\n",
       "                                           Full_text     Judge Posi_score  \\\n",
       "0  【鬼滅の刃コラボ中！】\\nローソン国際展示場駅前店では1日限定で「鬼滅の刃」コラボを実施中で...  positive         20   \n",
       "1  若月健矢選手、立花理香さん\\n\\nご結婚おめでとうございます！\\n末長くお幸せに✨\\n\\nバ...  positive         71   \n",
       "2     【ご報告】この度、みなさまにご報告したいことができました。ぜひご覧いただけますと幸いです。   positive        145   \n",
       "3  【#本日12月28日は竈門禰豆子の誕生日!!】\\n本日は、鬼でありながら鬼殺隊に所属する\\n...  positive         82   \n",
       "4  「鬼滅の刃」グッズプレゼント🎉\\n\\n大好評のため第2段\\nシリーズ累計2500万部記念\\n...  positive         23   \n",
       "\n",
       "  Nega_score Followers                     link  \n",
       "0         18    961791  https://t.co/TgagYYCcAU  \n",
       "1          3    288636                     None  \n",
       "2         19    175344  https://t.co/hR8m5IHoBD  \n",
       "3         10    961791  https://t.co/gOF5Qp6woi  \n",
       "4         14     14631  https://t.co/xEvX9hhyVv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "↓↓↓negativeサンプル↓↓↓\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Full_text</th>\n",
       "      <th>Judge</th>\n",
       "      <th>Posi_score</th>\n",
       "      <th>Nega_score</th>\n",
       "      <th>Followers</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1210708419159609344</td>\n",
       "      <td>2019-12-28 08:45</td>\n",
       "      <td>noooooooorth</td>\n",
       "      <td>どなたかが呟かれてましたけど、成人のADHDでは「何か思いつくと、それをタスクの一番最後に追...</td>\n",
       "      <td>negative</td>\n",
       "      <td>46</td>\n",
       "      <td>55</td>\n",
       "      <td>15670</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1210707713971212288</td>\n",
       "      <td>2019-12-28 08:43</td>\n",
       "      <td>takeshi_tsuruno</td>\n",
       "      <td>除夜の鐘が煩い、お祭りが煩い、花火が煩い、風鈴が煩い、園児が煩い…\\n少数の煩いクレームにど...</td>\n",
       "      <td>negative</td>\n",
       "      <td>92</td>\n",
       "      <td>135</td>\n",
       "      <td>615857</td>\n",
       "      <td>https://t.co/YNAgYwudXY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1210688744660975616</td>\n",
       "      <td>2019-12-28 07:27</td>\n",
       "      <td>comiketofficial</td>\n",
       "      <td>【注意喚起！】寝ている人から財布等を抜き取る犯罪が発生しているという連絡が警察からありました...</td>\n",
       "      <td>negative</td>\n",
       "      <td>59</td>\n",
       "      <td>111</td>\n",
       "      <td>221597</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id              Date             Name  \\\n",
       "0  1210708419159609344  2019-12-28 08:45     noooooooorth   \n",
       "1  1210707713971212288  2019-12-28 08:43  takeshi_tsuruno   \n",
       "2  1210688744660975616  2019-12-28 07:27  comiketofficial   \n",
       "\n",
       "                                           Full_text     Judge Posi_score  \\\n",
       "0  どなたかが呟かれてましたけど、成人のADHDでは「何か思いつくと、それをタスクの一番最後に追...  negative         46   \n",
       "1  除夜の鐘が煩い、お祭りが煩い、花火が煩い、風鈴が煩い、園児が煩い…\\n少数の煩いクレームにど...  negative         92   \n",
       "2  【注意喚起！】寝ている人から財布等を抜き取る犯罪が発生しているという連絡が警察からありました...  negative         59   \n",
       "\n",
       "  Nega_score Followers                     link  \n",
       "0         55     15670                     None  \n",
       "1        135    615857  https://t.co/YNAgYwudXY  \n",
       "2        111    221597                     None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "↓↓↓fire_tweetサンプル↓↓↓\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Full_text</th>\n",
       "      <th>Judge</th>\n",
       "      <th>Posi_score</th>\n",
       "      <th>Nega_score</th>\n",
       "      <th>Followers</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, Date, Name, Full_text, Judge, Posi_score, Nega_score, Followers, link]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "csvへの書き出しが完了しました。新規データ数28、全データ数：28\n",
      "サンプルが0件の場合は、15分後に再度実行すると取得できる場合があります。\n",
      "fire_tweetは出現率が非常に低いです。\n"
     ]
    }
   ],
   "source": [
    "# 指定日のツイートを取得（API制限のため取得できるのは約一週間前のものまで）\n",
    "day = '2019-12-28'\n",
    "# リクエスト制限対応：True:リクエスト上限に達したら15分待機ののちツイート取得続行/ False:待機せずcsv取得\n",
    "reload = True\n",
    "\n",
    "#除外ワード\n",
    "exclud_words = \"配信スタート ＃キャンペーン　リツイートキャンペーン WWWWWWWWW\"\n",
    "\n",
    "#その他設定可能パラメータ\n",
    "#リプライをprint（print_rep = True/Fals), 最低RT数(RT_count = 5000)\n",
    "\n",
    "GT = Get_Twitter(day, reload, exclud_words)\n",
    "GT.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ２）データセットの前処理\n",
    "　・正規表現、ストップワード除去など\n",
    " \n",
    " #### 参考サイト\n",
    "Pythonで全角・半角記号をまとめて消し去る　http://prpr.hatenablog.jp/entry/2016/11/23/Python%E3%81%A7%E5%85%A8%E8%A7%92%E3%83%BB%E5%8D%8A%E8%A7%92%E8%A8%98%E5%8F%B7%E3%82%92%E3%81%BE%E3%81%A8%E3%82%81%E3%81%A6%E6%B6%88%E3%81%97%E5%8E%BB%E3%82%8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.6/site-packages (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.6/site-packages (from gensim) (1.13.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim) (0.19.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: smart_open>=1.2.1 in /opt/conda/lib/python3.6/site-packages (from gensim) (1.5.3)\n",
      "Requirement already satisfied: boto>=2.32 in /opt/conda/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim) (2.48.0)\n",
      "Requirement already satisfied: bz2file in /opt/conda/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim) (2.18.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->smart_open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->smart_open>=1.2.1->gensim) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->smart_open>=1.2.1->gensim) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->smart_open>=1.2.1->gensim) (2018.1.18)\n",
      "Requirement already satisfied: natto-py in /opt/conda/lib/python3.6/site-packages (0.9.0)\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.6/site-packages (from natto-py) (1.10.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi->natto-py) (2.18)\n",
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.6/site-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "#必要なツールをインストール(初回のみ実行)\n",
    "! pip install gensim\n",
    "! pip install natto-py\n",
    "! pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ツイートデータを学習用に整形\n",
    "#from natto import MeCab\n",
    "import MeCab\n",
    "import re\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import emoji\n",
    "import neologdn\n",
    "import urllib.request\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "class For_Model():\n",
    "    \n",
    "    def __init__(self, data, columns, out_file, mode, text, similar = None):\n",
    "        self.mecab = MeCab.Tagger(\"-Owakati\")\n",
    "        self.data = data\n",
    "        self.columns = columns\n",
    "        self.out_file = out_file\n",
    "        self.mode = mode\n",
    "        self.text = text\n",
    "        self.similar = str(similar)\n",
    "\n",
    "    #データを読み込む\n",
    "    def Load_tweets(self):        \n",
    "        df = pd.read_csv(self.data, usecols = self.columns)\n",
    "        print(\"読み込んだツイート\", df.shape)\n",
    "        \n",
    "        #３０w以下のtweet行を削除\n",
    "        index = []\n",
    "        for i in range(len(df)):\n",
    "            text = df.iloc[i, 2]\n",
    "            text = re.sub('https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", text)\n",
    "            text = re.sub('http?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", text)\n",
    "            df.iloc[i, 2] = text\n",
    "            if len(text) < 30:\n",
    "                index.append(i)\n",
    "        df_tweet = df.drop(df.index[index])\n",
    "        df_tweet = df_tweet.reset_index(drop=True)\n",
    "        \n",
    "        #判定用テキストをリストの最後に追加\n",
    "        tweets = []\n",
    "        for i in df_tweet[self.text]:\n",
    "            tweets.append(i)\n",
    "        if self.similar == None:\n",
    "            pass\n",
    "        else:\n",
    "            tweets.append(self.similar)\n",
    "        return df_tweet, tweets\n",
    "\n",
    "    def Stop_Words(self):\n",
    "        # ストップワードをダウンロード\n",
    "        url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "        urllib.request.urlretrieve(url, './output/stop_word.txt')\n",
    "\n",
    "        with open('./output/stop_word.txt', 'r', encoding='utf-8') as file:\n",
    "            stopwords = [word.replace('\\n', '') for word in file.readlines()]\n",
    "\n",
    "        #追加ストップワードを設定（助詞や意味のない平仮名１文字）\n",
    "        add_words = ['あ','い','う','え','お','か','き','く','け','こ','さ','し','す','せ','そ','た','ち','つ','て','と',\n",
    "                     'な','に','ぬ','ね','の','は','ひ','ふ','へ','ほ','ま','み','む','め','も','や','ゆ','よ',\n",
    "                     'ら','り','る','れ','ろ','わ','を','ん','が','ぎ','ぐ','げ','ご','ざ','じ','ず','ぜ','ぞ',\n",
    "                     'だ','ぢ','づ','で','ど','ば','び','ぶ','べ','ぼ','ぱ','ぴ','ぷ','ぺ','ぽ',\n",
    "                     'くん','です','ます','ました','そして','でも','だから','だが','くらい','その','それ','かも',\n",
    "                     'あれ','あの','あっ','そんな','この','これ','とか','とも','する','という','ござい',\n",
    "                     'ので','なんて','たら', 'られ','たい','さて','てる','ください','なる','けど','でし',\n",
    "                     'じゃん','だっ','なっ','でしょ', 'ある','って','こんな','ねえ'\n",
    "                    ]\n",
    "        stopwords = stopwords + add_words\n",
    "        return stopwords\n",
    "\n",
    "    def Tokenizer(self, text, stopwords):\n",
    "\n",
    "        words = []\n",
    "        text = self.mecab.parse(text)\n",
    "        text = text.split(' ')\n",
    "        for j in range(len(text)):\n",
    "            if text[j] not in stopwords:\n",
    "                words.append(text[j])\n",
    "        return words\n",
    "\n",
    "    def remove_emoji(self, text):\n",
    "        return ''.join(c for c in text if c not in emoji.UNICODE_EMOJI)\n",
    "\n",
    "    #記号削除\n",
    "    def format_text(self, text):\n",
    "        text = unicodedata.normalize(\"NFKC\", text)  # 全角記号を半角へ置換\n",
    "        # 記号を消し去るための魔法のテーブル作成\n",
    "        table = str.maketrans(\"\", \"\", string.punctuation  + \"「」、。・*`+-|?#!()\\[]<>=~/\")\n",
    "        text = text.translate(table)\n",
    "        return text\n",
    "\n",
    "    def main(self):\n",
    "        tweets_num = 0\n",
    "        stopwords = self.Stop_Words()\n",
    "        df_tweet, tweets = self.Load_tweets()\n",
    "        #ツイートを分かち書きしてcsvに出力(モード'a'はデータ追加、モード'w'は新規作成)\n",
    "        with open('./output/' + self.out_file, self.mode) as f:\n",
    "            for i in tweets:\n",
    "                tweets_num += 1\n",
    "                i = neologdn.normalize(i)\n",
    "                i = re.sub('\\n', \"\", i)\n",
    "                i = re.sub(r'[!-~]', \"\", i)#半角記号,数字,英字\n",
    "                i = re.sub(r'[︰-＠]', \"\", i)#全角記号\n",
    "                i = self.format_text(i)#記号削除\n",
    "                i = re.sub(r'[【】●ㅅ●Ф☆✩︎♡→←▼①②③④⑤『』ω《》∠∇∩♪∀◞ཀCщ≧≦ ́◤◢■◆★※↑↓〇◯○◎⇒▽◉Θ♫♬〃“”◇ᄉ⊂⊃д°]', \"\", i)\n",
    "                #i = re.sub(r'[!-~、。‥…？！〜「」｢｣:：“”【】※♪♩♫♬『』→↓↑《》〈〉[]≧∇≦・゜・●ㅅ●´Д´°ω°•ω•★＊☆♡（）✔Θ∀´∀｀˘ω˘‼бωб￣▽￣◉→←▼①②③④⑤]', \"\", i)\n",
    "                i = self.remove_emoji(i)\n",
    "                i = self.Tokenizer(i, stopwords)\n",
    "                i = ' '.join(i) #リストを文字列に変換\n",
    "                i = str(i)\n",
    "                f.write(i)\n",
    "\n",
    "        print('CSV出力完了：'+ self.out_file)\n",
    "        with open('./output/' + self.out_file) as f:\n",
    "             wakati = f.read()\n",
    "\n",
    "        print(\"学習用データに追加したツイート数：\", tweets_num)\n",
    "        print()\n",
    "        print(\"分かち書きサンプル\\n\", wakati[:50])\n",
    "        return df_tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match names.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-32c4a140396e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mFM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFor_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-b36e9d4ad608>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mtweets_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStop_Words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mdf_tweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;31m#ツイートを分かち書きしてcsvに出力(モード'a'はデータ追加、モード'w'は新規作成)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./output/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b36e9d4ad608>\u001b[0m in \u001b[0;36mLoad_tweets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#データを読み込む\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoad_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"読み込んだツイート\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1648\u001b[0m             if (self.usecols_dtype == 'string' and\n\u001b[1;32m   1649\u001b[0m                     not set(usecols).issubset(self.orig_names)):\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Usecols do not match names.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Usecols do not match names."
     ]
    }
   ],
   "source": [
    "#パラメータの設定\n",
    "\n",
    "#取得したデータのパス\n",
    "data = './output/buzz_tweet.csv'\n",
    "#取得したい列名\n",
    "columns = [\"Followers\", \"Full_text\",\"Posi_score\", \"Nega_score\",\"Judge\"]\n",
    "#出力ファイル名\n",
    "out_file = \"train_buzz.txt\"\n",
    "#学習データの保存モード　'a'：追加／'w'：上書き\n",
    "mode = 'w'\n",
    "#ツイートテキストの列を指定\n",
    "text = \"Full_text\"\n",
    "#判定させたいツイート予定文書（類似度確認のため、データセット内にあるツイート文を使用）\n",
    "similar = \"イオンマスク禁止従業員の人嫌がるのわかるわ。\\\n",
    "インフルで出校停止中なんだけど薬効いて体元気だからイオン遊ばせに来た。みたいな事凄く多いんだよ。\\\n",
    "『店員が媒介にならないよう全店でマスク奨励してます。ご理解下さい』\\\n",
    "ってアナウンスされる方が余程良いのでイオンさん、マスク禁止撤回して\"\n",
    "\n",
    "FM = For_Model(data, columns, out_file, mode, text, similar)\n",
    "df_tweet = FM.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ３）予測モデルを生成\n",
    "　・データセットをDoc２vecで学習<br>\n",
    "\n",
    "#### 参考サイト\n",
    "fastTextとDoc2Vecのモデルを作成してニュース記事の多クラス分類の精度を比較する<br> https://qiita.com/kazuki_hayakawa/items/ca5d4735b9514895e197<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Doc2Vecモデルの学習\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "f = open('./output/train_buzz.txt','r')#空白で単語を区切り、改行で文書を区切っているテキストデータ\n",
    "\n",
    "#１文書ずつ、単語に分割してリストに入れていく[([単語1,単語2,単語3],文書id),...]こんなイメージ\n",
    "#words：文書に含まれる単語のリスト（単語の重複あり）\n",
    "# tags：文書の識別子（リストで指定．1つの文書に複数のタグを付与できる）\n",
    "#fにテキスト データをいれる\n",
    "trainings = [TaggedDocument(words = data.split(),tags = [i]) for i,data in enumerate(f)]\n",
    "#print(type(trainings))\n",
    "print(\"Doc２vec文書ベクトル用モデルに学習させたツイート数\",len(trainings))\n",
    "# print(trainings[:20])\n",
    "\n",
    "#文書ベクトル用ツイートテキストの学習\n",
    "model = Doc2Vec(\n",
    "    documents= trainings, \n",
    "    dm = 1, \n",
    "    vector_size=300, \n",
    "    window=10, \n",
    "    alpha = 0.05, \n",
    "    min_count=1, \n",
    "    sample = 0, \n",
    "    workers=4, \n",
    "    epochs = 50\n",
    ")\n",
    "\n",
    "#出力用ディレクトリ作成（存在しない場合のみ）\n",
    "def Make_Dir():\n",
    "    new_dir_path = 'model'\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "# モデルのセーブ\n",
    "Make_Dir()\n",
    "model.save(\"./model/doc2vec.model\")\n",
    "\n",
    "# モデルのロード(モデルが用意してあれば、ここからで良い)\n",
    "m = Doc2Vec.load('./model/doc2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ４）ツイート予定文章のネガポジ予測を返す\n",
    "　・データセットから、入力しておいたツイート予定文書と似ている文書を探す<br>\n",
    "・ネガポジスコア付きで、類似ツイート上位１０個を返す<br>\n",
    "#### 結果：成功。入力文書と同じツイート文が類似度１位に。ネガポジもデータズレなく表示できた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#類似判定と類似している上位10件の文書を出力\n",
    "\n",
    "top10 = m.docvecs.most_similar(len(trainings) - 1)\n",
    "\n",
    "print(\"=========== 判定したいツイート ===========\\n\")\n",
    "print(similar)\n",
    "\n",
    "print()\n",
    "print(\"======= 類似度上位１０（全{}ツイート中） =======\".format(len(trainings)))\n",
    "print()\n",
    "for i in range(len(top10)):\n",
    "    score = top10[i]\n",
    "    index = int(score[0])\n",
    "    similar_score = score[1]\n",
    "    tweet = df_tweet[\"Full_text\"]\n",
    "    judge = df_tweet[\"Judge\"]\n",
    "    posi_score = df_tweet[\"Posi_score\"]\n",
    "    nega_score = df_tweet[\"Nega_score\"]\n",
    "    print(\"…………　類似ツイート{}位：類似度 {:.4g}　…………\".format((i+1), similar_score))\n",
    "    print()\n",
    "    print(tweet[index])\n",
    "    print()\n",
    "    print(\"【極性】：\", judge[index])\n",
    "    print(\"posi_score：\",posi_score[index], \"／\", \"nega_score：\", nega_score[index])\n",
    "\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# その他試みたこと\n",
    "断念、または精度が全く良くない。覚書として記録"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## １）文章ベクトルを特徴量としたネガポジ予測モデル\n",
    "　・文章ベクトルとフォロワー数を特徴量X、ネガポジスコアを目的変数yとしたデータを学習<br>\n",
    "　・文章ベクトルはDoc２vecとTf-idfの２種を作成<br>\n",
    "　・ツイート予定文書を入力してネガポジスコアを予測する<br>\n",
    "　・試した予測モデル<br>\n",
    "　・MultiOutputRegressor、SVRのrbf と　SVRの線形、lightgbm、ランダムフォレスト<br>\n",
    "#### 結果：精度が低すぎて断念<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vecで文章ベクトル取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Doc2vecでベクトル化\n",
    "from natto import MeCab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df_buzz = pd.read_csv('./output/buzz_tweet.csv',\n",
    "                      usecols = [\"Full_text\", \"Posi_score\", \"Nega_score\", \"Followers\"])\n",
    "#.to_csv('./output/for_training.csv', mode = \"a\", index = False, header = None)\n",
    "#pd.read_csv('./output/fire_buzz_tweet.csv', usecols = [\"Full_text\", \"Judge\", \"Sentiment\"]).to_csv('./output/for_training.csv', mode = \"a\", index = False, header = None)\n",
    "print(\"ベクトル化するセンチメントスコア付きデータ数：\", len(df_buzz))\n",
    "display(df_buzz.head())\n",
    "\n",
    "#doc2vecでベクトル化\n",
    "for_training = df_buzz['Full_text']\n",
    "#print(for_training)\n",
    "vector_tweet = []\n",
    "for i in for_training:\n",
    "    i = m.infer_vector(i)\n",
    "    vector_tweet.append(i)\n",
    "\n",
    "df_vector = pd.DataFrame(data = vector_tweet)\n",
    "\n",
    "# print(\"Doc2vecベクトル\")\n",
    "# display(df_vector.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idfでベクトル取得\n",
    "#### 参考サイト\n",
    "\n",
    "機械学習_サポートベクターマシーン_pythonで実装<br>\n",
    "https://dev.classmethod.jp/machine-learning/2017ad_20171214_svm_python/<br>\n",
    "Tf-idfベクトルってなんだ？<br> https://qiita.com/MasatoTsutsumi/items/5b0a140b1ecbdd0396e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2-1.tf-idf計算\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def Stop_Words():\n",
    "    # ストップワードをダウンロード\n",
    "    url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "    urllib.request.urlretrieve(url, './output/stop_word.txt')\n",
    "\n",
    "    with open('./output/stop_word.txt', 'r', encoding='utf-8') as file:\n",
    "        stopwords = [word.replace('\\n', '') for word in file.readlines()]\n",
    "\n",
    "    #追加ストップワードを設定（助詞や意味のない平仮名１文字）\n",
    "    add_words = ['あ','い','う','え','お','か','き','く','け','こ','さ','し','す','せ','そ','た','ち','つ','て','と',\n",
    "                 'な','に','ぬ','ね','の','は','ひ','ふ','へ','ほ','ま','み','む','め','も','や','ゆ','よ',\n",
    "                 'ら','り','る','れ','ろ','わ','を','ん','が','ぎ','ぐ','げ','ご','ざ','じ','ず','ぜ','ぞ',\n",
    "                 'だ','ぢ','づ','で','ど','ば','び','ぶ','べ','ぼ','ぱ','ぴ','ぷ','ぺ','ぽ',\n",
    "                 'くん','です','ます','ました','そして','でも','だから','だが','くらい','その','それ','かも',\n",
    "                 'あれ','あの','あっ','そんな','この','これ','とか','とも','する','という','ござい',\n",
    "                 'ので','なんて','たら', 'られ','たい','さて','てる','ください','なる','けど','でし',\n",
    "                 'じゃん','だっ','なっ','でしょ', 'ある','って','こんな','ねえ'\n",
    "                ]\n",
    "    stopwords = stopwords + add_words\n",
    "    return stopwords\n",
    "\n",
    "stopwords = Stop_Words()\n",
    "tfidfv = TfidfVectorizer(lowercase=True, stop_words=stopwords) # stop words処理\n",
    " \n",
    "tfv_vector_fit = tfidfv.fit(for_training)\n",
    "tfv_vector = tfv_vector_fit.transform(for_training)\n",
    "print(tfv_vector.shape) \n",
    "\n",
    "# 2-2.次元削減(「lsa」を使って次元削減を行う)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# 2-2-1.パラメータの調整\n",
    "list_n_comp = [5,10,50,100,500,1000] # 特徴量を何個に削減するか、というパラメータです。できるだけ情報量を欠損しないで、かつ次元数は少なくしたいですね。\n",
    "for i in list_n_comp:\n",
    "    lsa = TruncatedSVD(n_components=i,n_iter=5, random_state = 0)\n",
    "    lsa.fit(tfv_vector) \n",
    "    tfv_vector_lsa = lsa.transform(tfv_vector)\n",
    "    print('次元削減後の特徴量が{0}の時の説明できる分散の割合合計は{1}です'.format(i,round((sum(lsa.explained_variance_ratio_)),2)))\n",
    "\n",
    "# 2-2-2.次元削減した状態のデータを作成\n",
    "# 上記で確認した「n_components」に指定した上で、次元削減（特徴抽出）を行う\n",
    "lsa = TruncatedSVD(n_components=1000, n_iter=5, random_state = 0) # 今回は次元数を1000に指定\n",
    "lsa.fit(tfv_vector)\n",
    "X_tf = lsa.transform(tfv_vector)\n",
    "# print()\n",
    "# print(\"次元削減後Tf-idfベクトル\\n\", X_tf.shape)\n",
    "# print(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X、yデータを作成\n",
    "\n",
    "#Doc2vecのベクトルデータ\n",
    "print(\"欠損値削除前データ\", df_buzz.shape)\n",
    "print()\n",
    "\n",
    "#文書ベクトルを含んだdf\n",
    "df_buzz_vec = pd.concat([df_buzz, df_vector], axis=1)\n",
    "df_buzz_vec = df_buzz_vec.dropna(subset = [\"Followers\"])#欠損値行削除\n",
    "df_buzz_vec = df_buzz_vec.drop([ \"Full_text\", \"Nega_score\", \"Posi_score\"], axis=1)\n",
    "X = df_buzz_vec.values\n",
    "print(\"Doc2vecベクトル\")\n",
    "print(\"X.shape\", X.shape)\n",
    "display(df_buzz_vec.head())\n",
    "\n",
    "#tf-idfのベクトルデータ\n",
    "tf_df = pd.DataFrame(data = X_tf)\n",
    "tf_df = pd.concat([df_buzz, tf_df], axis=1)\n",
    "tf_df = tf_df.dropna(subset = [\"Followers\"])#欠損値行削除\n",
    "tf_df = tf_df.drop([ \"Full_text\", \"Nega_score\", \"Posi_score\"], axis=1)\n",
    "X_tf_idf = tf_df.values\n",
    "print(\"Tf-idfベクトル\")\n",
    "print(\"X_tf_idf.shape\", tf_df.shape)\n",
    "display(tf_df.head())\n",
    "\n",
    "#yデータ作成\n",
    "df_buzz = df_buzz.dropna(subset = [\"Followers\"])#y用に\"Followers\"の欠損行削除\n",
    "y = df_buzz.loc[:,['Posi_score', 'Nega_score']]#できればDateも特徴量に入れたい\n",
    "y_p = df_buzz['Posi_score']\n",
    "y_n = df_buzz['Nega_score']\n",
    "y = y.values\n",
    "print()\n",
    "print(\"y.shape\", y.shape)\n",
    "y_p = y_p.values\n",
    "y_n = y_n.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiOutputRegressorで複数の回帰¶\n",
    "#### 結果：D2vベクトルよりTf-idfがややマシ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MultiOutputRegressorで複数の回帰\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#D2vベクトル\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.7, test_size=0.3, random_state=0)\n",
    "\n",
    "#X, y = make_regression(n_samples=10, n_targets=3, random_state=1)\n",
    "MOR = MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X_train, y_train)\n",
    "y_pred = MOR.predict(X_test)\n",
    "score = MOR.score(X_test, y_test)\n",
    "\n",
    "print(\"正解\\n\", y_test)\n",
    "print()\n",
    "print(y_pred)\n",
    "print(\"R ^ 2_score(1に近いほど良い）：\", score)\n",
    "print()\n",
    "\n",
    "#X_tf tf-idfベクトルを使った予測\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tf_idf, y, train_size=0.7, test_size=0.3, random_state=0)\n",
    "\n",
    "MOR = MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X_train, y_train)\n",
    "y_pred = MOR.predict(X_test)\n",
    "score = MOR.score(X_test, y_test)\n",
    "print(y_pred)\n",
    "print(\"R ^ 2_score(1に近いほど良い）：\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVRのrbf と　SVRの線形で予測\n",
    "#### 結果：予測値が全くダメ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ポジ、ネガ別々で予測する\n",
    "#SVRのrbf と　SVRの線形\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "#D2vベクトル(ポジのみ)\n",
    "X_train, X_test, y_p_train, y_p_test = train_test_split(\n",
    "    X, y_p, train_size=0.7, test_size=0.3, random_state=0)\n",
    "\n",
    "svr_rbf = SVR(kernel='rbf', C=1, gamma=0.1)\n",
    "svr_lin = SVR(kernel='linear', C=1)\n",
    "y_rbf = svr_rbf.fit(X_train, y_p_train)\n",
    "y_lin = svr_lin.fit(X_train, y_p_train)\n",
    "\n",
    "pred_rbf = svr_rbf.predict(X_test)\n",
    "pred_lin = svr_lin.predict(X_test)\n",
    "\n",
    "#精度\n",
    "\n",
    "# 相関係数計算\n",
    "rbf_corr = np.corrcoef(y_p_test, pred_rbf)[0, 1]\n",
    "lin_corr = np.corrcoef(y_p_test, pred_lin)[0, 1]\n",
    "\n",
    "# RMSEを計算（０に近いほど良い）\n",
    "rbf_rmse = sqrt(mean_squared_error(pred_rbf, y_p_test))\n",
    "lin_rmse = sqrt(mean_squared_error(pred_lin, y_p_test))\n",
    "\n",
    "print(\"RBF: RMSE（０に近いほど良い） {} \".format(rbf_rmse))\n",
    "print(\"Linear: RMSE（０に近いほど良い） {}\" .format(lin_rmse))\n",
    "print()\n",
    "print(\"正解\", y_p_test)\n",
    "print(\"rbf推定\", pred_rbf)\n",
    "print(\"lin推定\", pred_lin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm\n",
    "#### 参考サイト\n",
    "\n",
    "Mercari Price Challenge -機械学習を使ったメルカリの価格予測 Ridge回帰 LightGBM\n",
    "\n",
    "http://rautaku.hatenablog.com/entry/2017/12/22/195649\n",
    "\n",
    "#### 結果：RMSEが0には程遠い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#必要なツールをインストール(初回のみ実行)\n",
    "! pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LightGBM を使った回帰予測(D2Vベクトル)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    #D2vベクトル(ポジのみ)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_p, train_size=0.7, test_size=0.3, random_state=0)\n",
    "\n",
    "    # データセットを生成する\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "    # LightGBM のハイパーパラメータ\n",
    "    lgbm_params = {\n",
    "        # 回帰問題\n",
    "        'objective': 'regression',\n",
    "        # RMSE (平均二乗誤差平方根) の最小化を目指す\n",
    "        'metric': 'rmse',\n",
    "    }\n",
    "\n",
    "    # 上記のパラメータでモデルを学習する\n",
    "    model = lgb.train(lgbm_params, lgb_train, \n",
    "                      valid_sets=lgb_eval, num_boost_round=8000, \n",
    "                      early_stopping_rounds=5000, verbose_eval=500)\n",
    "\n",
    "    # テストデータを予測する\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "    # RMSE を計算する\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"RMSE（０に近いほど良い）\", rmse)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LightGBM を使った回帰予測（Tfーidfベクトル）\n",
    "\n",
    "def main():\n",
    "\n",
    "    #X_tf tf-idfベクトルを使った予測(ポジのみ)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_tf_idf, y_p, train_size=0.7, test_size=0.3, random_state=0)\n",
    "\n",
    "    # データセットを生成する\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "    # LightGBM のハイパーパラメータ\n",
    "    lgbm_params = {\n",
    "        # 回帰問題\n",
    "        'objective': 'regression',\n",
    "        # RMSE (平均二乗誤差平方根) の最小化を目指す\n",
    "        'metric': 'rmse',\n",
    "    }\n",
    "    \n",
    "    # 上記のパラメータでモデルを学習する\n",
    "    model = lgb.train(lgbm_params, lgb_train, \n",
    "                      valid_sets=lgb_eval, num_boost_round=8000, \n",
    "                      early_stopping_rounds=5000, verbose_eval=500)\n",
    "#     model = lgb.LGBMRegressor()\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "    # テストデータを予測する\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # RMSE を計算する\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"RMSE（０に近いほど良い）\",rmse)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ランダムフォレスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#D2vベクトル\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.7, test_size=0.3, random_state=0)\n",
    "# ランダムフォレスト回帰オブジェクト生成\n",
    "rfr = RandomForestRegressor(n_estimators=100)\n",
    "# 学習の実行\n",
    "rfr.fit(X_train, y_train)\n",
    "# テストデータで予測実行\n",
    "predict_y = rfr.predict(X_test)\n",
    "# R2決定係数で評価\n",
    "r2_score = r2_score(y_test, predict_y)\n",
    "print(\"R^2(1に近いほど良い）:\", r2_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ２）ツイッターAPI制限への挑戦（データセットの拡大）\n",
    "　・古いツイートを大量取得できるパッケージを発見（通常は１週間程度しか遡れない）<br>\n",
    "#### 結果：取得データから反応ツイートの取得を試みたができなかった<br>\n",
    "\n",
    "### GetOldTweets3 0.0.11\n",
    "古いツイートをトークン申請なしで大量取得できるパッケージ<br>\n",
    "https://pypi.org/project/GetOldTweets3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#必要なツールをインストール(初回のみ実行)\n",
    "! pip install GetOldTweets3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#指定日のトップツイートを取得、'./output/toptweets.csv'に保存\n",
    "! GetOldTweets3 --lang ja  --toptweets  --querysearch \"\" --since 2019-2-10 --until 2019-2-11 --output './output/toptweets.csv'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
