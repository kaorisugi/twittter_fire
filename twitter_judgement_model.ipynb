{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 極性判定とDoc２Vecを使ったTwitterネガポジ予測\n",
    "＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝\n",
    "### 【このnotebookについて】\n",
    "2019年7〜10月までフルタイムで通っていたスクールの卒業課題テーマを、機械学習の勉強のために発展させたものです<br>\n",
    "卒業発表スライド　https://www.slideshare.net/secret/y0m7g1nZdxpVYP<br>\n",
    "＊当初は炎上予測がテーマだったので、このnotebookの内容とはややズレます<br>\n",
    "＊表紙スライドの字が見えない場合は２枚目から戻ると見えます<br>\n",
    "\n",
    "＊ちなみに…<br>\n",
    "スクールで取り組んだ課題のリポジトリ \n",
    "https://github.com/kaorisugi/diveintocode-ml<br>\n",
    "論文読解課題のスライドシェア \n",
    "https://www.slideshare.net/secret/qGmdiwl4uGS20O<br>\n",
    "\n",
    "### 【ゴール】\n",
    "これからツイートする予定の文章に対し、過去の類似ツイートを探し、反応のネガポジスコア付きで上位１０位まで提示する。<br>\n",
    "### 【モデルの仕組み】\n",
    "１）ツイートデータセットを取得<br>\n",
    "　・TwitterAPIを使ってツイートを取得<br>\n",
    "　・各ツイートに対する反応ツイート（リプライ、引用RT）を取得<br>\n",
    "　・反応ツイートの極性表現数をカウントしてネガポジスコアとpositive/negative/fire!!!判定を得る<br>\n",
    " 　（positive/negativeの判定基準：極性表現数が多い方、fire!!!(炎上）の判定基準：極性表現の７０％以上がnegative）<br>\n",
    "２）データセットの前処理<br>\n",
    "　・正規表現、ストップワード除去など<br>\n",
    "３）予測モデルを生成<br>\n",
    "　・データセットをDoc２vecで学習<br>\n",
    "４）ツイート予定文章のネガポジ予測を返す<br>\n",
    "　・データセットから、ツイート予定文書と似ている文書を探す<br>\n",
    " ・ネガポジスコア付きで、類似ツイート上位１０個を返す\n",
    "### 【結果】\n",
    "類似度確認用にデータセット内にあるものと同じ文を入力したところ、類似度1位で返ってきた。また、２位、３位にもマスクに関する似た話題のツイートが提示されたので類似ツイートの抽出は成功。ネガポジスコアもデータズレなどなく正確に表示され、目的は達成できた。<br>\n",
    "ツイッターAPI制限により、まだサンプルが少ない（完成時２００件程度）が、データを蓄積できる仕様にしているので、ツイート文のバリエーションを増やしていけば、様々な入力文に対応できるようになると思う。<br>\n",
    "ネガポジ判定については、ネガティブなテーマへの言及に共感したコメントでネガ判定が出ているケースも多く、必ずしもツイート主へのネガ感情ではないことに注意が必要。<br>\n",
    "\n",
    "### 【その他試みたこと】\n",
    "１）文章ベクトルを特徴量としたネガポジ予測モデル<br>\n",
    "　・文章ベクトルとフォロワー数を特徴量X、ネガポジスコアを目的変数yとしたデータを学習<br>\n",
    "　・文章ベクトルはDoc２vecとTf-idfの２種を作成<br>\n",
    "　・ツイート予定文書を入力してネガポジスコアを予測する<br>\n",
    "　・試した予測モデル<br>\n",
    "　・MultiOutputRegressor、SVRのrbf と　SVRの線形、lightgbm、ランダムフォレスト<br>\n",
    "　  　→精度が低すぎて断念<br>\n",
    "２）ツイッターAPI制限への挑戦（データセットの拡大）<br>\n",
    "　・古いツイートを大量取得できるパッケージを発見（通常は１週間程度しか遡れない）<br>\n",
    "　　　→取得データから反応ツイートの取得を試みたができなかった<br>\n",
    "   \n",
    "### 【利用するには】\n",
    "・config.py ファイルにツイッターAPIトークンを記入<br>\n",
    "・インストールが必要なツールは、notebook内にマジックコマンドにて記載してあります<br>\n",
    "・jupyternotebook上でのMeCabの利用で詰まる場合は、下記のDockerイメージを使うとうまくいくかと思います。<br>\n",
    "Docker Japanese NLP<br>\n",
    "https://github.com/hoto17296/docker-japanese-nlp<br>\n",
    "＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## １）ツイートデータセットを取得\n",
    "・TwitterAPIでツイートを取得<br>\n",
    "・各ツイートに対するリプライ、引用RTを取得<br>\n",
    "・極性表現数をカウントしてネガポジスコアを得る<br>\n",
    "\n",
    "#### 参考サイト\n",
    "【Python】tweepyでTwitterのツイートを検索して取得<br>\n",
    "https://vatchlog.com/tweepy-search/<br>\n",
    "【Python】tweepyで期間指定してツイートを検索する<br>\n",
    "https://vatchlog.com/tweepy-search-time/<br>\n",
    "バズったツイートへのリアクションを感情分析してみる<br>【Google Natural Language API / Python】<br>\n",
    "https://qiita.com/matsuri0828/items/029b4d0d510dcfb5c5dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\r\n",
      "  Downloading https://files.pythonhosted.org/packages/00/b6/9cfa56b4081ad13874b0c6f96af8ce16cfbc1cb06bedf8e9164ce5551ec1/pip-19.3.1-py2.py3-none-any.whl (1.4MB)\r\n",
      "\u001b[?25l\r",
      "\u001b[K    0% |▎                               | 10kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K    1% |▌                               | 20kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K    2% |▊                               | 30kB 1.9MB/s eta 0:00:01\r",
      "\u001b[K    2% |█                               | 40kB 1.5MB/s eta 0:00:01\r",
      "\u001b[K    3% |█▏                              | 51kB 1.3MB/s eta 0:00:02\r",
      "\u001b[K    4% |█▍                              | 61kB 1.5MB/s eta 0:00:01\r",
      "\u001b[K    5% |█▋                              | 71kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K    5% |█▉                              | 81kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K    6% |██                              | 92kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K    7% |██▎                             | 102kB 1.9MB/s eta 0:00:01\r",
      "\u001b[K    7% |██▌                             | 112kB 1.9MB/s eta 0:00:01\r",
      "\u001b[K    8% |██▉                             | 122kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K    9% |███                             | 133kB 1.9MB/s eta 0:00:01\r",
      "\u001b[K    10% |███▎                            | 143kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K    10% |███▌                            | 153kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K    11% |███▊                            | 163kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K    12% |████                            | 174kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K    13% |████▏                           | 184kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K    13% |████▍                           | 194kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K    14% |████▋                           | 204kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K    15% |████▉                           | 215kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K    15% |█████                           | 225kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K    16% |█████▎                          | 235kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K    17% |█████▋                          | 245kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K    18% |█████▉                          | 256kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K    18% |██████                          | 266kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K    19% |██████▎                         | 276kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K    20% |██████▌                         | 286kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K    20% |██████▊                         | 296kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K    21% |███████                         | 307kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K    22% |███████▏                        | 317kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K    23% |███████▍                        | 327kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K    23% |███████▋                        | 337kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K    24% |███████▉                        | 348kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K    25% |████████                        | 358kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K    26% |████████▍                       | 368kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K    26% |████████▋                       | 378kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K    27% |████████▉                       | 389kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K    28% |█████████                       | 399kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K    28% |█████████▎                      | 409kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K    29% |█████████▌                      | 419kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K    30% |█████████▊                      | 430kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K    31% |██████████                      | 440kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K    31% |██████████▏                     | 450kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K    32% |██████████▍                     | 460kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K    33% |██████████▋                     | 471kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K    34% |██████████▉                     | 481kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K    34% |███████████▏                    | 491kB 1.9MB/s eta 0:00:01\r",
      "\u001b[K    35% |███████████▍                    | 501kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K    36% |███████████▋                    | 512kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K    36% |███████████▉                    | 522kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K    37% |████████████                    | 532kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K    38% |████████████▎                   | 542kB 1.6MB/s eta 0:00:01\r",
      "\u001b[K    39% |████████████▌                   | 552kB 1.9MB/s eta 0:00:01\r",
      "\u001b[K    39% |████████████▊                   | 563kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K    40% |█████████████                   | 573kB 1.9MB/s eta 0:00:01\r",
      "\u001b[K    41% |█████████████▏                  | 583kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K    41% |█████████████▍                  | 593kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K    42% |█████████████▋                  | 604kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K    43% |██████████████                  | 614kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K    44% |██████████████▏                 | 624kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K    44% |██████████████▍                 | 634kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K    45% |██████████████▋                 | 645kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K    46% |██████████████▉                 | 655kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K    47% |███████████████                 | 665kB 3.6MB/s eta 0:00:01\r",
      "\u001b[K    47% |███████████████▎                | 675kB 3.6MB/s eta 0:00:01\r",
      "\u001b[K    48% |███████████████▌                | 686kB 3.6MB/s eta 0:00:01\r",
      "\u001b[K    49% |███████████████▊                | 696kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K    49% |████████████████                | 706kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K    50% |████████████████▏               | 716kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K    51% |████████████████▍               | 727kB 3.0MB/s eta 0:00:01\r",
      "\u001b[K    52% |████████████████▊               | 737kB 3.0MB/s eta 0:00:01\r",
      "\u001b[K    52% |█████████████████               | 747kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K    53% |█████████████████▏              | 757kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K    54% |█████████████████▍              | 768kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K    54% |█████████████████▋              | 778kB 4.0MB/s eta 0:00:01\r",
      "\u001b[K    55% |█████████████████▉              | 788kB 3.2MB/s eta 0:00:01\r",
      "\u001b[K    56% |██████████████████              | 798kB 4.2MB/s eta 0:00:01\r",
      "\u001b[K    57% |██████████████████▎             | 808kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K    57% |██████████████████▌             | 819kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K    58% |██████████████████▊             | 829kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K    59% |███████████████████             | 839kB 5.7MB/s eta 0:00:01\r",
      "\u001b[K    60% |███████████████████▏            | 849kB 5.6MB/s eta 0:00:01\r",
      "\u001b[K    60% |███████████████████▌            | 860kB 4.6MB/s eta 0:00:01\r",
      "\u001b[K    61% |███████████████████▊            | 870kB 4.6MB/s eta 0:00:01\r",
      "\u001b[K    62% |████████████████████            | 880kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K    62% |████████████████████▏           | 890kB 6.9MB/s eta 0:00:01\r",
      "\u001b[K    63% |████████████████████▍           | 901kB 5.7MB/s eta 0:00:01\r",
      "\u001b[K    64% |████████████████████▋           | 911kB 5.7MB/s eta 0:00:01\r",
      "\u001b[K    65% |████████████████████▉           | 921kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K    65% |█████████████████████           | 931kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K    66% |█████████████████████▎          | 942kB 7.1MB/s eta 0:00:01\r",
      "\u001b[K    67% |█████████████████████▌          | 952kB 7.0MB/s eta 0:00:01\r",
      "\u001b[K    68% |█████████████████████▊          | 962kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K    68% |██████████████████████          | 972kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K    69% |██████████████████████▎         | 983kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K    70% |██████████████████████▌         | 993kB 11.2MB/s eta 0:00:01\r",
      "\u001b[K    70% |██████████████████████▊         | 1.0MB 16.9MB/s eta 0:00:01\r",
      "\u001b[K    71% |███████████████████████         | 1.0MB 16.6MB/s eta 0:00:01\r",
      "\u001b[K    72% |███████████████████████▏        | 1.0MB 16.6MB/s eta 0:00:01\r",
      "\u001b[K    73% |███████████████████████▍        | 1.0MB 12.5MB/s eta 0:00:01\r",
      "\u001b[K    73% |███████████████████████▋        | 1.0MB 13.0MB/s eta 0:00:01\r",
      "\u001b[K    74% |███████████████████████▉        | 1.1MB 13.6MB/s eta 0:00:01\r",
      "\u001b[K    75% |████████████████████████        | 1.1MB 12.9MB/s eta 0:00:01\r",
      "\u001b[K    75% |████████████████████████▎       | 1.1MB 12.8MB/s eta 0:00:01\r",
      "\u001b[K    76% |████████████████████████▌       | 1.1MB 13.4MB/s eta 0:00:01\r",
      "\u001b[K    77% |████████████████████████▊       | 1.1MB 13.2MB/s eta 0:00:01\r",
      "\u001b[K    78% |█████████████████████████       | 1.1MB 12.0MB/s eta 0:00:01\r",
      "\u001b[K    78% |█████████████████████████▎      | 1.1MB 10.9MB/s eta 0:00:01\r",
      "\u001b[K    79% |█████████████████████████▌      | 1.1MB 12.9MB/s eta 0:00:01\r",
      "\u001b[K    80% |█████████████████████████▊      | 1.1MB 13.7MB/s eta 0:00:01\r",
      "\u001b[K    81% |██████████████████████████      | 1.1MB 12.4MB/s eta 0:00:01\r",
      "\u001b[K    81% |██████████████████████████▏     | 1.2MB 12.5MB/s eta 0:00:01\r",
      "\u001b[K    82% |██████████████████████████▍     | 1.2MB 11.5MB/s eta 0:00:01\r",
      "\u001b[K    83% |██████████████████████████▋     | 1.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K    83% |██████████████████████████▉     | 1.2MB 11.9MB/s eta 0:00:01\r",
      "\u001b[K    84% |███████████████████████████     | 1.2MB 12.2MB/s eta 0:00:01\r",
      "\u001b[K    85% |███████████████████████████▎    | 1.2MB 13.9MB/s eta 0:00:01\r",
      "\u001b[K    86% |███████████████████████████▌    | 1.2MB 16.1MB/s eta 0:00:01\r",
      "\u001b[K    86% |███████████████████████████▉    | 1.2MB 13.4MB/s eta 0:00:01\r",
      "\u001b[K    87% |████████████████████████████    | 1.2MB 16.4MB/s eta 0:00:01\r",
      "\u001b[K    88% |████████████████████████████▎   | 1.2MB 13.8MB/s eta 0:00:01\r",
      "\u001b[K    88% |████████████████████████████▌   | 1.3MB 13.5MB/s eta 0:00:01\r",
      "\u001b[K    89% |████████████████████████████▊   | 1.3MB 9.5MB/s eta 0:00:01\r",
      "\u001b[K    90% |█████████████████████████████   | 1.3MB 7.7MB/s eta 0:00:01\r",
      "\u001b[K    91% |█████████████████████████████▏  | 1.3MB 7.8MB/s eta 0:00:01\r",
      "\u001b[K    91% |█████████████████████████████▍  | 1.3MB 6.4MB/s eta 0:00:01\r",
      "\u001b[K    92% |█████████████████████████████▋  | 1.3MB 6.2MB/s eta 0:00:01\r",
      "\u001b[K    93% |█████████████████████████████▉  | 1.3MB 6.2MB/s eta 0:00:01\r",
      "\u001b[K    94% |██████████████████████████████  | 1.3MB 6.9MB/s eta 0:00:01\r",
      "\u001b[K    94% |██████████████████████████████▎ | 1.3MB 6.9MB/s eta 0:00:01\r",
      "\u001b[K    95% |██████████████████████████████▋ | 1.4MB 5.6MB/s eta 0:00:01\r",
      "\u001b[K    96% |██████████████████████████████▉ | 1.4MB 4.1MB/s eta 0:00:01\r",
      "\u001b[K    96% |███████████████████████████████ | 1.4MB 5.0MB/s eta 0:00:01\r",
      "\u001b[K    97% |███████████████████████████████▎| 1.4MB 4.0MB/s eta 0:00:01\r",
      "\u001b[K    98% |███████████████████████████████▌| 1.4MB 4.0MB/s eta 0:00:01\r",
      "\u001b[K    99% |███████████████████████████████▊| 1.4MB 4.4MB/s eta 0:00:01\r",
      "\u001b[K    99% |████████████████████████████████| 1.4MB 4.5MB/s eta 0:00:01\r",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 713kB/s \r\n",
      "\u001b[?25hInstalling collected packages: pip\r\n",
      "  Found existing installation: pip 9.0.1\r\n",
      "    Uninstalling pip-9.0.1:\r\n",
      "      Successfully uninstalled pip-9.0.1\r\n",
      "Successfully installed pip-19.3.1\r\n",
      "Collecting tweepy\r\n",
      "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /opt/conda/lib/python3.6/site-packages (from tweepy) (1.6.7)\r\n",
      "Requirement already satisfied: requests>=2.11.1 in /opt/conda/lib/python3.6/site-packages (from tweepy) (2.18.4)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tweepy) (1.11.0)\r\n",
      "Collecting requests-oauthlib>=0.7.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.11.1->tweepy) (2.6)\r\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.11.1->tweepy) (1.22)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.11.1->tweepy) (2018.1.18)\r\n",
      "Collecting oauthlib>=3.0.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\r\n",
      "\r",
      "\u001b[K     |██▎                             | 10kB 4.1MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 20kB 6.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 30kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 40kB 11.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 51kB 13.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 61kB 14.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 71kB 15.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 81kB 17.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 92kB 11.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 102kB 11.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 112kB 11.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 122kB 11.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 133kB 11.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 143kB 11.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 153kB 11.9MB/s \r\n",
      "\u001b[?25hInstalling collected packages: oauthlib, requests-oauthlib, tweepy\r\n",
      "Successfully installed oauthlib-3.1.0 requests-oauthlib-1.3.0 tweepy-3.8.0\r\n",
      "Collecting oseti\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/ac/84c61da1e5dd9620b71e863dd4e1bef68eb5533a228fc17dd4d8f98d908d/oseti-0.2.tar.gz (75kB)\r\n",
      "\r",
      "\u001b[K     |████▎                           | 10kB 13.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 20kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 30kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 40kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 51kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 61kB 4.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 71kB 4.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 81kB 4.4MB/s \r\n",
      "\u001b[?25hCollecting sengiri\r\n",
      "  Downloading https://files.pythonhosted.org/packages/66/51/584352e8c50c2d73696c40b3be8a78b6eab7ee3db7b021c1c7e247ceb399/sengiri-0.2.1.tar.gz\r\n",
      "Requirement already satisfied: neologdn in /opt/conda/lib/python3.6/site-packages (from oseti) (0.2.1)\r\n",
      "Requirement already satisfied: mecab-python3 in /opt/conda/lib/python3.6/site-packages (from oseti) (0.7)\r\n",
      "Collecting emoji\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\r\n",
      "\r",
      "\u001b[K     |███████▌                        | 10kB 19.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 20kB 22.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 30kB 26.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 40kB 29.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 11.8MB/s \r\n",
      "\u001b[?25hBuilding wheels for collected packages: oseti, sengiri, emoji\r\n",
      "  Building wheel for oseti (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for oseti: filename=oseti-0.2-cp36-none-any.whl size=75997 sha256=fa59bef52cdf3ac5af8d719db2eb3a2d4039c64d698cf2ac99807caeba04a01d\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/a0/b8/306332f20803a7af8bbb35ab3ddaf625dfef62076a6372eec7\r\n",
      "  Building wheel for sengiri (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sengiri: filename=sengiri-0.2.1-cp36-none-any.whl size=5545 sha256=fb4986a7e7eb7710f27046c0a02362d815672d79ba9e14742bdee52327630f24\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/dd/c1/36/d123492db6ec7445fe4124351505e1150a35e03e5c08f3dad0\r\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=43030 sha256=9c5155f8758d5d3ee35d557362c3e5a15d2fa9927ff2082ef4a3c1f57da28fcb\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\r\n",
      "Successfully built oseti sengiri emoji\r\n",
      "Installing collected packages: emoji, sengiri, oseti\r\n",
      "Successfully installed emoji-0.5.4 oseti-0.2 sengiri-0.2.1\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (2.18.4)\r\n",
      "Requirement already satisfied: requests_oauthlib in /opt/conda/lib/python3.6/site-packages (1.3.0)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests) (2.6)\r\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests) (1.22)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests) (2018.1.18)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from requests_oauthlib) (3.1.0)\r\n",
      "Requirement already satisfied: sengiri in /opt/conda/lib/python3.6/site-packages (0.2.1)\r\n",
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.6/site-packages (from sengiri) (0.5.4)\r\n"
     ]
    }
   ],
   "source": [
    "#必要なツールをインストール（初回のみ実行）\n",
    "! pip install --upgrade pip\n",
    "! pip install tweepy\n",
    "! pip install oseti\n",
    "! pip install requests requests_oauthlib\n",
    "! pip install sengiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import re\n",
    "import emoji\n",
    "import oseti\n",
    "from datetime import datetime, date, timedelta, time\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import config\n",
    "\n",
    "class Get_Twitter():\n",
    "\n",
    "    def __init__(self, day, reload, print_rep = False, exclud_words = \"配信スタート ＃キャンペーン　リツイートキャンペーン\", RT_count = 5000):\n",
    "        self.oseti_analyzer = oseti.Analyzer()  #極性判定\n",
    "        self.CK = config.CONSUMER_KEY\n",
    "        self.CS = config.CONSUMER_SECRET\n",
    "        self.AT = config.ACCESS_TOKEN\n",
    "        self.AS = config.ACCESS_TOKEN_SECRET\n",
    "        self.ew = exclud_words\n",
    "        self.print_rep = print_rep\n",
    "        self.rt = str(RT_count)\n",
    "        self.columns = [\n",
    "            \"Id\", \"Date\", \"Name\", \"Full_text\",\n",
    "            \"Judge\", \"Posi_score\", \"Nega_score\", \"Followers\", \"link\"\n",
    "        ]\n",
    "        self.posi_pd = pd.DataFrame([], columns = self.columns)\n",
    "        self.nega_pd = pd.DataFrame([], columns = self.columns)\n",
    "        self.fire_pd = pd.DataFrame([], columns = self.columns)\n",
    "        self.wait = 0\n",
    "        self.reload = reload\n",
    "        day = datetime.strptime(day, '%Y-%m-%d')\n",
    "        self.day = day.strftime('%Y-%m-%d')\n",
    "\n",
    "    def main(self):\n",
    "        self._Make_Dir() # データ格納ファイルの準備\n",
    "\n",
    "        #ツイートを取得、センチメント判定\n",
    "        try:\n",
    "            status = self.Get_Buzz() #バズったツイート取得\n",
    "            for i in status:            \n",
    "                if self.wait == 10:\n",
    "                    print(\"10回待機したため終了\")\n",
    "                    break\n",
    "                self.Status(i)\n",
    "                if self.Exclude_Word(self.buzz_full_text) == True:# 除外ワードを含むツイートは除外\n",
    "                    continue\n",
    "                if self.Text_Count() == True: #30W以下のツイートは除外\n",
    "                    continue\n",
    "                self.Get_Rep() #リプライを取得\n",
    "                self.Get_RT() #RTコメントを取得\n",
    "                if self.Min_Rep() == False: # コメントが少ないツイートは除外\n",
    "                    continue\n",
    "                self.Get_Senti() #コメントをセンチメント判定\n",
    "                self._Get_Analysis() #ツイートをセンチメント判定\n",
    "        #エラー時はスキップして次のツイート取得\n",
    "        except (ValueError,  KeyError, TypeError, tweepy.TweepError) as e:\n",
    "            pass\n",
    "        #リクエスト回数が上限に達した場合はリセット時間まで待機して継続\n",
    "        except tweepy.RateLimitError as e:\n",
    "            if self.reload:\n",
    "                self.wait += 1\n",
    "                print(\"==========\")\n",
    "                print('get_buzzのリクエスト回数が上限に達しました。リセット時間まで待機')\n",
    "                print('Wait 15min...')\n",
    "                print()\n",
    "                for _ in tqdm(range(15 * 60)):\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        #生成したデータをprint\n",
    "        print()\n",
    "        print(\"↓↓↓positiveサンプル↓↓↓\")\n",
    "        display(self.posi_pd.head())\n",
    "        print()            \n",
    "        print(\"↓↓↓negativeサンプル↓↓↓\")\n",
    "        display(self.nega_pd.head())\n",
    "        print()\n",
    "        print(\"↓↓↓fire_tweetサンプル↓↓↓\")\n",
    "        display(self.fire_pd.head())\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        #生成したPandasDataFrameをcsvで書き出す\n",
    "        total_pd = pd.concat([self.posi_pd, self.nega_pd, self.fire_pd], ignore_index=True)\n",
    "        buzz_old = pd.read_csv('./output/buzz_tweet.csv')\n",
    "        buzz_new = pd.concat([buzz_old, total_pd])#既存データと連結\n",
    "        buzz_new.drop_duplicates(subset=\"Id\",inplace=True)#重複ID行を削除            \n",
    "        buzz_new.to_csv('./output/buzz_tweet.csv', index = False, header = True)\n",
    "        print(\"csvへの書き出しが完了しました。新規データ数{}、全データ数：{}\".format(len(buzz_new) - len(buzz_old), len(buzz_new)))\n",
    "        print(\"サンプルが0件の場合は、15分後に再度実行すると取得できる場合があります。\") \n",
    "        print(\"fire_tweetは出現率が非常に低いです。\")\n",
    "\n",
    "    #Api認証\n",
    "    def _Auth(self):\n",
    "        auth = tweepy.OAuthHandler(self.CK, self.CS)\n",
    "        auth.set_access_token(self.AT, self.AS)\n",
    "        api = tweepy.API(auth)\n",
    "        return api\n",
    "\n",
    "    #出力用ディレクトリとcsvファイルを作成（存在しない場合のみ）\n",
    "    def _Make_Dir(self):\n",
    "        new_dir_path = 'output'\n",
    "        try:\n",
    "            os.makedirs(new_dir_path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        if (os.path.isfile('./output/buzz_tweet.csv')) == False:\n",
    "            self.posi_pd.to_csv('./output/buzz_tweet.csv', index = False)  \n",
    "\n",
    "    #絵文字削除\n",
    "    def _remove_emoji(self, text):\n",
    "        return ''.join(c for c in text if c not in emoji.UNICODE_EMOJI)\n",
    "\n",
    "    #テキストを正規表現処理、絵文字削除\n",
    "    def _format_text(self, text):\n",
    "        text=re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", text)\n",
    "        text=re.sub('\\n', \"\", text)\n",
    "        text=re.sub(r'@?[!-~]+', \"\", text)\n",
    "        text=self._remove_emoji(text)\n",
    "        return text\n",
    "    \n",
    "    #　日付表記を整える、日本時間に修正\n",
    "    def _date_format(self, date):\n",
    "        date = datetime.strptime(str(date), '%a %b %d %H:%M:%S %z %Y')\n",
    "        date = date + timedelta(hours=9)\n",
    "        return datetime.strftime(date, '%Y-%m-%d %H:%M')\n",
    "\n",
    "    def Status(self, status): \n",
    "        self.buzz_id = status._json['id']\n",
    "        self.buzz_id_str = status._json['id_str']\n",
    "        self.buzz_name = status._json['user']['screen_name']\n",
    "        self.buzz_full_text = status._json['full_text']\n",
    "        self.date = status._json['created_at']\n",
    "        self.date = self._date_format(self.date)\n",
    "        self.favo = status._json['favorite_count']\n",
    "        self.rt_count = status._json['retweet_count']\n",
    "        api = self._Auth()\n",
    "        self.followers = status._json['user']['followers_count']\n",
    "        #self.followers = len(api.followers(status._json['user']['screen_name']))\n",
    "    \n",
    "    #除外ワード\n",
    "    def Exclude_Word(self, text):                        \n",
    "        if self.ew in str(text):\n",
    "            print(\"==========\")\n",
    "            print(\"除外ワード\")\n",
    "            print()\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    #ツイート内にリンクがあれば分割\n",
    "    def Text_Count(self):\n",
    "        if re.search(\"(https://t.co/\\w+)\", self.buzz_full_text) == None:\n",
    "            self.link = None\n",
    "        else:                   \n",
    "            self.buzz_full_text = re.split(\"(https://t.co/\\w+)\", self.buzz_full_text)\n",
    "            self.link = self.buzz_full_text[1]\n",
    "            self.buzz_full_text = self.buzz_full_text[0]\n",
    "        if len(self.buzz_full_text) < 30:\n",
    "            return True\n",
    "\n",
    "    #リプライ＋引用RTコメントが100未満のツイートは除外\n",
    "    def Min_Rep(self):\n",
    "        reply_texts_rows = []\n",
    "        if self.rep_cnt + self.RTcomme_cnt > 100:\n",
    "            reply_texts_rows.append(self.rep_row)\n",
    "            reply_texts_rows.append(self.rt_row)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    #sentiment_listを一次元にし、ツイートごとの極性表現の総和の辞書にする\n",
    "    def Get_Senti(self):\n",
    "        self.sentiment_list = sum(self.sentiment_list, [])#１次元にする\n",
    "        self.sentiment = dict((key, sum(d[key] for d in self.sentiment_list)) for key in self.sentiment_list[0])\n",
    "\n",
    "    #バズったツイートを取得(デフォルト：5000RT以上)\n",
    "    def Get_Buzz(self):\n",
    "        api = self._Auth()\n",
    "        try:\n",
    "            status = api.search(q = 'filter:safe min_retweets:' + self.rt + ' exclude:retweets until:' + self.day,\n",
    "                lang ='ja', count =100, tweet_mode = 'extended', result_type = 'recent')\n",
    "            return status\n",
    "        #エラー時はスキップして次のツイート取得\n",
    "        except (ValueError,  KeyError) as e:\n",
    "            pass\n",
    "        #リクエスト回数が上限に達した場合はリセット時間まで待機して継続\n",
    "        except (tweepy.RateLimitError, tweepy.TweepError) as e:\n",
    "            if self.reload:\n",
    "                self.wait += 1\n",
    "                print(\"==========\")\n",
    "                print('get_buzzのリクエスト回数が上限に達しました。リセット時間まで待機')\n",
    "                print('Wait 15min...')\n",
    "                print()\n",
    "                for _ in tqdm(range(15 * 60)):\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                pass\n",
    "        #return status\n",
    "    \n",
    "    #リプライを取得\n",
    "    def Get_Rep(self):\n",
    "        api = self._Auth()     \n",
    "        query_reply = '@' + self.buzz_name + ' exclude:retweets'\n",
    "        self.rep_row = []\n",
    "        self.sentiment_list = []\n",
    "        self.rep_cnt =0\n",
    "        wait_cnt = 0\n",
    "        try:\n",
    "            for status_reply in api.search(q=query_reply, lang='ja', count=100):\n",
    "                if status_reply._json['in_reply_to_status_id_str'] == self.buzz_id_str:\n",
    "                    row = self._format_text(status_reply._json['text'])\n",
    "                    #極性判定\n",
    "                    sentiment_score = self.oseti_analyzer.count_polarity(str(row))#strにする\n",
    "                    self.sentiment_list.append(sentiment_score)\n",
    "                    self.rep_row.append(row)\n",
    "                    self.rep_cnt += 1\n",
    "                else:\n",
    "                    pass\n",
    "        #エラーはスキップして次のツイート取得\n",
    "        except (ValueError,  KeyError, tweepy.TweepError) as e:\n",
    "            pass\n",
    "        #リクエスト回数が上限に達した場合はリセット時間まで待機して継続\n",
    "        except tweepy.RateLimitError as e:\n",
    "            self.wait += 1\n",
    "            if self.reload:\n",
    "                print(\"==========\")\n",
    "                print('get_repのリクエスト回数が上限に達しました。リセット時間まで待機')\n",
    "                print('Wait 15min...')\n",
    "                print()\n",
    "                for _ in tqdm(range(15 * 60)):\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    # 引用RTを取得\n",
    "    def Get_RT(self):\n",
    "        api = self._Auth()\n",
    "        query_quote = self.buzz_id_str + ' exclude:retweets'\n",
    "        self.RTcomme_cnt = 0\n",
    "        self.rt_row = []\n",
    "        try:\n",
    "            for status_quote in api.search(q=query_quote, lang='ja', count=100):\n",
    "                if status_quote._json['id_str'] == self.buzz_id_str:\n",
    "                    continue\n",
    "                else:\n",
    "                    row = self._format_text(status_quote._json['text'])\n",
    "                #極性判定\n",
    "                sentiment_score = self.oseti_analyzer.count_polarity(str(row))#strにする\n",
    "                self.sentiment_list.append(sentiment_score)\n",
    "                self.rt_row.append(row)\n",
    "                self.RTcomme_cnt += 1\n",
    "        #エラーはスキップして次のツイート取得\n",
    "        except (ValueError,  KeyError, tweepy.TweepError) as e:\n",
    "            pass\n",
    "        #リクエスト回数が上限に達した場合はリセット時間まで待機して継続\n",
    "        except tweepy.RateLimitError as e:\n",
    "            self.wait += 1\n",
    "            if self.reload:\n",
    "                print(\"==========\")\n",
    "                print('get_rtのリクエスト回数が上限に達しました。リセット時間まで待機')\n",
    "                print('Wait 15min...')\n",
    "                print()\n",
    "                for _ in tqdm(range(15 * 60)):\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                pass        \n",
    "\n",
    "    #取得したTweetをprint\n",
    "    def _Print(self):\n",
    "        print(\"name：\", self.buzz_name, \"／フォロワー数：\", self.followers)\n",
    "        print(\"date：\", self.date, \"／ツイートID：\", self.buzz_id_str)\n",
    "        print(\"RT数：\", self.rt_count, \"／favorite数：\", self.favo)\n",
    "        print(\"リプライ数：\", self.rep_cnt, \"／RTコメント数(上限１００）：\", self.RTcomme_cnt)\n",
    "        if self.print_rep == True:\n",
    "            print(\"リプライ\\n\", self.rep_row)\n",
    "            print(\"RTコメント\\n\", self.rt_row)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    #センチメント判定結果を取得\n",
    "    def _Get_Analysis(self):\n",
    "        total = self.sentiment[\"positive\"] + self.sentiment[\"negative\"]\n",
    "        if self.sentiment[\"positive\"] >= self.sentiment[\"negative\"]:\n",
    "            print(\"==========\")\n",
    "            print(self.buzz_full_text)\n",
    "            print()\n",
    "            print(\"【判定:positive】　　極性表現数\", self.sentiment)\n",
    "            self._Print()\n",
    "            s = pd.Series([self.buzz_id, self.date, self.buzz_name, self.buzz_full_text, \"positive\", self.sentiment[\"positive\"], self.sentiment[\"negative\"], self.followers, self.link], index = self.columns)\n",
    "            self.posi_pd = self.posi_pd.append(s, ignore_index=True)\n",
    "        elif self.sentiment[\"negative\"]/total >= 0.7:\n",
    "            print(\"==========\")\n",
    "            print(self.buzz_full_text)\n",
    "            print()\n",
    "            print(\"【判定:fire!!!】　　極性表現数\", self.sentiment)\n",
    "            print(\"ネガ表現の割合{:.3g}\".format(self.sentiment[\"negative\"]/total))\n",
    "            self._Print()\n",
    "            s = pd.Series([self.buzz_id, self.date, self.buzz_name, self.buzz_full_text, \"fire\", self.sentiment[\"positive\"], self.sentiment[\"negative\"], self.followers, self.link], index = self.columns)\n",
    "            self.fire_pd = self.fire_pd.append(s, ignore_index=True)\n",
    "        else:\n",
    "            print(\"==========\")\n",
    "            print(self.buzz_full_text)\n",
    "            print()\n",
    "            print(\"【判定:negative】　　極性表現数\", self.sentiment)\n",
    "            self._Print()\n",
    "            s = pd.Series([self.buzz_id, self.date, self.buzz_name, self.buzz_full_text, \"negative\", self.sentiment[\"positive\"], self.sentiment[\"negative\"], self.followers, self.link], index = self.columns)\n",
    "            self.nega_pd = self.nega_pd.append(s, ignore_index=True)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ツイートデータセット取得　実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "このスイマーバという商品が存在してくれたことに夫婦揃って心から感謝している。自由に手足を動かしまくれることを心から楽しんでいる。１日の中で明らかに最もイキイキとする時間。装着したとたんに大興奮する。ありがとうスイマーバ（のぼせさせないためにも目は絶対離してはダメ）。 \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 128, 'negative': 68}\n",
      "name： 7_color_world ／フォロワー数： 8529\n",
      "date： 2020-01-15 07:47 ／ツイートID： 1217216822463746049\n",
      "RT数： 5926 ／favorite数： 30745\n",
      "リプライ数： 9 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "現役保育士だが、\n",
      "つるの氏はともかく\n",
      "『私は◯人育児してきたからプロフェッショナルよ！』と豪語できる人はむしろ保育現場向きではない。\n",
      "\n",
      "我が子の成功論は必ずしも他の子には通用しない。むしろ邪魔になることもある。大事なのは目の前のこどもに真摯に向き合い続けること。\n",
      "→\n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 169, 'negative': 125}\n",
      "name： jiyuunaokan ／フォロワー数： 9597\n",
      "date： 2020-01-15 07:42 ／ツイートID： 1217215495729844224\n",
      "RT数： 5028 ／favorite数： 17391\n",
      "リプライ数： 16 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "ネット・ゲーム依存症より遥かに深刻なのが大人の会社勤め依存症。週5〜7日，一回8時間以上(14時間以上という例も)も時間を費やしてしまい、生活に支障が出ている例も少なくないことから、\n",
      "法整備を含め、政府や厚生労働省などの動きが加速することを期待する。\n",
      "\n",
      "【判定:negative】　　極性表現数 {'positive': 101, 'negative': 181}\n",
      "name： kzmakino ／フォロワー数： 4159\n",
      "date： 2020-01-15 06:40 ／ツイートID： 1217199885881004032\n",
      "RT数： 16044 ／favorite数： 24711\n",
      "リプライ数： 49 ／RTコメント数(上限１００）： 80\n",
      "\n",
      "==========\n",
      "AAAが僕にくれたもの。\n",
      "\n",
      "「Attack All Around」\n",
      "\n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 141, 'negative': 37}\n",
      "name： WJF_SHIROSE ／フォロワー数： 118153\n",
      "date： 2020-01-15 06:35 ／ツイートID： 1217198528297963520\n",
      "RT数： 13402 ／favorite数： 37833\n",
      "リプライ数： 1 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "本日1月15日は #いちごの日 🍓💕\n",
      "という訳で念願の #苺の家系図、完成したので公開しまーす。クレジット消さなければご自由にお使い頂いてOKです🍓＼(^o^)／🍓 でも連絡くれたらこのｱｶで宣伝するのでよろしくお願いします。全ての #苺 好きさんへ捧げます！\n",
      "#苺の断面図 \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 80, 'negative': 33}\n",
      "name： danmenzukan ／フォロワー数： 330\n",
      "date： 2020-01-15 05:48 ／ツイートID： 1217186712759128064\n",
      "RT数： 5308 ／favorite数： 8680\n",
      "リプライ数： 35 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "今回のアップデートからジブラルタルがドーム内で味方を蘇生するとモーションが変化し、蘇生が速くなりました🦄\n",
      "\n",
      "#ApexLegends #エーペックスレジェンズ \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 29, 'negative': 17}\n",
      "name： ApexTimes ／フォロワー数： 47914\n",
      "date： 2020-01-15 04:23 ／ツイートID： 1217165290003263489\n",
      "RT数： 5280 ／favorite数： 11030\n",
      "リプライ数： 2 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "【AAAから大切なお知らせ】\n",
      "AAAからファンクラブ会員の皆さまに大切なお知らせがございます。\n",
      "※システムの都合上、深夜のメルマガ配信の対応が致しかねるため、このようなお知らせとなり誠に申し訳ございません。\n",
      "\n",
      "■大切なお知らせ\n",
      "\n",
      "\n",
      "【判定:negative】　　極性表現数 {'positive': 60, 'negative': 63}\n",
      "name： AAA_staff ／フォロワー数： 625344\n",
      "date： 2020-01-15 00:04 ／ツイートID： 1217100137668870144\n",
      "RT数： 11155 ／favorite数： 41819\n",
      "リプライ数： 2 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "成人式にがしゃどくろの着物は変じゃない\n",
      "髑髏＝悪と取るのは西洋の感覚\n",
      "和装における髑髏は再生と魔除け\n",
      "江戸時代まで武装に好まれ使われている\n",
      "かつ有名な図案は将門公の息女が父の仇を討つために召喚した怪異で\n",
      "親孝行の証でもあるから\n",
      "新たな旅立ちと親への感謝を伝える意味では相応しい柄でもある \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 131, 'negative': 68}\n",
      "name： sabineko69 ／フォロワー数： 377\n",
      "date： 2020-01-14 23:40 ／ツイートID： 1217094213000523776\n",
      "RT数： 9938 ／favorite数： 20996\n",
      "リプライ数： 26 ／RTコメント数(上限１００）： 86\n",
      "\n",
      "==========\n",
      "「自衛隊に『行くな』と言う人はいても、誰もタンカーに『行くな』と言わないのはおかしいじゃないか」\n",
      "\n",
      "これに尽きる。\n",
      "#primenews \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 121, 'negative': 87}\n",
      "name： xAegvg0JipIY0hD ／フォロワー数： 3474\n",
      "date： 2020-01-14 23:14 ／ツイートID： 1217087581638119425\n",
      "RT数： 5027 ／favorite数： 14238\n",
      "リプライ数： 47 ／RTコメント数(上限１００）： 99\n",
      "\n",
      "==========\n",
      "【発売日変更のお知らせ】\n",
      "3/3(火)発売を予定しておりました『FINAL FANTASY VII REMAKE』ですが、4/10(金）へ発売日を変更させていただきます。\n",
      "発売をお待ちいただいている皆様には大変ご迷惑をおかけいたしますことを、深くお詫び申し上げます。#FF7R \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 122, 'negative': 73}\n",
      "name： FFVIIR_CLOUD ／フォロワー数： 82113\n",
      "date： 2020-01-14 23:01 ／ツイートID： 1217084206976622593\n",
      "RT数： 44769 ／favorite数： 35092\n",
      "リプライ数： 66 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "今シーズンも声優の佐倉綾音さんが展示会に来て下さり動画を撮らせて頂いたのですが、以前にも増してあざとすぎるため恐らくツイッターが壊れると思います。\n",
      "\n",
      "ご迷惑をお掛けしますこと心より深くお詫び申し上げます。 \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 59, 'negative': 32}\n",
      "name： MatsuiRyosuke ／フォロワー数： 111539\n",
      "date： 2020-01-14 22:44 ／ツイートID： 1217080083518590977\n",
      "RT数： 14222 ／favorite数： 50178\n",
      "リプライ数： 3 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "東京都国分寺にある「eggg cafe」の、中には濃厚なカスタードが入り、表面を香ばしくブリュレした「ブリュレパンケーキ」✨\n",
      "\n",
      "高品質の「幸せなたまご」を使用し、カラメルソースをかければ至極の一品です！ \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 50, 'negative': 8}\n",
      "name： sweetroad5 ／フォロワー数： 375703\n",
      "date： 2020-01-14 22:33 ／ツイートID： 1217077171774087169\n",
      "RT数： 6611 ／favorite数： 29725\n",
      "リプライ数： 6 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "色んな国でクロワッサンを必ず食べる友達、大のクロワッサン好きというわけではなく「美味しいクロワッサンに出会ったことがないので出会える日までクロワッサンを食べ続ける」というスタンスで本当に応援しているが、私の中のダイソーの店員が フランスになければないですねと冷たい目を向けている\n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 195, 'negative': 44}\n",
      "name： osukisushi ／フォロワー数： 7359\n",
      "date： 2020-01-14 22:25 ／ツイートID： 1217075152770355206\n",
      "RT数： 13737 ／favorite数： 73129\n",
      "リプライ数： 66 ／RTコメント数(上限１００）： 73\n",
      "\n",
      "==========\n",
      "佐藤健の新ドラマ、天堂という名前で魔王と呼ばれる佐藤健に小児科のナースを志す渡邉圭祐で平成が大渋滞してる。 \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 44, 'negative': 25}\n",
      "name： mtmtSF ／フォロワー数： 3119\n",
      "date： 2020-01-14 22:11 ／ツイートID： 1217071696886153216\n",
      "RT数： 10688 ／favorite数： 19391\n",
      "リプライ数： 51 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "WANIMAのオタクくん構文、本人達が結構嫌がってるらしく、Twitterに生息するオタクの悪乗りのせいで全然過失が無い本人達が気に病んでるの気の毒過ぎるなと思っている。\n",
      "\n",
      "【判定:negative】　　極性表現数 {'positive': 76, 'negative': 119}\n",
      "name： Ryang_Fang4 ／フォロワー数： 1197\n",
      "date： 2020-01-14 22:02 ／ツイートID： 1217069457874407431\n",
      "RT数： 6535 ／favorite数： 18847\n",
      "リプライ数： 43 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "2020年4月22日(水)発売のビジュアルブック📖撮影メイキング Short ver.を公開🎥\n",
      "今回は、ヨコハマ・ディビジョン“MAD TRIGGER CREW“をご紹介！\n",
      "今後モバイルサイト内にてLong ver.も公開予定なので、是非この機会にモバイルサイトにご登録ください☑\n",
      "\n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 67, 'negative': 37}\n",
      "name： hm_rtstage ／フォロワー数： 56577\n",
      "date： 2020-01-14 22:00 ／ツイートID： 1217068849037746176\n",
      "RT数： 8222 ／favorite数： 20569\n",
      "リプライ数： 21 ／RTコメント数(上限１００）： 90\n",
      "\n",
      "==========\n",
      "たまたま見つけたんですけど、意味がわからなくて戻って写真撮った。腐葉土担ぎ放題ってなんかの隠語なの？？教えてくれ、腐葉土担ぎ放題になんのメリットがあるのか \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 59, 'negative': 31}\n",
      "name： rui_sola ／フォロワー数： 389\n",
      "date： 2020-01-14 21:58 ／ツイートID： 1217068493385789440\n",
      "RT数： 16808 ／favorite数： 29963\n",
      "リプライ数： 53 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "==========\n",
      "【希少】地球上でもっとも速く移動出来るファルコン、飛行形態が美しすぎると話題に。 \n",
      "\n",
      "【判定:positive】　　極性表現数 {'positive': 36, 'negative': 9}\n",
      "name： nakamanian ／フォロワー数： 99063\n",
      "date： 2020-01-14 21:41 ／ツイートID： 1217064257277153282\n",
      "RT数： 7034 ／favorite数： 44517\n",
      "リプライ数： 4 ／RTコメント数(上限１００）： 100\n",
      "\n",
      "\n",
      "↓↓↓positiveサンプル↓↓↓\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Full_text</th>\n",
       "      <th>Judge</th>\n",
       "      <th>Posi_score</th>\n",
       "      <th>Nega_score</th>\n",
       "      <th>Followers</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1217216822463746049</td>\n",
       "      <td>2020-01-15 07:47</td>\n",
       "      <td>7_color_world</td>\n",
       "      <td>このスイマーバという商品が存在してくれたことに夫婦揃って心から感謝している。自由に手足を動か...</td>\n",
       "      <td>positive</td>\n",
       "      <td>128</td>\n",
       "      <td>68</td>\n",
       "      <td>8529</td>\n",
       "      <td>https://t.co/6r8g80JmlE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1217215495729844224</td>\n",
       "      <td>2020-01-15 07:42</td>\n",
       "      <td>jiyuunaokan</td>\n",
       "      <td>現役保育士だが、\\nつるの氏はともかく\\n『私は◯人育児してきたからプロフェッショナルよ！』...</td>\n",
       "      <td>positive</td>\n",
       "      <td>169</td>\n",
       "      <td>125</td>\n",
       "      <td>9597</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1217198528297963520</td>\n",
       "      <td>2020-01-15 06:35</td>\n",
       "      <td>WJF_SHIROSE</td>\n",
       "      <td>AAAが僕にくれたもの。\\n\\n「Attack All Around」\\n</td>\n",
       "      <td>positive</td>\n",
       "      <td>141</td>\n",
       "      <td>37</td>\n",
       "      <td>118153</td>\n",
       "      <td>https://t.co/pGSiZ3FNN8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1217186712759128064</td>\n",
       "      <td>2020-01-15 05:48</td>\n",
       "      <td>danmenzukan</td>\n",
       "      <td>本日1月15日は #いちごの日 🍓💕\\nという訳で念願の #苺の家系図、完成したので公開しま...</td>\n",
       "      <td>positive</td>\n",
       "      <td>80</td>\n",
       "      <td>33</td>\n",
       "      <td>330</td>\n",
       "      <td>https://t.co/CUT2RuGX0D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1217165290003263489</td>\n",
       "      <td>2020-01-15 04:23</td>\n",
       "      <td>ApexTimes</td>\n",
       "      <td>今回のアップデートからジブラルタルがドーム内で味方を蘇生するとモーションが変化し、蘇生が速く...</td>\n",
       "      <td>positive</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>47914</td>\n",
       "      <td>https://t.co/FXPQsOk2gV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id              Date           Name  \\\n",
       "0  1217216822463746049  2020-01-15 07:47  7_color_world   \n",
       "1  1217215495729844224  2020-01-15 07:42    jiyuunaokan   \n",
       "2  1217198528297963520  2020-01-15 06:35    WJF_SHIROSE   \n",
       "3  1217186712759128064  2020-01-15 05:48    danmenzukan   \n",
       "4  1217165290003263489  2020-01-15 04:23      ApexTimes   \n",
       "\n",
       "                                           Full_text     Judge Posi_score  \\\n",
       "0  このスイマーバという商品が存在してくれたことに夫婦揃って心から感謝している。自由に手足を動か...  positive        128   \n",
       "1  現役保育士だが、\\nつるの氏はともかく\\n『私は◯人育児してきたからプロフェッショナルよ！』...  positive        169   \n",
       "2              AAAが僕にくれたもの。\\n\\n「Attack All Around」\\n  positive        141   \n",
       "3  本日1月15日は #いちごの日 🍓💕\\nという訳で念願の #苺の家系図、完成したので公開しま...  positive         80   \n",
       "4  今回のアップデートからジブラルタルがドーム内で味方を蘇生するとモーションが変化し、蘇生が速く...  positive         29   \n",
       "\n",
       "  Nega_score Followers                     link  \n",
       "0         68      8529  https://t.co/6r8g80JmlE  \n",
       "1        125      9597                     None  \n",
       "2         37    118153  https://t.co/pGSiZ3FNN8  \n",
       "3         33       330  https://t.co/CUT2RuGX0D  \n",
       "4         17     47914  https://t.co/FXPQsOk2gV  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "↓↓↓negativeサンプル↓↓↓\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Full_text</th>\n",
       "      <th>Judge</th>\n",
       "      <th>Posi_score</th>\n",
       "      <th>Nega_score</th>\n",
       "      <th>Followers</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1217199885881004032</td>\n",
       "      <td>2020-01-15 06:40</td>\n",
       "      <td>kzmakino</td>\n",
       "      <td>ネット・ゲーム依存症より遥かに深刻なのが大人の会社勤め依存症。週5〜7日，一回8時間以上(1...</td>\n",
       "      <td>negative</td>\n",
       "      <td>101</td>\n",
       "      <td>181</td>\n",
       "      <td>4159</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1217100137668870144</td>\n",
       "      <td>2020-01-15 00:04</td>\n",
       "      <td>AAA_staff</td>\n",
       "      <td>【AAAから大切なお知らせ】\\nAAAからファンクラブ会員の皆さまに大切なお知らせがございま...</td>\n",
       "      <td>negative</td>\n",
       "      <td>60</td>\n",
       "      <td>63</td>\n",
       "      <td>625344</td>\n",
       "      <td>https://t.co/QOtrdorAnH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1217069457874407431</td>\n",
       "      <td>2020-01-14 22:02</td>\n",
       "      <td>Ryang_Fang4</td>\n",
       "      <td>WANIMAのオタクくん構文、本人達が結構嫌がってるらしく、Twitterに生息するオタクの...</td>\n",
       "      <td>negative</td>\n",
       "      <td>76</td>\n",
       "      <td>119</td>\n",
       "      <td>1197</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id              Date         Name  \\\n",
       "0  1217199885881004032  2020-01-15 06:40     kzmakino   \n",
       "1  1217100137668870144  2020-01-15 00:04    AAA_staff   \n",
       "2  1217069457874407431  2020-01-14 22:02  Ryang_Fang4   \n",
       "\n",
       "                                           Full_text     Judge Posi_score  \\\n",
       "0  ネット・ゲーム依存症より遥かに深刻なのが大人の会社勤め依存症。週5〜7日，一回8時間以上(1...  negative        101   \n",
       "1  【AAAから大切なお知らせ】\\nAAAからファンクラブ会員の皆さまに大切なお知らせがございま...  negative         60   \n",
       "2  WANIMAのオタクくん構文、本人達が結構嫌がってるらしく、Twitterに生息するオタクの...  negative         76   \n",
       "\n",
       "  Nega_score Followers                     link  \n",
       "0        181      4159                     None  \n",
       "1         63    625344  https://t.co/QOtrdorAnH  \n",
       "2        119      1197                     None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "↓↓↓fire_tweetサンプル↓↓↓\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Full_text</th>\n",
       "      <th>Judge</th>\n",
       "      <th>Posi_score</th>\n",
       "      <th>Nega_score</th>\n",
       "      <th>Followers</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, Date, Name, Full_text, Judge, Posi_score, Nega_score, Followers, link]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "csvへの書き出しが完了しました。新規データ数18、全データ数：514\n",
      "サンプルが0件の場合は、15分後に再度実行すると取得できる場合があります。\n",
      "fire_tweetは出現率が非常に低いです。\n"
     ]
    }
   ],
   "source": [
    "# 指定日のツイートを取得（API制限のため取得できるのは約一週間前のものまで）\n",
    "day = '2020-1-16'\n",
    "# リクエスト制限対応：True:リクエスト上限に達したら15分待機ののちツイート取得続行/ False:待機せずcsv取得\n",
    "reload = True\n",
    "\n",
    "#除外ワード\n",
    "exclud_words = \"配信スタート ＃キャンペーン　リツイートキャンペーン WWWWWWWWW wwwwwwwwww\"\n",
    "\n",
    "#その他設定可能パラメータ\n",
    "#リプライをprint（print_rep = True/Fals), 最低RT数(RT_count = 5000)\n",
    "\n",
    "GT = Get_Twitter(day, reload, exclud_words)\n",
    "GT.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ２）データセットの前処理\n",
    "　・正規表現、ストップワード除去など\n",
    " \n",
    " #### 参考サイト\n",
    "Pythonで全角・半角記号をまとめて消し去る<br>　http://prpr.hatenablog.jp/entry/2016/11/23/Python%E3%81%A7%E5%85%A8%E8%A7%92%E3%83%BB%E5%8D%8A%E8%A7%92%E8%A8%98%E5%8F%B7%E3%82%92%E3%81%BE%E3%81%A8%E3%82%81%E3%81%A6%E6%B6%88%E3%81%97%E5%8E%BB%E3%82%8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.6/site-packages (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.6/site-packages (from gensim) (1.13.3)\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim) (0.19.1)\r\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim) (1.11.0)\r\n",
      "Requirement already satisfied: smart_open>=1.2.1 in /opt/conda/lib/python3.6/site-packages (from gensim) (1.5.3)\r\n",
      "Requirement already satisfied: boto>=2.32 in /opt/conda/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim) (2.48.0)\r\n",
      "Requirement already satisfied: bz2file in /opt/conda/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim) (0.98)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim) (2.18.4)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->smart_open>=1.2.1->gensim) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->smart_open>=1.2.1->gensim) (2.6)\r\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->smart_open>=1.2.1->gensim) (1.22)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->smart_open>=1.2.1->gensim) (2018.1.18)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natto-py\r\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/14/1d4258247a00b7b8a115563effb1d0bd30501d69580629d36593ce0af92d/natto-py-0.9.2.tar.gz\r\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.6/site-packages (from natto-py) (1.10.0)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi->natto-py) (2.18)\r\n",
      "Building wheels for collected packages: natto-py\r\n",
      "  Building wheel for natto-py (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for natto-py: filename=natto_py-0.9.2-cp36-none-any.whl size=50571 sha256=65c64e8f0f386ee150bb2ab7d8bbef980b44df48abc5c100e20583126ac1b3d0\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/51/dd/67f87608b124a23eecf5c1fc3557cc0b7ffdeae33fe6ee89df\r\n",
      "Successfully built natto-py\r\n",
      "Installing collected packages: natto-py\r\n",
      "Successfully installed natto-py-0.9.2\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.6/site-packages (0.5.4)\r\n"
     ]
    }
   ],
   "source": [
    "#必要なツールをインストール(初回のみ実行)\n",
    "! pip install gensim\n",
    "! pip install natto-py\n",
    "! pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ツイートデータを学習用に整形\n",
    "#from natto import MeCab\n",
    "import MeCab\n",
    "import re\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import emoji\n",
    "import neologdn\n",
    "import urllib.request\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "class For_Model():\n",
    "    \n",
    "    def __init__(self, data, columns, out_file, mode, text, similar = None):\n",
    "        self.mecab = MeCab.Tagger(\"-Owakati\")\n",
    "        self.data = data\n",
    "        self.columns = columns\n",
    "        self.out_file = out_file\n",
    "        self.mode = mode\n",
    "        self.text = text\n",
    "        self.similar = str(similar)\n",
    "\n",
    "    #データを読み込む\n",
    "    def Load_tweets(self):        \n",
    "        df = pd.read_csv(self.data, usecols = self.columns)\n",
    "        print(\"読み込んだツイート\", df.shape[0])\n",
    "        \n",
    "        #３０w以下のtweet行を削除\n",
    "        index = []\n",
    "        for i in range(len(df)):\n",
    "            line = df.iloc[i]\n",
    "            text = str(line[\"Full_text\"])\n",
    "            text = re.sub('https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", text)\n",
    "            text = re.sub('http?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", text)\n",
    "            line[\"Full_text\"] = text\n",
    "            if len(text) < 30:\n",
    "                index.append(i)\n",
    "        df_tweet = df.drop(df.index[index])\n",
    "        df_tweet = df_tweet.reset_index(drop=True)\n",
    "        print(\"30w以下削除後のツイート数\", df_tweet.shape[0])\n",
    "        display(df_tweet.head())\n",
    "        \n",
    "        #判定用テキストをリストの最後に追加\n",
    "        tweets = []\n",
    "        for i in df_tweet[self.text]:\n",
    "            tweets.append(i)\n",
    "        if self.similar == None:\n",
    "            pass\n",
    "        else:\n",
    "            tweets.append(self.similar)\n",
    "        return df_tweet, tweets\n",
    "\n",
    "    def Stop_Words(self):\n",
    "        # ストップワードをダウンロード\n",
    "        url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "        urllib.request.urlretrieve(url, './output/stop_word.txt')\n",
    "\n",
    "        with open('./output/stop_word.txt', 'r', encoding='utf-8') as file:\n",
    "            stopwords = [word.replace('\\n', '') for word in file.readlines()]\n",
    "\n",
    "        #追加ストップワードを設定（助詞や意味のない平仮名１文字）\n",
    "        add_words = ['あ','い','う','え','お','か','き','く','け','こ','さ','し','す','せ','そ','た','ち','つ','て','と',\n",
    "                     'な','に','ぬ','ね','の','は','ひ','ふ','へ','ほ','ま','み','む','め','も','や','ゆ','よ',\n",
    "                     'ら','り','る','れ','ろ','わ','を','ん','が','ぎ','ぐ','げ','ご','ざ','じ','ず','ぜ','ぞ',\n",
    "                     'だ','ぢ','づ','で','ど','ば','び','ぶ','べ','ぼ','ぱ','ぴ','ぷ','ぺ','ぽ',\n",
    "                     'くん','です','ます','ました','そして','でも','だから','だが','くらい','その','それ','かも',\n",
    "                     'あれ','あの','あっ','そんな','この','これ','とか','とも','する','という','ござい',\n",
    "                     'ので','なんて','たら', 'られ','たい','さて','てる','ください','なる','けど','でし',\n",
    "                     'じゃん','だっ','なっ','でしょ', 'ある','って','こんな','ねえ'\n",
    "                    ]\n",
    "        stopwords = stopwords + add_words\n",
    "        return stopwords\n",
    "\n",
    "    def Tokenizer(self, text, stopwords):\n",
    "\n",
    "        words = []\n",
    "        text = self.mecab.parse(text)\n",
    "        text = text.split(' ')\n",
    "        for j in range(len(text)):\n",
    "            if text[j] not in stopwords:\n",
    "                words.append(text[j])\n",
    "        return words\n",
    "\n",
    "    def remove_emoji(self, text):\n",
    "        return ''.join(c for c in text if c not in emoji.UNICODE_EMOJI)\n",
    "\n",
    "    #記号削除\n",
    "    def format_text(self, text):\n",
    "        text = unicodedata.normalize(\"NFKC\", text)  # 全角記号を半角へ置換\n",
    "        # 記号を消し去るための魔法のテーブル作成\n",
    "        table = str.maketrans(\"\", \"\", string.punctuation  + \"「」、。・*`+-|?#!()\\[]<>=~/\")\n",
    "        text = text.translate(table)\n",
    "        return text\n",
    "\n",
    "    def main(self):\n",
    "        tweets_num = 0\n",
    "        stopwords = self.Stop_Words()\n",
    "        df_tweet, tweets = self.Load_tweets()\n",
    "        #ツイートを分かち書きしてcsvに出力(モード'a'はデータ追加、モード'w'は新規作成)\n",
    "        with open('./output/' + self.out_file, self.mode) as f:\n",
    "            for i in tweets:\n",
    "                tweets_num += 1\n",
    "                i = neologdn.normalize(i)\n",
    "                i = re.sub('\\n', \"\", i)\n",
    "                i = re.sub(r'[!-~]', \"\", i)#半角記号,数字,英字\n",
    "                i = re.sub(r'[︰-＠]', \"\", i)#全角記号\n",
    "                i = self.format_text(i)#記号削除\n",
    "                i = re.sub(r'[【】●ㅅ●Ф☆✩︎♡→←▼①②③④⑤『』ω《》∠∇∩♪∀◞ཀCщ≧≦ ́◤◢■◆★※↑↓〇◯○◎⇒▽◉Θ♫♬〃“”◇ᄉ⊂⊃д°]', \"\", i)\n",
    "                #i = re.sub(r'[!-~、。‥…？！〜「」｢｣:：“”【】※♪♩♫♬『』→↓↑《》〈〉[]≧∇≦・゜・●ㅅ●´Д´°ω°•ω•★＊☆♡（）✔Θ∀´∀｀˘ω˘‼бωб￣▽￣◉→←▼①②③④⑤]', \"\", i)\n",
    "                i = self.remove_emoji(i)\n",
    "                i = self.Tokenizer(i, stopwords)\n",
    "                i = ' '.join(i) #リストを文字列に変換\n",
    "                i = str(i)\n",
    "                f.write(i)\n",
    "\n",
    "        print('CSV出力完了：'+ self.out_file)\n",
    "        with open('./output/' + self.out_file) as f:\n",
    "             wakati = f.read()\n",
    "\n",
    "        print(\"学習用データに書き込んだツイート数（判定用ツイート含む）：\", tweets_num)\n",
    "        print()\n",
    "        print(\"分かち書きサンプル\\n\", wakati[:500])\n",
    "        return df_tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "読み込んだツイート (514, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30w以下削除後のツイート数 (459, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Followers</th>\n",
       "      <th>Full_text</th>\n",
       "      <th>Judge</th>\n",
       "      <th>Nega_score</th>\n",
       "      <th>Posi_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>絶対断らないと評判の病児保育室、助成金下りず2億円の赤字を出し閉鎖\\n\\n全国に2886カ所...</td>\n",
       "      <td>positive</td>\n",
       "      <td>148.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2217.0</td>\n",
       "      <td>全国の皆さんへ\\nどうか皆様のお力を貸してください。\\n\\n１日も早く娘を助けたいです。\\n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>50.0</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3756.0</td>\n",
       "      <td>o0( 歴史上、さんざん他国の料理を魔改造してきた我が国が「寿司ポリス」などどは片腹痛い！あ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>85.0</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10999.0</td>\n",
       "      <td>これすんごい。GABAのフォースリープ、ツイッターで人気だったから半信半疑で夜に3粒食べてみ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>78.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56685.0</td>\n",
       "      <td>TVアニメ「Ｄｒ．ＳＴＯＮＥ」第2期制作が決定いたしました！第1期をご視聴・応援くださってい...</td>\n",
       "      <td>positive</td>\n",
       "      <td>20.0</td>\n",
       "      <td>156.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Followers                                          Full_text     Judge  \\\n",
       "0     1564.0  絶対断らないと評判の病児保育室、助成金下りず2億円の赤字を出し閉鎖\\n\\n全国に2886カ所...  positive   \n",
       "1     2217.0  全国の皆さんへ\\nどうか皆様のお力を貸してください。\\n\\n１日も早く娘を助けたいです。\\n...  positive   \n",
       "2     3756.0  o0( 歴史上、さんざん他国の料理を魔改造してきた我が国が「寿司ポリス」などどは片腹痛い！あ...  positive   \n",
       "3    10999.0  これすんごい。GABAのフォースリープ、ツイッターで人気だったから半信半疑で夜に3粒食べてみ...  positive   \n",
       "4    56685.0  TVアニメ「Ｄｒ．ＳＴＯＮＥ」第2期制作が決定いたしました！第1期をご視聴・応援くださってい...  positive   \n",
       "\n",
       "   Nega_score  Posi_score  \n",
       "0       148.0       168.0  \n",
       "1        50.0       129.0  \n",
       "2        85.0       137.0  \n",
       "3        78.0        97.0  \n",
       "4        20.0       156.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV出力完了：train_buzz.txt\n",
      "学習用データに書き込んだツイート数（判定用ツイート含む）： 460\n",
      "\n",
      "分かち書きサンプル\n",
      " 絶対 断ら ない 評判 病児保育 助成金 下り 赤字 出し 閉鎖 全国 病児保育 赤字 運営 おり 東海 キッズケア 助成金 求め 署名 集め 助成金 下りる あり ませ 社会保障 税金 使わ ませ \n",
      "全国 皆さん どうか 皆様 お力 貸し 早く 娘 助け 家族 揃っ 笑顔 クリスマス 新年 迎え 娘 目撃 情報 娘 繋がる 些細なこと 連絡 連絡先 大月 警察 暑 電話 もしくは 最寄り 警察 署 拡散希望 小倉 美咲 \n",
      "歴史 さんざん 他国 料理 魔 改造 我が国 寿司 ポリス どど 片腹痛い あらゆる 文化 文化 独自 解釈 いい イノベーション 生む 思っ しかし ヘルシンキ 〈 クリスマス トッピング • バナナ 巻き寿司 〉 寛容 試さ いる \n",
      "すん ごい フォースリープツイッター 人気 半信半疑 夜 粒 食べ 昨日 めちゃ ぐっすり 眠れ 例える なら 旅行 気持ち良く 遊び 疲れ 夜 睡眠 クオリティ しかも 味 まろやか ミルク チョコ 好み すぎる 目安 一日 粒 摂 れる \n",
      "アニメ 期 制作 決定 いたし 期 視聴 応援 くださっ いる 皆様 本当に ありがとう \n"
     ]
    }
   ],
   "source": [
    "#パラメータの設定\n",
    "\n",
    "#取得したデータのパス\n",
    "data = './output/buzz_tweet.csv'\n",
    "#取得したい列名\n",
    "columns = [\"Followers\", \"Full_text\",\"Posi_score\", \"Nega_score\",\"Judge\"]\n",
    "#出力ファイル名\n",
    "out_file = \"train_buzz.txt\"\n",
    "#学習データの保存モード　'a'：追加／'w'：上書き\n",
    "mode = 'w'\n",
    "#ツイートテキストの列を指定\n",
    "text = \"Full_text\"\n",
    "#判定させたいツイート予定文書（類似度確認のため、データセット内にあるツイート文を使用）\n",
    "similar = \"電車内で咳やくしゃみをしている人、マスクをしないのが信じられません。ウイルスをばら撒いている自覚がないのだろうか。頼むからマスクしてくれ。\"\n",
    "FM = For_Model(data, columns, out_file, mode, text, similar)\n",
    "df_tweet = FM.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ３）予測モデルを生成\n",
    "　・データセットをDoc２vecで学習<br>\n",
    "\n",
    "#### 参考サイト\n",
    "fastTextとDoc2Vecのモデルを作成してニュース記事の多クラス分類の精度を比較する<br> https://qiita.com/kazuki_hayakawa/items/ca5d4735b9514895e197<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc２vec文書ベクトル用モデルに学習させたツイート数 460\n"
     ]
    }
   ],
   "source": [
    "#Doc2Vecモデルの学習\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "f = open('./output/train_buzz.txt','r')#空白で単語を区切り、改行で文書を区切っているテキストデータ\n",
    "\n",
    "#１文書ずつ、単語に分割してリストに入れていく[([単語1,単語2,単語3],文書id),...]こんなイメージ\n",
    "#words：文書に含まれる単語のリスト（単語の重複あり）\n",
    "# tags：文書の識別子（リストで指定．1つの文書に複数のタグを付与できる）\n",
    "#fにテキスト データをいれる\n",
    "trainings = [TaggedDocument(words = data.split(),tags = [i]) for i,data in enumerate(f)]\n",
    "#print(type(trainings))\n",
    "print(\"Doc２vec文書ベクトル用モデルに学習させたツイート数\",len(trainings))\n",
    "# print(trainings[:20])\n",
    "\n",
    "#文書ベクトル用ツイートテキストの学習(パラメータ：dm=1:dmpv それ以外：DBow)\n",
    "model = Doc2Vec(\n",
    "    documents= trainings,\n",
    "    dm = 1,\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    alpha = 0.05,\n",
    "    min_count=1,\n",
    "    sample = 0,\n",
    "    workers=4, \n",
    "    epochs = 1000\n",
    ")\n",
    "\n",
    "#出力用ディレクトリ作成（存在しない場合のみ）\n",
    "def Make_Dir():\n",
    "    new_dir_path = 'model'\n",
    "    try:\n",
    "        os.makedirs(new_dir_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "# モデルのセーブ\n",
    "Make_Dir()\n",
    "model.save(\"./model/doc2vec.model\")\n",
    "\n",
    "# モデルのロード(モデルが用意してあれば、ここからで良い)\n",
    "m = Doc2Vec.load('./model/doc2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ４）ツイート予定文章のネガポジ予測を返す\n",
    "　・データセットから、入力しておいたツイート予定文書と似ている文書を探す<br>\n",
    "・ネガポジスコア付きで、類似ツイート上位１０個を返す<br>\n",
    "#### 結果：成功。入力文書と同じツイート文が類似度１位に。ネガポジもデータズレなく表示できた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== 判定したいツイート ===========\n",
      "\n",
      "電車内で咳やくしゃみをしている人、マスクをしないのが信じられません。ウイルスをばら撒いている自覚がないのだろうか。頼むからマスクしてくれ。\n",
      "\n",
      "======= 類似度上位１０（全460ツイート中） =======\n",
      "※nega/posiスコアはフォロワー数のlogで割っています。フォロワー数はばらつきが大きいので対数変換しています\n",
      "\n",
      "…………　類似ツイート1位：類似度 0.726　…………\n",
      "\n",
      "AAAが僕にくれたもの。\n",
      "\n",
      "「Attack All Around」\n",
      "\n",
      "\n",
      "【極性】： positive\n",
      "posi_score： 12.07 ／ nega_score： 3.16 ／followers： 118153\n",
      "\n",
      "…………　類似ツイート2位：類似度 0.698　…………\n",
      "\n",
      "そんな、まさか❗️　あいつは…\n",
      "\n",
      "@MorbiusMovieJP \n",
      "\n",
      "【極性】： positive\n",
      "posi_score： 4.82 ／ nega_score： 3.82 ／followers： 164741\n",
      "\n",
      "…………　類似ツイート3位：類似度 0.679　…………\n",
      "\n",
      "天皇ですら人間宣言したのにただの客が神様なわけねぇだろボケナス大逆罪共が\n",
      "\n",
      "【極性】： positive\n",
      "posi_score： 11.29 ／ nega_score： 6.83 ／followers： 835\n",
      "\n",
      "…………　類似ツイート4位：類似度 0.667　…………\n",
      "\n",
      "上手すぎだろwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww \n",
      "\n",
      "【極性】： positive\n",
      "posi_score： 11.45 ／ nega_score： 3.2 ／followers： 378\n",
      "\n",
      "…………　類似ツイート5位：類似度 0.657　…………\n",
      "\n",
      "マスクせずに咳してる人見ると、\n",
      "「感染者だ！撃ち殺せ！」\n",
      "「ですが！アイツはまだ人間じゃないですか！」\n",
      "「馬鹿野郎！ここで殺らなきゃ、俺達の大事な奴まで感染者にされる可能性だってあるんだ！迷うな！引き金を引け！」\n",
      "「畜生！感染者め！」\n",
      "みたいな気持ちになる。マスクしやがり下さい。\n",
      "\n",
      "【極性】： negative\n",
      "posi_score： 12.83 ／ nega_score： 22.97 ／followers： 10623\n",
      "\n",
      "…………　類似ツイート6位：類似度 0.637　…………\n",
      "\n",
      "「クルマ社会と電車社会の境目はどこにあるんだろう？」と思い、簡単に可視化してみた。\n",
      "\n",
      "2010年国勢調査のデータより、【15歳以上の男性が通勤で使う交通手段1位】で塗り分け。\n",
      "複数回答なので、例えば「駅まで車・駅から電車」の人は自家用車と電車の両方にカウントされるのでご注意を。 \n",
      "\n",
      "【極性】： positive\n",
      "posi_score： 9.95 ／ nega_score： 3.2 ／followers： 6267\n",
      "\n",
      "…………　類似ツイート7位：類似度 0.635　…………\n",
      "\n",
      "Twitterでは「水にありがとうという言葉を見せるときれいな結晶を作る」はほぼ完全に否定されているが「卒論の締切が近くなるとプリンタが壊れる」はかなり信じられている。\n",
      "\n",
      "【極性】： negative\n",
      "posi_score： 13.22 ／ nega_score： 21.59 ／followers： 903\n",
      "\n",
      "…………　類似ツイート8位：類似度 0.632　…………\n",
      "\n",
      "「#クリスマス音楽祭2019」放送中\n",
      "まもなく登場 #SexyZone の皆さんから\n",
      "本番直前に意気込みをいただきました！🎄\n",
      "＃CDTV #生放送 \n",
      "\n",
      "【極性】： positive\n",
      "posi_score： 9.89 ／ nega_score： 1.68 ／followers： 251072\n",
      "\n",
      "…………　類似ツイート9位：類似度 0.631　…………\n",
      "\n",
      "デマ注意！\n",
      "「男へのクリスマスプレゼントはネクタイがオススメ」というデマが流れていますが絶対に信用しないでください！\n",
      "男が本当に望んでいるものは「名誉ある戦死」です！\n",
      "\n",
      "【極性】： positive\n",
      "posi_score： 9.84 ／ nega_score： 9.34 ／followers： 3060\n",
      "\n",
      "…………　類似ツイート10位：類似度 0.624　…………\n",
      "\n",
      "はれのひ事件は絶対に忘れません。\n",
      "家は母子家庭でお金もないのに母が必死に働いて、一生の晴れ舞台のために振袖のお金を用意してくれて。なのに、あの社長は勝手に逃げて。思い出すだけでも涙が止まりません。\n",
      "多くの人の人生を崩したことと同じです。あいつもうすぐ出てくるんですね。\n",
      "#仰天ニュース\n",
      "\n",
      "【極性】： positive\n",
      "posi_score： 43.16 ／ nega_score： 40.96 ／followers： 38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#類似判定と類似している上位10件の文書を出力\n",
    "\n",
    "top10 = m.docvecs.most_similar(len(trainings) - 1)\n",
    "\n",
    "print(\"=========== 判定したいツイート ===========\\n\")\n",
    "print(similar)\n",
    "\n",
    "print()\n",
    "print(\"======= 類似度上位１０（全{}ツイート中） =======\".format(len(trainings)))\n",
    "print(\"※nega/posiスコアはフォロワー数のlogで割っています。フォロワー数はばらつきが大きいので対数変換しています\")\n",
    "print()\n",
    "for i in range(len(top10)):\n",
    "    score = top10[i]\n",
    "    index = int(score[0])\n",
    "    similar_score = score[1]\n",
    "    tweet = df_tweet[\"Full_text\"]\n",
    "    judge = df_tweet[\"Judge\"]\n",
    "    posi_score = df_tweet[\"Posi_score\"]\n",
    "    nega_score = df_tweet[\"Nega_score\"]\n",
    "    followers = df_tweet[\"Followers\"]\n",
    "    print(\"…………　類似ツイート{}位：類似度 {}　…………\".format((i+1), math.floor(similar_score*1000)/1000))\n",
    "    print()\n",
    "    print(tweet[index])\n",
    "    print()\n",
    "    print(\"【極性】：\", judge[index])\n",
    "    print(\"posi_score：\",math.floor((posi_score[index]/np.log(followers[index]))*100)/100, \"／\", \"nega_score：\", math.floor((nega_score[index]/np.log(followers[index]))*100)/100, \"／followers：\", math.floor(followers[index]))\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# その他試みたこと\n",
    "断念、または精度が全く良くない。覚書として記録"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## １）文章ベクトルを特徴量としたネガポジ予測モデル\n",
    "　・文章ベクトルとフォロワー数を特徴量X、ネガポジスコアを目的変数yとしたデータを学習<br>\n",
    "　・文章ベクトルはDoc２vecとTf-idfの２種を作成<br>\n",
    "　・ツイート予定文書を入力してネガポジスコアを予測する<br>\n",
    "　・試した予測モデル<br>\n",
    "　・MultiOutputRegressor、SVRのrbf と　SVRの線形、lightgbm、ランダムフォレスト<br>\n",
    "#### 結果：精度が低すぎて断念<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vecで文章ベクトル取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ベクトル化するセンチメントスコア付きデータ数： 374\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Followers</th>\n",
       "      <th>Full_text</th>\n",
       "      <th>Nega_score</th>\n",
       "      <th>Posi_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6800.0</td>\n",
       "      <td>君、すごい食い方やな\\n https://t.co/yRTGvd43wT</td>\n",
       "      <td>31.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>絶対断らないと評判の病児保育室、助成金下りず2億円の赤字を出し閉鎖\\n\\n全国に2886カ所...</td>\n",
       "      <td>148.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2217.0</td>\n",
       "      <td>全国の皆さんへ\\nどうか皆様のお力を貸してください。\\n\\n１日も早く娘を助けたいです。\\n...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3756.0</td>\n",
       "      <td>o0( 歴史上、さんざん他国の料理を魔改造してきた我が国が「寿司ポリス」などどは片腹痛い！あ...</td>\n",
       "      <td>85.0</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7363.0</td>\n",
       "      <td>百合関係図です。 https://t.co/hZxIYOsSag</td>\n",
       "      <td>58.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Followers                                          Full_text  Nega_score  \\\n",
       "0     6800.0               君、すごい食い方やな\\n https://t.co/yRTGvd43wT        31.0   \n",
       "1     1564.0  絶対断らないと評判の病児保育室、助成金下りず2億円の赤字を出し閉鎖\\n\\n全国に2886カ所...       148.0   \n",
       "2     2217.0  全国の皆さんへ\\nどうか皆様のお力を貸してください。\\n\\n１日も早く娘を助けたいです。\\n...        50.0   \n",
       "3     3756.0  o0( 歴史上、さんざん他国の料理を魔改造してきた我が国が「寿司ポリス」などどは片腹痛い！あ...        85.0   \n",
       "4     7363.0                   百合関係図です。 https://t.co/hZxIYOsSag        58.0   \n",
       "\n",
       "   Posi_score  \n",
       "0        75.0  \n",
       "1       168.0  \n",
       "2       129.0  \n",
       "3       137.0  \n",
       "4        82.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Doc2vecでベクトル化\n",
    "from natto import MeCab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df_buzz = pd.read_csv('./output/buzz_tweet.csv',\n",
    "                      usecols = [\"Full_text\", \"Posi_score\", \"Nega_score\", \"Followers\"])\n",
    "#.to_csv('./output/for_training.csv', mode = \"a\", index = False, header = None)\n",
    "#pd.read_csv('./output/fire_buzz_tweet.csv', usecols = [\"Full_text\", \"Judge\", \"Sentiment\"]).to_csv('./output/for_training.csv', mode = \"a\", index = False, header = None)\n",
    "print(\"ベクトル化するセンチメントスコア付きデータ数：\", len(df_buzz))\n",
    "display(df_buzz.head())\n",
    "\n",
    "#doc2vecでベクトル化\n",
    "for_training = df_buzz['Full_text']\n",
    "#print(for_training)\n",
    "vector_tweet = []\n",
    "for i in for_training:\n",
    "    i = m.infer_vector(i)\n",
    "    vector_tweet.append(i)\n",
    "\n",
    "df_vector = pd.DataFrame(data = vector_tweet)\n",
    "\n",
    "# print(\"Doc2vecベクトル\")\n",
    "# display(df_vector.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idfでベクトル取得\n",
    "#### 参考サイト\n",
    "\n",
    "機械学習_サポートベクターマシーン_pythonで実装<br>\n",
    "https://dev.classmethod.jp/machine-learning/2017ad_20171214_svm_python/<br>\n",
    "Tf-idfベクトルってなんだ？<br> https://qiita.com/MasatoTsutsumi/items/5b0a140b1ecbdd0396e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(374, 2402)\n",
      "次元削減後の特徴量が5の時の説明できる分散の割合合計は0.04です\n",
      "次元削減後の特徴量が10の時の説明できる分散の割合合計は0.05です\n",
      "次元削減後の特徴量が50の時の説明できる分散の割合合計は0.17です\n",
      "次元削減後の特徴量が100の時の説明できる分散の割合合計は0.3です\n",
      "次元削減後の特徴量が500の時の説明できる分散の割合合計は1.0です\n",
      "次元削減後の特徴量が1000の時の説明できる分散の割合合計は1.0です\n",
      "\n",
      "次元削減後Tf-idf\n",
      " (374, 374)\n"
     ]
    }
   ],
   "source": [
    "# 2-1.tf-idf計算\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def Stop_Words():\n",
    "    # ストップワードをダウンロード\n",
    "    url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "    urllib.request.urlretrieve(url, './output/stop_word.txt')\n",
    "\n",
    "    with open('./output/stop_word.txt', 'r', encoding='utf-8') as file:\n",
    "        stopwords = [word.replace('\\n', '') for word in file.readlines()]\n",
    "\n",
    "    #追加ストップワードを設定（助詞や意味のない平仮名１文字）\n",
    "    add_words = ['あ','い','う','え','お','か','き','く','け','こ','さ','し','す','せ','そ','た','ち','つ','て','と',\n",
    "                 'な','に','ぬ','ね','の','は','ひ','ふ','へ','ほ','ま','み','む','め','も','や','ゆ','よ',\n",
    "                 'ら','り','る','れ','ろ','わ','を','ん','が','ぎ','ぐ','げ','ご','ざ','じ','ず','ぜ','ぞ',\n",
    "                 'だ','ぢ','づ','で','ど','ば','び','ぶ','べ','ぼ','ぱ','ぴ','ぷ','ぺ','ぽ',\n",
    "                 'くん','です','ます','ました','そして','でも','だから','だが','くらい','その','それ','かも',\n",
    "                 'あれ','あの','あっ','そんな','この','これ','とか','とも','する','という','ござい',\n",
    "                 'ので','なんて','たら', 'られ','たい','さて','てる','ください','なる','けど','でし',\n",
    "                 'じゃん','だっ','なっ','でしょ', 'ある','って','こんな','ねえ'\n",
    "                ]\n",
    "    stopwords = stopwords + add_words\n",
    "    return stopwords\n",
    "\n",
    "stopwords = Stop_Words()\n",
    "tfidfv = TfidfVectorizer(lowercase=True, stop_words=stopwords) # stop words処理\n",
    " \n",
    "tfv_vector_fit = tfidfv.fit(for_training)\n",
    "tfv_vector = tfv_vector_fit.transform(for_training)\n",
    "print(tfv_vector.shape) \n",
    "\n",
    "# 2-2.次元削減(「lsa」を使って次元削減を行う)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# 2-2-1.パラメータの調整\n",
    "list_n_comp = [5,10,50,100,500,1000] # 特徴量を何個に削減するか、というパラメータです。できるだけ情報量を欠損しないで、かつ次元数は少なくしたいですね。\n",
    "for i in list_n_comp:\n",
    "    lsa = TruncatedSVD(n_components=i,n_iter=5, random_state = 0)\n",
    "    lsa.fit(tfv_vector) \n",
    "    tfv_vector_lsa = lsa.transform(tfv_vector)\n",
    "    print('次元削減後の特徴量が{0}の時の説明できる分散の割合合計は{1}です'.format(i,round((sum(lsa.explained_variance_ratio_)),2)))\n",
    "\n",
    "# 2-2-2.次元削減した状態のデータを作成\n",
    "# 上記で確認した「n_components」に指定した上で、次元削減（特徴抽出）を行う\n",
    "lsa = TruncatedSVD(n_components=1000, n_iter=5, random_state = 0) # 今回は次元数を1000に指定\n",
    "lsa.fit(tfv_vector)\n",
    "X_tf = lsa.transform(tfv_vector)\n",
    "print()\n",
    "print(\"次元削減後Tf-idf\\n\", X_tf.shape)\n",
    "# print(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欠損値削除前データ (374, 4)\n",
      "\n",
      "Doc2vecベクトル\n",
      "X.shape (373, 301)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Followers</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6800.0</td>\n",
       "      <td>-0.002347</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>0.017913</td>\n",
       "      <td>0.006379</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>-0.013634</td>\n",
       "      <td>-0.001045</td>\n",
       "      <td>0.017018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014283</td>\n",
       "      <td>-0.010716</td>\n",
       "      <td>-0.018207</td>\n",
       "      <td>0.013203</td>\n",
       "      <td>0.021252</td>\n",
       "      <td>-0.011075</td>\n",
       "      <td>-0.009633</td>\n",
       "      <td>-0.008679</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>-0.003732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.027724</td>\n",
       "      <td>-0.005945</td>\n",
       "      <td>0.029363</td>\n",
       "      <td>0.015228</td>\n",
       "      <td>0.030413</td>\n",
       "      <td>-0.002512</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.026079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032680</td>\n",
       "      <td>-0.021952</td>\n",
       "      <td>-0.063932</td>\n",
       "      <td>0.006111</td>\n",
       "      <td>0.054873</td>\n",
       "      <td>-0.017913</td>\n",
       "      <td>-0.025832</td>\n",
       "      <td>-0.019413</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>-0.002683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2217.0</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>0.021177</td>\n",
       "      <td>-0.013813</td>\n",
       "      <td>0.025786</td>\n",
       "      <td>0.017142</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>-0.025777</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.025575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.009682</td>\n",
       "      <td>-0.025858</td>\n",
       "      <td>0.006571</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>0.009201</td>\n",
       "      <td>-0.001735</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>-0.002077</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3756.0</td>\n",
       "      <td>-0.010936</td>\n",
       "      <td>-0.010247</td>\n",
       "      <td>-0.003010</td>\n",
       "      <td>0.031322</td>\n",
       "      <td>0.007310</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>-0.033298</td>\n",
       "      <td>-0.004619</td>\n",
       "      <td>0.028420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015579</td>\n",
       "      <td>-0.020676</td>\n",
       "      <td>-0.011868</td>\n",
       "      <td>0.030162</td>\n",
       "      <td>0.037181</td>\n",
       "      <td>-0.023565</td>\n",
       "      <td>-0.011537</td>\n",
       "      <td>-0.002477</td>\n",
       "      <td>-0.002884</td>\n",
       "      <td>-0.002214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7363.0</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>0.011890</td>\n",
       "      <td>-0.008067</td>\n",
       "      <td>0.019329</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.015009</td>\n",
       "      <td>-0.012856</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.013483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003167</td>\n",
       "      <td>-0.000344</td>\n",
       "      <td>-0.024332</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.017458</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>-0.002972</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>-0.002779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Followers         0         1         2         3         4         5  \\\n",
       "0     6800.0 -0.002347  0.004516 -0.002467  0.017913  0.006379  0.001529   \n",
       "1     1564.0  0.001994  0.027724 -0.005945  0.029363  0.015228  0.030413   \n",
       "2     2217.0 -0.008996  0.021177 -0.013813  0.025786  0.017142  0.016225   \n",
       "3     3756.0 -0.010936 -0.010247 -0.003010  0.031322  0.007310  0.002045   \n",
       "4     7363.0 -0.000134  0.011890 -0.008067  0.019329  0.007480  0.015009   \n",
       "\n",
       "          6         7         8    ...          290       291       292  \\\n",
       "0 -0.013634 -0.001045  0.017018    ...    -0.014283 -0.010716 -0.018207   \n",
       "1 -0.002512  0.003670  0.026079    ...    -0.032680 -0.021952 -0.063932   \n",
       "2 -0.025777  0.001117  0.025575    ...     0.001572  0.009682 -0.025858   \n",
       "3 -0.033298 -0.004619  0.028420    ...    -0.015579 -0.020676 -0.011868   \n",
       "4 -0.012856  0.001064  0.013483    ...    -0.003167 -0.000344 -0.024332   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.013203  0.021252 -0.011075 -0.009633 -0.008679 -0.001196 -0.003732  \n",
       "1  0.006111  0.054873 -0.017913 -0.025832 -0.019413  0.003702 -0.002683  \n",
       "2  0.006571  0.017764  0.009201 -0.001735  0.008258 -0.002077  0.001832  \n",
       "3  0.030162  0.037181 -0.023565 -0.011537 -0.002477 -0.002884 -0.002214  \n",
       "4  0.003653  0.017458  0.000162 -0.002972  0.002077  0.002838 -0.002779  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idfベクトル\n",
      "X_tf_idf.shape (373, 375)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Followers</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6800.0</td>\n",
       "      <td>0.308188</td>\n",
       "      <td>-0.000429</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>-0.002650</td>\n",
       "      <td>-1.302487e-16</td>\n",
       "      <td>-0.001072</td>\n",
       "      <td>-0.003650</td>\n",
       "      <td>-0.008917</td>\n",
       "      <td>-0.015199</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000972</td>\n",
       "      <td>-0.003923</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>1.482354e-17</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>0.184497</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.001684</td>\n",
       "      <td>7.847122e-17</td>\n",
       "      <td>-0.000692</td>\n",
       "      <td>-0.002466</td>\n",
       "      <td>-0.006032</td>\n",
       "      <td>-0.010359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000497</td>\n",
       "      <td>-0.002015</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>-9.693617e-17</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2217.0</td>\n",
       "      <td>0.115796</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.001162</td>\n",
       "      <td>-1.803444e-16</td>\n",
       "      <td>-0.000493</td>\n",
       "      <td>-0.001979</td>\n",
       "      <td>-0.004864</td>\n",
       "      <td>-0.008556</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>-0.001123</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.058502e-16</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3756.0</td>\n",
       "      <td>0.118023</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.001099</td>\n",
       "      <td>-7.856138e-17</td>\n",
       "      <td>-0.000454</td>\n",
       "      <td>-0.001647</td>\n",
       "      <td>-0.004030</td>\n",
       "      <td>-0.006941</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000303</td>\n",
       "      <td>-0.001232</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>-4.674257e-17</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7363.0</td>\n",
       "      <td>0.308188</td>\n",
       "      <td>-0.000429</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>-0.002650</td>\n",
       "      <td>-1.911629e-16</td>\n",
       "      <td>-0.001072</td>\n",
       "      <td>-0.003650</td>\n",
       "      <td>-0.008917</td>\n",
       "      <td>-0.015199</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000972</td>\n",
       "      <td>-0.003923</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>1.141115e-16</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 375 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Followers         0         1         2         3             4         5  \\\n",
       "0     6800.0  0.308188 -0.000429 -0.000171 -0.002650 -1.302487e-16 -0.001072   \n",
       "1     1564.0  0.184497 -0.000264 -0.000108 -0.001684  7.847122e-17 -0.000692   \n",
       "2     2217.0  0.115796 -0.000171 -0.000074 -0.001162 -1.803444e-16 -0.000493   \n",
       "3     3756.0  0.118023 -0.000170 -0.000070 -0.001099 -7.856138e-17 -0.000454   \n",
       "4     7363.0  0.308188 -0.000429 -0.000171 -0.002650 -1.911629e-16 -0.001072   \n",
       "\n",
       "          6         7         8    ...          364       365       366  \\\n",
       "0 -0.003650 -0.008917 -0.015199    ...    -0.000030 -0.000064 -0.000972   \n",
       "1 -0.002466 -0.006032 -0.010359    ...    -0.000015 -0.000032 -0.000497   \n",
       "2 -0.001979 -0.004864 -0.008556    ...    -0.000008 -0.000018 -0.000276   \n",
       "3 -0.001647 -0.004030 -0.006941    ...    -0.000009 -0.000020 -0.000303   \n",
       "4 -0.003650 -0.008917 -0.015199    ...    -0.000030 -0.000064 -0.000972   \n",
       "\n",
       "        367       368       369       370           371       372       373  \n",
       "0 -0.003923  0.000312 -0.000123  0.000327  1.482354e-17 -0.000037 -0.000026  \n",
       "1 -0.002015  0.000163 -0.000065  0.000176 -9.693617e-17 -0.000020 -0.000014  \n",
       "2 -0.001123  0.000091 -0.000037  0.000100  1.058502e-16 -0.000011 -0.000008  \n",
       "3 -0.001232  0.000100 -0.000040  0.000109 -4.674257e-17 -0.000012 -0.000009  \n",
       "4 -0.003923  0.000312 -0.000123  0.000327  1.141115e-16 -0.000037 -0.000026  \n",
       "\n",
       "[5 rows x 375 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y.shape (373, 2)\n"
     ]
    }
   ],
   "source": [
    "#X、yデータを作成\n",
    "\n",
    "#Doc2vecのベクトルデータ\n",
    "print(\"欠損値削除前データ\", df_buzz.shape)\n",
    "print()\n",
    "\n",
    "#文書ベクトルを含んだdf\n",
    "df_buzz_vec = pd.concat([df_buzz, df_vector], axis=1)\n",
    "df_buzz_vec = df_buzz_vec.dropna(subset = [\"Followers\"])#欠損値行削除\n",
    "df_buzz_vec = df_buzz_vec.drop([ \"Full_text\", \"Nega_score\", \"Posi_score\"], axis=1)\n",
    "X = df_buzz_vec.values\n",
    "print(\"Doc2vecベクトル\")\n",
    "print(\"X.shape\", X.shape)\n",
    "display(df_buzz_vec.head())\n",
    "\n",
    "#tf-idfのベクトルデータ\n",
    "tf_df = pd.DataFrame(data = X_tf)\n",
    "tf_df = pd.concat([df_buzz, tf_df], axis=1)\n",
    "tf_df = tf_df.dropna(subset = [\"Followers\"])#欠損値行削除\n",
    "tf_df = tf_df.drop([ \"Full_text\", \"Nega_score\", \"Posi_score\"], axis=1)\n",
    "X_tf_idf = tf_df.values\n",
    "print(\"Tf-idfベクトル\")\n",
    "print(\"X_tf_idf.shape\", tf_df.shape)\n",
    "display(tf_df.head())\n",
    "\n",
    "#yデータ作成\n",
    "df_buzz = df_buzz.dropna(subset = [\"Followers\"])#y用に\"Followers\"の欠損行削除\n",
    "y = df_buzz.loc[:,['Posi_score', 'Nega_score']]#できればDateも特徴量に入れたい\n",
    "y_p = df_buzz['Posi_score']\n",
    "y_n = df_buzz['Nega_score']\n",
    "y = y.values\n",
    "print()\n",
    "print(\"y.shape\", y.shape)\n",
    "y_p = y_p.values\n",
    "y_n = y_n.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiOutputRegressorで複数の回帰¶\n",
    "#### 結果：D2vベクトルよりTf-idfがややマシ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解\n",
      " [[ 141.   39.]\n",
      " [ 123.  139.]\n",
      " [  77.   32.]\n",
      " [  40.   63.]\n",
      " [  93.  125.]\n",
      " [  91.   89.]\n",
      " [  99.   24.]\n",
      " [ 141.  131.]\n",
      " [  91.   29.]\n",
      " [  94.   62.]\n",
      " [  46.   51.]\n",
      " [  48.   32.]\n",
      " [  49.   58.]\n",
      " [ 116.  142.]\n",
      " [  76.   46.]\n",
      " [  88.   24.]\n",
      " [ 124.  102.]\n",
      " [  54.   20.]\n",
      " [  75.   29.]\n",
      " [ 122.   48.]\n",
      " [ 266.   76.]\n",
      " [ 118.  224.]\n",
      " [  25.   27.]\n",
      " [ 104.   84.]\n",
      " [ 118.  227.]\n",
      " [ 128.    8.]\n",
      " [ 125.   43.]\n",
      " [  68.   42.]\n",
      " [  60.   28.]\n",
      " [ 168.  148.]\n",
      " [  40.   45.]\n",
      " [ 103.   56.]\n",
      " [  71.   58.]\n",
      " [  65.   43.]\n",
      " [  61.   46.]\n",
      " [  88.   24.]\n",
      " [ 156.   33.]\n",
      " [  59.   54.]\n",
      " [  18.   46.]\n",
      " [ 102.   81.]\n",
      " [ 114.   43.]\n",
      " [  92.   69.]\n",
      " [ 160.  144.]\n",
      " [ 133.   28.]\n",
      " [  55.   59.]\n",
      " [ 117.   26.]\n",
      " [  63.   29.]\n",
      " [ 205.   45.]\n",
      " [  97.   78.]\n",
      " [  40.   80.]\n",
      " [ 100.   27.]\n",
      " [ 126.  124.]\n",
      " [ 121.   16.]\n",
      " [  31.   37.]\n",
      " [ 116.   83.]\n",
      " [  28.   16.]\n",
      " [  62.   36.]\n",
      " [  69.   20.]\n",
      " [  37.   49.]\n",
      " [ 179.   41.]\n",
      " [ 140.  120.]\n",
      " [ 121.   78.]\n",
      " [  79.   21.]\n",
      " [  97.   47.]\n",
      " [  38.   42.]\n",
      " [  52.   15.]\n",
      " [  85.   35.]\n",
      " [  81.   14.]\n",
      " [ 102.   29.]\n",
      " [  77.  115.]\n",
      " [ 122.   38.]\n",
      " [ 156.   20.]\n",
      " [  90.   19.]\n",
      " [  44.   12.]\n",
      " [ 102.   49.]\n",
      " [  72.   39.]\n",
      " [  92.   57.]\n",
      " [ 107.   29.]\n",
      " [ 171.  214.]\n",
      " [ 186.  106.]\n",
      " [ 190.   21.]\n",
      " [ 119.  150.]\n",
      " [  39.   27.]\n",
      " [ 214.   86.]\n",
      " [ 115.   37.]\n",
      " [ 149.   50.]\n",
      " [  57.   15.]\n",
      " [ 122.  137.]\n",
      " [  26.   20.]\n",
      " [  67.    3.]\n",
      " [ 168.  197.]\n",
      " [ 146.   61.]\n",
      " [  21.    3.]\n",
      " [ 169.   64.]\n",
      " [  38.   41.]\n",
      " [ 118.  149.]\n",
      " [  44.   30.]\n",
      " [  79.   32.]\n",
      " [  58.   56.]\n",
      " [  95.   32.]\n",
      " [  41.   27.]\n",
      " [  89.   95.]\n",
      " [  64.  143.]\n",
      " [  77.    5.]\n",
      " [  48.   39.]\n",
      " [  94.   22.]\n",
      " [  97.   38.]\n",
      " [  83.  113.]\n",
      " [  70.   22.]\n",
      " [  51.   24.]\n",
      " [ 210.  169.]\n",
      " [  87.   28.]]\n",
      "\n",
      "[[ 182.57213217   55.30549938]\n",
      " [ 119.43255331  131.34516462]\n",
      " [  78.66887954   53.53536235]\n",
      " [  89.77300047   33.24727143]\n",
      " [  81.22660923   72.87186611]\n",
      " [  94.96625337   50.58584838]\n",
      " [  72.81803108   64.88823247]\n",
      " [  73.62025754   43.03321997]\n",
      " [  65.75840884   38.60931143]\n",
      " [  68.98565047   26.03228811]\n",
      " [ 192.83008009   92.02278882]\n",
      " [ 112.29108608   61.69149998]\n",
      " [ 102.33868136   31.96878777]\n",
      " [  79.79396109   40.90419371]\n",
      " [ 136.30581895   71.1414937 ]\n",
      " [ 157.450785     96.11173494]\n",
      " [ 111.32907132   73.13927466]\n",
      " [ 140.02657913  108.76870187]\n",
      " [  67.66406389   41.62001292]\n",
      " [  94.81828993   67.34101388]\n",
      " [ 127.37983654   42.23146752]\n",
      " [  90.75298736   57.73427312]\n",
      " [ 135.85629746   35.06009786]\n",
      " [  95.29889142   78.7895381 ]\n",
      " [  90.23383033   63.04373428]\n",
      " [  71.98100622   12.59109439]\n",
      " [  68.35898706   40.24177482]\n",
      " [  97.33975542   81.84845067]\n",
      " [  59.57272772   33.21978322]\n",
      " [ 104.41348339   84.50725711]\n",
      " [  73.02593448   46.87591048]\n",
      " [  73.1554071    62.63725317]\n",
      " [ 100.01878265   50.04354924]\n",
      " [ 135.64284558   79.30709146]\n",
      " [ 153.72923729  108.45893052]\n",
      " [  78.63292642   48.0630029 ]\n",
      " [ 162.47678197   62.21405129]\n",
      " [  82.8937582    64.13620665]\n",
      " [  78.97902345   99.94899966]\n",
      " [ 100.12319491   80.89997595]\n",
      " [  79.6223555    37.28161499]\n",
      " [ 136.61879754   57.59214818]\n",
      " [ 199.34185179  154.88464636]\n",
      " [ 123.86048584   46.03839451]\n",
      " [  85.22055844   74.11017639]\n",
      " [  97.42824889   53.72209061]\n",
      " [  64.55909195   20.1757372 ]\n",
      " [  90.10696649  129.67569447]\n",
      " [ 123.34240518   66.08515186]\n",
      " [  73.38100825   60.18408049]\n",
      " [ 132.51466095   24.64724937]\n",
      " [  75.1580447    84.71634841]\n",
      " [  96.87501209   31.72462701]\n",
      " [  49.19770518   28.14664035]\n",
      " [ 143.94034498   96.31778944]\n",
      " [  68.42970263   50.76631843]\n",
      " [ 116.22564956   43.51804719]\n",
      " [  57.10757096   19.75470474]\n",
      " [  98.7588802    64.02126851]\n",
      " [ 108.88967417   55.22200396]\n",
      " [  83.23552836   30.949429  ]\n",
      " [ 118.73110782   81.41497998]\n",
      " [  78.632416     17.76868206]\n",
      " [  93.23910784   35.83128738]\n",
      " [ 109.72083538  117.34664668]\n",
      " [  90.23462594  103.02519934]\n",
      " [  82.29131474   63.38659479]\n",
      " [ 108.0753899    42.98756935]\n",
      " [ 151.44462844  144.8966561 ]\n",
      " [ 128.98539208   94.15538179]\n",
      " [ 100.07190071   91.31197095]\n",
      " [  99.61949665   51.81378103]\n",
      " [ 134.30328756   49.84390434]\n",
      " [ 100.94329317   53.76097402]\n",
      " [  85.00885877   76.58746246]\n",
      " [ 102.88155815   97.07312653]\n",
      " [  63.28238158   36.78939772]\n",
      " [  89.6821565    39.25937638]\n",
      " [ 139.1464633   113.94270265]\n",
      " [  71.51255492   53.7724914 ]\n",
      " [  96.43452983   47.59575163]\n",
      " [  80.10993577   40.7433453 ]\n",
      " [ 127.07797612  110.89607393]\n",
      " [  76.08875965   33.38840222]\n",
      " [ 137.06851273   43.72847503]\n",
      " [ 122.71629409   34.81734291]\n",
      " [ 123.21691301   47.25437222]\n",
      " [  70.07238924  115.9576567 ]\n",
      " [ 108.04505432   50.60403896]\n",
      " [  91.94618281   30.8993067 ]\n",
      " [  66.11264307   58.91127176]\n",
      " [  88.78416241   28.00640335]\n",
      " [  86.69098746    7.99546959]\n",
      " [ 131.63721254  104.32231856]\n",
      " [ 204.87286645   79.66058148]\n",
      " [  89.91599664   74.54913549]\n",
      " [ 132.26280972   55.42874515]\n",
      " [  84.61044952   72.07389708]\n",
      " [  82.44963597   42.78052522]\n",
      " [ 108.27168362   66.77988784]\n",
      " [ 127.72607128  126.51301216]\n",
      " [  95.23951275   71.49827439]\n",
      " [ 104.502728     84.08353233]\n",
      " [  94.52169157   33.88396989]\n",
      " [ 123.39758644  104.50710093]\n",
      " [  67.78870487   39.29561231]\n",
      " [ 137.394228     74.61419362]\n",
      " [ 124.65658478   72.44306876]\n",
      " [ 100.29826117   89.50560107]\n",
      " [  61.56207356   75.45389489]\n",
      " [ 107.16619937  114.66248498]\n",
      " [  66.63200261  102.26827272]]\n",
      "R ^ 2_score(1に近いほど良い）： -0.233854491738\n",
      "\n",
      "[[ 101.82772489   27.62712804]\n",
      " [  98.82716427   86.66279208]\n",
      " [  87.38519523   45.86080771]\n",
      " [  91.34625592   67.75884584]\n",
      " [  97.11010243   99.68061436]\n",
      " [  91.34625592   72.53435976]\n",
      " [  86.6520366    35.60593331]\n",
      " [  92.60054657   44.17825846]\n",
      " [  88.70521191   46.26119269]\n",
      " [  88.94384497   46.33858037]\n",
      " [  91.34625592   68.94929689]\n",
      " [ 106.06610562   82.40662004]\n",
      " [ 109.86216018   75.98102025]\n",
      " [  72.74928683   46.2492811 ]\n",
      " [ 113.39343206   84.00315032]\n",
      " [  86.93097869   27.62712804]\n",
      " [ 103.78308877  113.09865942]\n",
      " [ 107.14760672  107.51781733]\n",
      " [ 114.37260247   38.50516358]\n",
      " [  62.61587154   44.17825846]\n",
      " [  82.59244441   80.26109556]\n",
      " [ 101.5181705    66.28324433]\n",
      " [ 100.56998707   92.51570028]\n",
      " [ 112.41508315   94.33919454]\n",
      " [  87.42540062   42.4923329 ]\n",
      " [ 125.37113249   34.24058463]\n",
      " [ 109.94471729   30.48093843]\n",
      " [  71.18880663   48.71718971]\n",
      " [  85.52027681  114.78786536]\n",
      " [  96.51449717   52.24171203]\n",
      " [ 124.69317783   97.90792759]\n",
      " [ 110.27506403   62.43031081]\n",
      " [  93.25622224   38.50516358]\n",
      " [ 154.38018001  107.80619982]\n",
      " [ 120.02915605  151.17699928]\n",
      " [  81.38792331   24.3403453 ]\n",
      " [ 121.19643219   99.20630418]\n",
      " [  87.93416198   85.05258165]\n",
      " [  64.81823483  112.04041436]\n",
      " [ 101.31148164   72.96489959]\n",
      " [  86.39536591   25.32706736]\n",
      " [  76.50969416   32.32912094]\n",
      " [ 100.5200565    70.63103236]\n",
      " [ 106.65972273   76.08384206]\n",
      " [ 134.80900335   75.52983162]\n",
      " [  96.75088816   43.96916233]\n",
      " [  90.13247874   35.83385163]\n",
      " [  56.11750783   92.43964176]\n",
      " [  96.57162064   62.47519849]\n",
      " [ 101.31148164   62.47519849]\n",
      " [  81.4138313    24.70817348]\n",
      " [ 125.22249678  101.26952453]\n",
      " [ 123.17899854   60.3219509 ]\n",
      " [ 121.62899854   96.8259896 ]\n",
      " [ 110.93254269   53.44263939]\n",
      " [ 129.32848504   62.58316071]\n",
      " [ 127.38162068   60.33772266]\n",
      " [ 111.86436663   88.36397603]\n",
      " [  68.04921554   31.54188892]\n",
      " [ 151.01595749   42.55633783]\n",
      " [  71.68727338   37.12316673]\n",
      " [  59.73293378   47.53325309]\n",
      " [  91.13108273   30.90444537]\n",
      " [ 101.31148164   50.59213762]\n",
      " [ 101.31148164   62.47519849]\n",
      " [ 104.15180773   91.54433844]\n",
      " [ 114.17836288   79.89497736]\n",
      " [ 102.59472278   68.87163559]\n",
      " [ 104.5902889    80.93192614]\n",
      " [ 105.32375052   74.11534145]\n",
      " [  74.24699846   40.02699424]\n",
      " [  85.14321718   35.00235141]\n",
      " [ 150.53130078   63.32670891]\n",
      " [ 103.04673494   65.8528813 ]\n",
      " [ 112.85274168   52.24171203]\n",
      " [ 109.13916373   89.12665521]\n",
      " [  78.98760227   67.22419436]\n",
      " [ 100.21590206   27.06013156]\n",
      " [ 133.50263621   85.07621151]\n",
      " [  82.80234827   42.55633783]\n",
      " [  91.13108273   30.90444537]\n",
      " [  99.57776963   44.17825846]\n",
      " [ 102.69376781  101.68631171]\n",
      " [ 109.575618     40.154738  ]\n",
      " [  85.40807306   46.43320676]\n",
      " [ 220.39454359   49.78923346]\n",
      " [ 114.10937878   67.85181238]\n",
      " [ 119.49104513   54.07465307]\n",
      " [ 111.09115616  119.36892828]\n",
      " [  89.54395747   69.12537803]\n",
      " [  69.37148135   46.96251871]\n",
      " [ 109.575618     40.154738  ]\n",
      " [  47.02576992   22.79626958]\n",
      " [ 114.5871       60.08428224]\n",
      " [  88.27210265   86.97684836]\n",
      " [ 104.94267585  106.16471664]\n",
      " [ 108.14657803   75.66983813]\n",
      " [ 127.08393081  112.29013291]\n",
      " [  55.02463948   36.83393298]\n",
      " [ 114.37260247   67.69749519]\n",
      " [  96.57162064   62.47519849]\n",
      " [  76.30217705   42.42705581]\n",
      " [ 101.31148164   62.47519849]\n",
      " [ 103.24501071   63.71351938]\n",
      " [ 128.28115292   43.96916233]\n",
      " [  74.2391914    89.1363719 ]\n",
      " [ 124.4313272    72.60338998]\n",
      " [  96.57162064   62.47519849]\n",
      " [ 123.60979739  121.52480777]\n",
      " [ 117.39554234   93.17593566]\n",
      " [ 116.48822866   64.45649235]\n",
      " [ 170.8599986    98.41619227]]\n",
      "R ^ 2_score(1に近いほど良い）： -0.296035869985\n"
     ]
    }
   ],
   "source": [
    "#MultiOutputRegressorで複数の回帰\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#D2vベクトル\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.7, test_size=0.3, random_state=0)\n",
    "\n",
    "#X, y = make_regression(n_samples=10, n_targets=3, random_state=1)\n",
    "MOR = MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X_train, y_train)\n",
    "y_pred = MOR.predict(X_test)\n",
    "score = MOR.score(X_test, y_test)\n",
    "\n",
    "print(\"正解\\n\", y_test)\n",
    "print()\n",
    "print(y_pred)\n",
    "print(\"R ^ 2_score(1に近いほど良い）：\", score)\n",
    "print()\n",
    "\n",
    "#X_tf tf-idfベクトルを使った予測\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tf_idf, y, train_size=0.7, test_size=0.3, random_state=0)\n",
    "\n",
    "MOR = MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X_train, y_train)\n",
    "y_pred = MOR.predict(X_test)\n",
    "score = MOR.score(X_test, y_test)\n",
    "print(y_pred)\n",
    "print(\"R ^ 2_score(1に近いほど良い）：\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVRのrbf と　SVRの線形で予測\n",
    "#### 結果：予測値が全くダメ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF: RMSE（０に近いほど良い） 46.977801729985174 \n",
      "Linear: RMSE（０に近いほど良い） 5332.519130224009\n",
      "\n",
      "正解 [ 141.  123.   77.   40.   93.   91.   99.  141.   91.   94.   46.   48.\n",
      "   49.  116.   76.   88.  124.   54.   75.  122.  266.  118.   25.  104.\n",
      "  118.  128.  125.   68.   60.  168.   40.  103.   71.   65.   61.   88.\n",
      "  156.   59.   18.  102.  114.   92.  160.  133.   55.  117.   63.  205.\n",
      "   97.   40.  100.  126.  121.   31.  116.   28.   62.   69.   37.  179.\n",
      "  140.  121.   79.   97.   38.   52.   85.   81.  102.   77.  122.  156.\n",
      "   90.   44.  102.   72.   92.  107.  171.  186.  190.  119.   39.  214.\n",
      "  115.  149.   57.  122.   26.   67.  168.  146.   21.  169.   38.  118.\n",
      "   44.   79.   58.   95.   41.   89.   64.   77.   48.   94.   97.   83.\n",
      "   70.   51.  210.   87.]\n",
      "rbf推定 [ 92.61958406  91.72157239  91.72157239  91.72157239  91.7212691\n",
      "  91.72157244  91.72157239  91.72157239  91.72157239  91.72157239\n",
      "  91.72157239  91.72157239  91.72157239  91.72157239  91.71672799\n",
      "  91.7231354   91.72157239  91.72157239  91.72157239  91.7229219\n",
      "  91.72157239  91.72157239  91.72157239  91.72156684  91.72157239\n",
      "  91.72157239  91.72157239  91.7215724   91.72157239  91.72157239\n",
      "  91.72157231  91.72157239  91.72157239  91.72157239  91.05350991\n",
      "  90.72404537  91.72894584  91.72157239  91.31951268  91.72157239\n",
      "  91.72157239  91.72157239  91.72157239  91.72157239  91.72157239\n",
      "  91.72157239  91.72157239  91.72157239  91.72157239  91.64645316\n",
      "  91.74089497  91.72157239  91.72157239  91.72157239  91.72157239\n",
      "  91.72157239  91.72157239  91.72157239  91.72157239  91.72157239\n",
      "  91.72157239  91.72157239  91.72157239  91.72157239  90.41777681\n",
      "  91.72157239  91.72157239  91.72157239  91.72157239  91.72157239\n",
      "  91.72157239  91.72157239  91.72157239  91.72157239  92.12650763\n",
      "  91.72157239  91.72157239  91.72157239  91.72157235  91.72157235\n",
      "  91.72157239  91.72157239  91.72157235  91.72157239  91.72186807\n",
      "  91.72157239  91.72157239  91.72157239  91.72157239  91.72157239\n",
      "  91.72157244  91.72157239  89.73215242  91.72157239  91.72157239\n",
      "  91.72157235  91.74881143  91.92278159  91.72157239  91.72157239\n",
      "  91.72157239  91.72157239  91.72152707  91.72157239  91.72157239\n",
      "  91.72157239  91.72157239  91.72157239  91.72157239  91.72157239\n",
      "  91.72157239  91.72157239]\n",
      "lin推定 [ -3.64769663e+03  -2.29434329e+02   1.07820148e+02  -2.25362844e+02\n",
      "   1.09195978e+02   4.35323345e+01   3.04070169e+01  -2.08036432e+01\n",
      "  -1.76445598e+03  -4.93289566e+03  -3.72710820e+01   1.91238620e+01\n",
      "  -8.65678545e+01  -6.17516042e+01   1.18640688e+02  -9.64351055e+02\n",
      "  -8.87532050e+01   5.65251805e+01  -8.62092862e+03   8.30022817e+01\n",
      "  -2.89794078e+03  -2.68418411e+02  -1.34506103e+04   1.26714947e+02\n",
      "   1.18309020e+02  -3.27059104e+02  -2.33501738e+02   1.28988723e+02\n",
      "   4.54964474e+01   1.08959682e+02   1.22196687e+02   1.02738602e+02\n",
      "  -1.43455109e+03   5.69653401e+01   1.26456950e+02  -8.83583637e+02\n",
      "   4.79804788e+01   1.23327039e+02   1.09905298e+02   1.07935297e+02\n",
      "  -4.62569722e+03  -1.09875871e+04  -5.02485307e+01  -6.50172167e+03\n",
      "   1.12995306e+02  -4.67261601e+01  -4.31257063e+02   3.06810758e+01\n",
      "  -2.08856988e+01   1.21774921e+02  -1.69742452e+04   7.54872675e+01\n",
      "  -1.06068536e+04  -5.54285738e+02  -1.40462825e+02  -2.50805579e+02\n",
      "  -3.44811659e+04  -2.01667391e+03  -4.00991063e+02   2.24056907e+01\n",
      "  -5.84822828e+03  -3.49919288e+02  -2.26641708e+03   9.50414814e+01\n",
      "   1.21180240e+02   1.02510942e+02   1.02297352e+02  -2.65497816e+03\n",
      "   1.25748825e+02  -6.62221168e+00  -1.70658120e+02  -6.48997386e+02\n",
      "  -1.61214574e+03  -4.74422627e+03   1.13497461e+02  -1.28246487e+02\n",
      "  -3.74413664e+01  -8.24302256e+02   9.62444236e+01  -1.51234865e+02\n",
      "  -2.68858574e+03   6.45022442e+01   5.64950924e+01  -2.55082240e+03\n",
      "   1.28618650e+02  -6.47577559e+03   4.78608246e+01  -3.95885842e+03\n",
      "  -2.84470221e+04  -3.82760921e+03   1.23794485e+02  -1.66534123e+03\n",
      "  -3.55294416e+03  -3.53696407e+02   1.86903539e+01   8.67891423e+01\n",
      "   1.24839391e+02   1.63834455e+01  -3.27721478e+03  -5.06797260e+02\n",
      "   1.23675353e+02  -3.33712406e+03   1.04614885e+02  -6.87108153e+03\n",
      "  -3.74753029e+01  -2.69909860e+03   1.11548159e+02   1.19599122e+02\n",
      "   1.12302633e+02   4.61635347e+01  -1.30732461e+02   4.24641091e+01]\n"
     ]
    }
   ],
   "source": [
    "#ポジ、ネガ別々で予測する\n",
    "#SVRのrbf と　SVRの線形\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "#D2vベクトル(ポジのみ)\n",
    "X_train, X_test, y_p_train, y_p_test = train_test_split(\n",
    "    X, y_p, train_size=0.7, test_size=0.3, random_state=0)\n",
    "\n",
    "svr_rbf = SVR(kernel='rbf', C=1, gamma=0.1)\n",
    "svr_lin = SVR(kernel='linear', C=1)\n",
    "y_rbf = svr_rbf.fit(X_train, y_p_train)\n",
    "y_lin = svr_lin.fit(X_train, y_p_train)\n",
    "\n",
    "pred_rbf = svr_rbf.predict(X_test)\n",
    "pred_lin = svr_lin.predict(X_test)\n",
    "\n",
    "#精度\n",
    "\n",
    "# 相関係数計算\n",
    "rbf_corr = np.corrcoef(y_p_test, pred_rbf)[0, 1]\n",
    "lin_corr = np.corrcoef(y_p_test, pred_lin)[0, 1]\n",
    "\n",
    "# RMSEを計算（０に近いほど良い）\n",
    "rbf_rmse = sqrt(mean_squared_error(pred_rbf, y_p_test))\n",
    "lin_rmse = sqrt(mean_squared_error(pred_lin, y_p_test))\n",
    "\n",
    "print(\"RBF: RMSE（０に近いほど良い） {} \".format(rbf_rmse))\n",
    "print(\"Linear: RMSE（０に近いほど良い） {}\" .format(lin_rmse))\n",
    "print()\n",
    "print(\"正解\", y_p_test)\n",
    "print(\"rbf推定\", pred_rbf)\n",
    "print(\"lin推定\", pred_lin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm\n",
    "#### 参考サイト\n",
    "\n",
    "Mercari Price Challenge -機械学習を使ったメルカリの価格予測 Ridge回帰 LightGBM\n",
    "\n",
    "http://rautaku.hatenablog.com/entry/2017/12/22/195649\n",
    "\n",
    "#### 結果：RMSEが0には程遠い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.6/site-packages (2.3.1)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from lightgbm) (0.19.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from lightgbm) (0.19.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from lightgbm) (1.13.3)\r\n"
     ]
    }
   ],
   "source": [
    "#必要なツールをインストール(初回のみ実行)\n",
    "! pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 5000 rounds\n",
      "[500]\tvalid_0's rmse: 55.8736\n",
      "[1000]\tvalid_0's rmse: 56.1001\n",
      "[1500]\tvalid_0's rmse: 56.1358\n",
      "[2000]\tvalid_0's rmse: 56.1429\n",
      "[2500]\tvalid_0's rmse: 56.1444\n",
      "[3000]\tvalid_0's rmse: 56.1446\n",
      "[3500]\tvalid_0's rmse: 56.1447\n",
      "[4000]\tvalid_0's rmse: 56.1447\n",
      "[4500]\tvalid_0's rmse: 56.1447\n",
      "[5000]\tvalid_0's rmse: 56.1447\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's rmse: 46.8401\n",
      "RMSE（０に近いほど良い） 46.8400537327\n"
     ]
    }
   ],
   "source": [
    "#LightGBM を使った回帰予測(D2Vベクトル)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    #D2vベクトル(ポジのみ)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_p, train_size=0.7, test_size=0.3, random_state=0)\n",
    "\n",
    "    # データセットを生成する\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "    # LightGBM のハイパーパラメータ\n",
    "    lgbm_params = {\n",
    "        # 回帰問題\n",
    "        'objective': 'regression',\n",
    "        # RMSE (平均二乗誤差平方根) の最小化を目指す\n",
    "        'metric': 'rmse',\n",
    "    }\n",
    "\n",
    "    # 上記のパラメータでモデルを学習する\n",
    "    model = lgb.train(lgbm_params, lgb_train, \n",
    "                      valid_sets=lgb_eval, num_boost_round=8000, \n",
    "                      early_stopping_rounds=5000, verbose_eval=500)\n",
    "\n",
    "    # テストデータを予測する\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "    # RMSE を計算する\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"RMSE（０に近いほど良い）\", rmse)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 5000 rounds\n",
      "[500]\tvalid_0's rmse: 51.6926\n",
      "[1000]\tvalid_0's rmse: 51.8494\n",
      "[1500]\tvalid_0's rmse: 51.8677\n",
      "[2000]\tvalid_0's rmse: 51.8695\n",
      "[2500]\tvalid_0's rmse: 51.8698\n",
      "[3000]\tvalid_0's rmse: 51.8698\n",
      "[3500]\tvalid_0's rmse: 51.8698\n",
      "[4000]\tvalid_0's rmse: 51.8698\n",
      "[4500]\tvalid_0's rmse: 51.8698\n",
      "[5000]\tvalid_0's rmse: 51.8698\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's rmse: 46.7848\n",
      "RMSE（０に近いほど良い） 46.7848457582\n"
     ]
    }
   ],
   "source": [
    "#LightGBM を使った回帰予測（Tfーidfベクトル）\n",
    "\n",
    "def main():\n",
    "\n",
    "    #X_tf tf-idfベクトルを使った予測(ポジのみ)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_tf_idf, y_p, train_size=0.7, test_size=0.3, random_state=0)\n",
    "\n",
    "    # データセットを生成する\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "    # LightGBM のハイパーパラメータ\n",
    "    lgbm_params = {\n",
    "        # 回帰問題\n",
    "        'objective': 'regression',\n",
    "        # RMSE (平均二乗誤差平方根) の最小化を目指す\n",
    "        'metric': 'rmse',\n",
    "    }\n",
    "    \n",
    "    # 上記のパラメータでモデルを学習する\n",
    "    model = lgb.train(lgbm_params, lgb_train, \n",
    "                      valid_sets=lgb_eval, num_boost_round=8000, \n",
    "                      early_stopping_rounds=5000, verbose_eval=500)\n",
    "#     model = lgb.LGBMRegressor()\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "    # テストデータを予測する\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # RMSE を計算する\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"RMSE（０に近いほど良い）\",rmse)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ランダムフォレスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2(1に近いほど良い）: -0.1350692768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#D2vベクトル\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.7, test_size=0.3, random_state=0)\n",
    "# ランダムフォレスト回帰オブジェクト生成\n",
    "rfr = RandomForestRegressor(n_estimators=100)\n",
    "# 学習の実行\n",
    "rfr.fit(X_train, y_train)\n",
    "# テストデータで予測実行\n",
    "predict_y = rfr.predict(X_test)\n",
    "# R2決定係数で評価\n",
    "r2_score = r2_score(y_test, predict_y)\n",
    "print(\"R^2(1に近いほど良い）:\", r2_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ２）ツイッターAPI制限への挑戦（データセットの拡大）\n",
    "　・古いツイートを大量取得できるパッケージを発見（通常は１週間程度しか遡れない）<br>\n",
    "#### 結果：取得データから反応ツイートの取得を試みたができなかった<br>\n",
    "\n",
    "### GetOldTweets3 0.0.11\n",
    "古いツイートをトークン申請なしで大量取得できるパッケージ<br>\n",
    "https://pypi.org/project/GetOldTweets3/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#必要なツールをインストール(初回のみ実行)\n",
    "! pip install GetOldTweets3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#指定日のトップツイートを取得、'./output/toptweets.csv'に保存\n",
    "! GetOldTweets3 --lang ja  --toptweets  --querysearch \"\" --since 2019-2-10 --until 2019-2-11 --output './output/toptweets.csv'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
