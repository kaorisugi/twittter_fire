{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mojimoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-77be508b0011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCaboCha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmojimoji\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mojimoji'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# MeCab: Yet Another Part-of-Speech and Morphological Analyzer\n",
    "# -------------------------------------------------------------\n",
    "# - http://taku910.github.io/mecab/\n",
    "# - https://github.com/taku910/mecab\n",
    "# exec following command when you check pos dic\n",
    "# $ cat ./venv/lib/mecab/lib/mecab/dic/ipadic/pos-id.def | nkf -w\n",
    "\n",
    "# Cabocha: Yet Another Japanese Dependency Structure Analyzer\n",
    "# -------------------------------------------------------------\n",
    "# - https://taku910.github.io/cabocha/\n",
    "\n",
    "import MeCab\n",
    "import CaboCha\n",
    "import mojimoji, os, sys\n",
    "from collections import namedtuple\n",
    "\n",
    "app_home = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))\n",
    "sys.path.insert(0, os.path.join(app_home, \"pymongo\"))\n",
    "sys.path.append(os.path.join(app_home, \"lib\"))\n",
    "from database import load_politly_dic\n",
    "\n",
    "# import sys, argparse\n",
    "# sys.path.insert(0, TOP_DIR)\n",
    "# from aramaki.database import get_politely_score, get_stopwords, bulk_search_for_politely_dict\n",
    "\n",
    "class JpParser:\n",
    "    \"\"\"\n",
    "    return parsed data with Mecab/Cabocha\n",
    "    japanese have tree types of structure in a sentence. \n",
    "    - sentence > chunk > token\n",
    "    - token is a tagged word\n",
    "    ex)  'デザインが気に入り購入しました。'\n",
    "        chunk1: 'デザイン / が '\n",
    "         -> head_token: デザイン\n",
    "        chunk2:' 気に入り'\n",
    "         -> head_token: 気に入り\n",
    "        chunk3: '購入 / し / まし / た /。'\n",
    "         -> head_token: し\n",
    "    \"\"\"\n",
    "    # exec following command when you check pos dic\n",
    "    # $ cat ./venv/lib/mecab/lib/mecab/dic/ipadic/pos-id.def | nkf -w\n",
    "    POS_DIC = {\n",
    "        'BOS/EOS': 'EOS', # end of sentense\n",
    "        '形容詞' : 'ADJ',\n",
    "        '連体詞' : 'JADJ', # Japanese-specific POS like a adjective\n",
    "        '副詞'   : 'ADV',\n",
    "        '名詞'   : 'NOUN',\n",
    "        '動詞'   : 'VERB',\n",
    "        '助動詞' : 'AUX',\n",
    "        '助詞'   : 'PART',\n",
    "        '感動詞' : 'INTJ',\n",
    "        '接続詞' : 'CONJ',\n",
    "        '記号'   : 'SYM', # symbol\n",
    "        '*'      : 'X',\n",
    "        'その他' : 'X',\n",
    "        'フィラー': 'X',\n",
    "        '接頭詞' : 'X',\n",
    "    }\n",
    "    POS_2ND_DIC = {\n",
    "        '代名詞':'PRON',\n",
    "    }\n",
    "\n",
    "    # ex) sys_dic_path='/usr/local/cai_venv/lib/mecab/lib/mecab/dic/ipadic-neologd/'):\n",
    "    # https://github.com/neologd/mecab-ipadic-neologd/\n",
    "    def __init__(self, *, sys_dic_path='', load_politely_dict=False):\n",
    "        opt_m = \"-Ochasen\"\n",
    "        opt_c = '-f4'\n",
    "        if sys_dic_path:\n",
    "            opt_m += ' -d {0}'.format(sys_dic_path)\n",
    "            opt_c += ' -d {0}'.format(sys_dic_path)\n",
    "        tagger = MeCab.Tagger(opt_m)\n",
    "        tagger.parse('') # for UnicodeDecodeError\n",
    "        self._tagger = tagger\n",
    "        self._parser = CaboCha.Parser(opt_c)\n",
    "        # load when you use sentimentanalysis\n",
    "        # dictionary size is big (20k)\n",
    "        if load_politely_dict:\n",
    "            self.pol_dic = load_politly_dic(\"politely_JP\")\n",
    "\n",
    "    def search_politely_dict(self, words):\n",
    "        politely_dict = dict()\n",
    "        default_score = 0  # return score when not found in plitely dict\n",
    "        for w in words:\n",
    "            res = self.pol_dic[self.pol_dic['headword']==w]\n",
    "            if len(res.index) > 0:\n",
    "                politely_dict.update({res['headword'].values[0]: res['score'].values[0]})\n",
    "            else:\n",
    "                politely_dict.update({w:default_score})\n",
    "        return politely_dict\n",
    "\n",
    "    def get_sentences(self, text):\n",
    "        \"\"\" \n",
    "        input: text have many sentences\n",
    "        output: ary of sentences ['sent1', 'sent2', ...]\n",
    "        \"\"\"\n",
    "        EOS_DIC = ['。', '．', '！','？','!?', '!', '?' ]\n",
    "        sentences = list()\n",
    "        sent = ''\n",
    "        # split in first when text include '\\n'\n",
    "        temp = text.split('\\\\n')\n",
    "        for each_text in temp:\n",
    "            if each_text == '':\n",
    "                continue\n",
    "            for token in self.tokenize(each_text):\n",
    "                # print(token.pos_jp, token.pos, token.surface, sent)\n",
    "                # TODO: this is simple way. ex)「今日は雨ね。」と母がいった\n",
    "                sent += token.surface\n",
    "                if token.surface in EOS_DIC and sent != '':\n",
    "                    sentences.append(sent)\n",
    "                    sent = ''\n",
    "            if sent != '':\n",
    "                sentences.append(sent)\n",
    "        return sentences\n",
    "\n",
    "    def normalize(self, src_text):\n",
    "        # Zenkaku to Hankaku ( handling japaneze character )\n",
    "        normalized = mojimoji.han_to_zen(src_text, digit=False, ascii=False)\n",
    "        normalized = mojimoji.zen_to_han(normalized, kana=False)\n",
    "        return normalized.lower()\n",
    "\n",
    "    def tokenize(self, sent):\n",
    "        node = self._tagger.parseToNode( sent )\n",
    "        tokens = list()\n",
    "        idx = 0\n",
    "        while node:\n",
    "            feature = node.feature.split(',')\n",
    "            token = namedtuple('Token', 'idx, surface, pos, pos_detail1, pos_detail2, pos_detail3,\\\n",
    "                                infl_type, infl_form, base_form, reading, phonetic')\n",
    "            token.idx         = idx\n",
    "            token.surface     = node.surface  # 表層形\n",
    "            token.pos_jp      = feature[0]    # 品詞\n",
    "            token.pos_detail1 = feature[1]    # 品詞細分類1\n",
    "            token.pos_detail2 = feature[2]    # 品詞細分類2\n",
    "            token.pos_detail3 = feature[3]    # 品詞細分類3\n",
    "            token.infl_type   = feature[4]    # 活用型\n",
    "            token.infl_form   = feature[5]    # 活用形\n",
    "            token.base_form   = feature[6] if feature[6]!='*' else node.surface # 原型 ex)MacMini's base_form=='*'\n",
    "            token.pos         = self.POS_DIC.get( feature[0], 'X' )     # 品詞\n",
    "            token.reading     = feature[7] if len(feature) > 7 else ''  # 読み\n",
    "            token.phonetic    = feature[8] if len(feature) > 8 else ''  # 発音\n",
    "            # for BOS/EOS\n",
    "            if token.pos != 'EOS':\n",
    "                tokens.append(token)\n",
    "                idx += 1\n",
    "            node = node.next\n",
    "        return tokens\n",
    "\n",
    "    def tokenize_filtered_by_pos(self, sent, pos=['NOUN',]):\n",
    "        tokens = [token for token in self.tokenize(sent) if token.pos in pos]\n",
    "        return tokens\n",
    "\n",
    "    def extract_words(self, text, *, filter_pos=[]):\n",
    "        if len(filter_pos)!=0:\n",
    "            return [ w.surface for w in self.tokenize(text) if not w.pos in filter_pos]\n",
    "        else:\n",
    "            return [ w.surface for w in self.tokenize(text)]\n",
    "\n",
    "    def get_chunk_data(self, sentence):\n",
    "        tree = self._parser.parse(sentence)\n",
    "        tokens = self.tokenize(sentence)\n",
    "        chunk_data = list()\n",
    "        for i in range(0, tree.chunk_size()):\n",
    "            chunk = namedtuple('Chunk', 'tokens, head_token, chunk_idx, depend_idx, src_idx, head_idx, func_idx,\\\n",
    "                                token_size, token_idx, feature_size, score, additional_info')\n",
    "            c = tree.chunk(i)\n",
    "            c_tokens = list()\n",
    "            for j in range(c.token_pos, c.token_pos+c.token_size):\n",
    "                c_tokens.append(tokens[j])\n",
    "            chunk.tokens       = c_tokens\n",
    "            chunk.head_token   = c_tokens[c.head_pos] # 主辞のtoken\n",
    "            chunk.chunk_idx    = i                    # chunk_index\n",
    "            chunk.depend_idx   = c.link               # dependecy chunk index\n",
    "            chunk.src_idx      = list()               # source chunk (recieve chunk) index\n",
    "            chunk.head_idx     = c.head_pos           # 主辞のindex\n",
    "            chunk.func_idx     = c.func_pos           # 機能語のindex\n",
    "            chunk.token_size   = c.token_size\n",
    "            chunk.token_idx    = c.token_pos          # chunk先頭tokenのindex\n",
    "            chunk.feature_size = c.feature_list_size\n",
    "            chunk.score        = c.score\n",
    "            chunk.additional_info = c.additional_info\n",
    "            chunk_data.append(chunk)\n",
    "        for i in range(0, tree.chunk_size()):\n",
    "            c = tree.chunk(i)\n",
    "            if c.link != -1:\n",
    "                chunk_data[c.link].src_idx.append(i)\n",
    "        return chunk_data\n",
    "\n",
    "    def get_child_tokens(self, chunks, chunk, token):\n",
    "        child_tokens = list()\n",
    "        # a non head_token have no child (return empty list)\n",
    "        if token.idx == chunk.head_token.idx:\n",
    "            child_tokens.extend([t for t in chunk.tokens if t.idx!=token.idx])\n",
    "            if len(chunk.src_idx)!=0:\n",
    "                child_tokens.extend([chunks[si].head_token for si in chunk.src_idx])\n",
    "        return child_tokens\n",
    "\n",
    "    # just call aramaki.databases\n",
    "    def get_stopwords(self):\n",
    "        return get_stopwords(suffix='_JP')\n",
    "\n",
    "    def debug(self, sentence):\n",
    "        tree = self._parser.parse(sentence)\n",
    "        # 0: tree format\n",
    "        print(tree.toString(0))\n",
    "        # 4: CONLL format\n",
    "        print(tree.toString(4))\n",
    "\n",
    "    # return senti_word_list\n",
    "    def senti_tokenize(self, sentence):\n",
    "        senti_tokens = {'pos':[], 'nue':[], 'neg':[]}\n",
    "        words = [s.base_form.lower()\n",
    "                    for s in self.tokenize(sentence)\n",
    "                    if s.base_form!='*' and s.pos!='SYM']\n",
    "        politely_dict = self.search_politely_dict(words)\n",
    "        for w,score in politely_dict.items():\n",
    "            key = 'pos' if int(score) > 0 else 'nue' if int(score) == 0 else 'neg'\n",
    "            senti_tokens[key].append(w)\n",
    "        return senti_tokens\n",
    "\n",
    "    def senti_analisys(self, sentence):\n",
    "        \"\"\"\n",
    "        output: sentiment score (1:positive < score < -1:negative)\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        num_all_words = 0\n",
    "        tokens = self.tokenize(sentence)\n",
    "        words = list()\n",
    "        words.extend([s.base_form.lower() for s in tokens])\n",
    "        politely_dict = self.search_politely_dict(words)\n",
    "        # politely_dict = bulk_search_for_politely_dict(words, suffix='_JP')\n",
    "        scores = list()\n",
    "        scores.extend([politely_dict[w] for w in words])\n",
    "        for i in range(0, len(tokens)):\n",
    "            s = tokens[i]\n",
    "            scores = self.apply_muliwords_rule_for_senti_analisys(i, tokens, scores)\n",
    "            scores = self.apply_politely_reverse_rule_for_senti_analisys(i, tokens, scores, sentence)\n",
    "        # evaluate score\n",
    "        # -------------------------------------------------------------------\n",
    "        for sc in scores:\n",
    "            score += sc\n",
    "            num_all_words += 1\n",
    "        return round(score/num_all_words, 2)\n",
    "\n",
    "    def apply_politely_reverse_rule_for_senti_analisys(self, i, tokens, scores, sentence):\n",
    "        # ref) http://must.c.u-tokyo.ac.jp/nlpann/pdf/nlp2013/C6-01.pdf\n",
    "        reverse_multiwords = [\n",
    "            # headword,N-gram,apply_type\n",
    "            ['の で は ない',       3, 'own'],\n",
    "            ['わけ で は ない',     3, 'own'],\n",
    "            ['わけ に は いく ない',4, 'src'],\n",
    "        ]\n",
    "        reverse_words = [\n",
    "            # headword,pos,apply_type\n",
    "            ['ない', 'AUX', 'own'],\n",
    "            ['ぬ',   'AUX', 'own'],\n",
    "            ['ない', 'ADJ', 'own'],\n",
    "        ]\n",
    "        apply_type = ''\n",
    "        # detect politely-reverse word ( like a 'not' )\n",
    "        # -------------------------------------------------------------------\n",
    "        for r in reverse_words:\n",
    "            if tokens[i].base_form==r[0] and tokens[i].pos==r[1]:\n",
    "                apply_type = r[2]\n",
    "        for r in reverse_multiwords:\n",
    "            if i >= r[1]:\n",
    "                multi_words = [ x.base_form.lower() for x in tokens if i-r[1] <= x.idx <= i]\n",
    "                if ' '.join(multi_words) == r[0]:\n",
    "                    apply_type = r[2]\n",
    "        # apply for score\n",
    "        # -------------------------------------------------------------------\n",
    "        if apply_type!='':\n",
    "            chunk = self.get_chunk_data(sentence)\n",
    "            for j in range(0,len(chunk)):\n",
    "                c = chunk[j]\n",
    "                if c.token_idx <= i <= c.token_idx+c.token_size-1:\n",
    "                    if apply_type=='own':\n",
    "                        start_idx, end_idx = c.token_idx, (c.token_idx+c.token_size)\n",
    "                    elif apply_type=='src':\n",
    "                        sc = chunk[c.src_idx[-1]]\n",
    "                        start_idx, end_idx = sc.token_idx, (sc.token_idx+sc.token_size)\n",
    "                    # elif apply_type=='depend':\n",
    "                    max_score_of_reverse = max(scores[start_idx:end_idx])\n",
    "                    # print('max:', max_score_of_reverse, scores[start_idx:end_idx] )\n",
    "                    del scores[start_idx:end_idx]\n",
    "                    scores.append(-1*int(max_score_of_reverse))\n",
    "                    scores.extend([0 for i in range(end_idx-start_idx-1)])\n",
    "                    break\n",
    "        return scores\n",
    "\n",
    "    def apply_muliwords_rule_for_senti_analisys(self, i, tokens, scores):\n",
    "        bigram_score, trigram_score = 0,0\n",
    "        if i >= 1:  # bigram\n",
    "            headword = ' '.join([tokens[i-1].base_form.lower(), tokens[i].base_form.lower()])\n",
    "            res = self.search_politely_dict([headword])\n",
    "            bigram_score = res[headword]\n",
    "        if i >= 2:  # triram\n",
    "            headword = ' '.join([tokens[i-2].base_form.lower(), tokens[i-1].base_form.lower(), tokens[i].base_form.lower()])\n",
    "            res = self.search_politely_dict([headword])\n",
    "            trigram_score = res[headword]\n",
    "        # apply for scores\n",
    "        if trigram_score != 0:\n",
    "            del scores[i-3:i]\n",
    "            scores.extend([trigram_score, 0, 0])\n",
    "        elif bigram_score != 0:\n",
    "            del scores[i-2:i]\n",
    "            scores.extend([bigram_score, 0])\n",
    "        return scores\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"INPUT\", help=\"Specify input sentences\",type=str,\n",
    "    #                     nargs='?',default='',const='')\n",
    "    # args = parser.parse_args()\n",
    "    inp =  input('input japanese text(default text=\"d\")>>')\n",
    "\n",
    "    if inp=='d':\n",
    "        # Japanese famous poem written by Soseki natusme.\n",
    "        input_sentences = '我輩は猫である。名前はまだ無い。どこで生れたかとんと見当けんとうがつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。'\n",
    "        input_sentences = 'チョコが甘くて好きです。２種類で５袋となっていて、残念でした。好みが合わなかったものは無駄になってしまいます。こんなに壊れやすいチョコは買うわけにはいかない'\n",
    "    else:\n",
    "        input_sentences = inp\n",
    "    \n",
    "    jp = JpParser(load_politely_dict=True)\n",
    "\n",
    "    # print('ｕｓｂｱｲｳ')\n",
    "    # print('normalized_str:', jp.normalize('ｕｓｂｱｲｳ'))\n",
    "\n",
    "    sentences = jp.get_sentences(input_sentences)\n",
    "    # print(jp.get_jp_stopwords())\n",
    "    # print(jp.POS_DIC)\n",
    "    # print(sentences)\n",
    "\n",
    "    for  sent in sentences:\n",
    "        print('BEGIN: '+sent+': --------------------------------------------------------------------------------------')\n",
    "        # print(jp.extract_words(sent))\n",
    "        # print(jp.extract_words(sent, filter_pos=['SYM']))\n",
    "        # token --------------------------------------\n",
    "        # sent_data = jp.tokenize(sent)\n",
    "        # for s in sent_data:\n",
    "        #   print(s.surface, s.base_form, s.pos)\n",
    "\n",
    "        # chunk --------------------------------------\n",
    "        # jp.debug(sent)\n",
    "        # chunk_data = jp.get_chunk_data(sent)\n",
    "        # for c in chunk_data:\n",
    "        #     print(c.depend_idx, len(c.tokens), \n",
    "        #             'dst='+chunk_data[c.depend_idx].head_token.surface, \n",
    "        #             'src='+c.tokens[0].surface )\n",
    "        #\n",
    "        #     # extract feature with dependence\n",
    "        #     for t in c.tokens:\n",
    "        #         if t.base_form == '我輩':\n",
    "        #             feature = chunk_data[c.depend_idx].head_token.base_form\n",
    "        #             print('我輩dep=' + feature)\n",
    "        print('senti_tokenize: ', jp.senti_tokenize(sent))\n",
    "        print('senti_score: ', jp.senti_analisys(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
