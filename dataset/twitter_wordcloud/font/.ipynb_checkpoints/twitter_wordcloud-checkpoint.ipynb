{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ツイートデータの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API remain: 799\n",
      "API remain: 798\n",
      "API remain: 797\n",
      "API remain: 796\n",
      "API remain: 795\n",
      "API remain: 794\n",
      "API remain: 793\n",
      "API remain: 792\n",
      "API remain: 791\n",
      "API remain: 790\n",
      "API remain: 789\n",
      "API remain: 788\n",
      "API remain: 787\n",
      "API remain: 786\n",
      "API remain: 785\n",
      "API remain: 784\n",
      "API remain: 783\n",
      "API remain: 782\n",
      "API remain: 781\n",
      "API remain: 780\n",
      "API remain: 779\n",
      "API remain: 778\n",
      "API remain: 777\n",
      "API remain: 776\n",
      "API remain: 775\n",
      "API remain: 774\n",
      "API remain: 773\n",
      "API remain: 772\n",
      "API remain: 771\n",
      "API remain: 770\n",
      "API remain: 769\n",
      "API remain: 768\n",
      "API remain: 767\n",
      "API remain: 766\n",
      "API remain: 765\n",
      "API remain: 764\n",
      "API remain: 763\n",
      "API remain: 762\n",
      "API remain: 761\n",
      "API remain: 760\n",
      "API remain: 759\n",
      "API remain: 758\n",
      "API remain: 757\n",
      "API remain: 756\n",
      "API remain: 755\n",
      "API remain: 754\n",
      "API remain: 753\n",
      "API remain: 752\n",
      "API remain: 751\n",
      "API remain: 750\n",
      "API remain: 749\n",
      "API remain: 748\n",
      "API remain: 747\n",
      "API remain: 746\n",
      "API remain: 745\n",
      "API remain: 744\n",
      "API remain: 743\n",
      "API remain: 742\n",
      "API remain: 741\n",
      "API remain: 740\n",
      "API remain: 739\n",
      "API remain: 738\n",
      "API remain: 737\n",
      "API remain: 736\n",
      "API remain: 735\n",
      "API remain: 734\n",
      "API remain: 733\n",
      "API remain: 732\n",
      "API remain: 731\n",
      "API remain: 730\n",
      "API remain: 729\n",
      "API remain: 728\n",
      "API remain: 727\n",
      "API remain: 726\n",
      "API remain: 725\n",
      "API remain: 724\n",
      "API remain: 723\n",
      "API remain: 722\n",
      "API remain: 721\n",
      "API remain: 720\n",
      "API remain: 719\n",
      "API remain: 718\n",
      "API remain: 717\n",
      "API remain: 716\n",
      "API remain: 715\n",
      "API remain: 714\n",
      "API remain: 713\n",
      "API remain: 712\n",
      "API remain: 711\n",
      "API remain: 710\n",
      "API remain: 709\n",
      "API remain: 708\n",
      "API remain: 707\n",
      "API remain: 706\n",
      "API remain: 705\n",
      "API remain: 704\n",
      "API remain: 703\n",
      "API remain: 702\n",
      "API remain: 701\n",
      "API remain: 700\n"
     ]
    }
   ],
   "source": [
    "# Python 用の OAuth 認証ライブラリ\n",
    "#pip install requests-oauthlib\n",
    "\n",
    "# Pythonの形態素解析エンジン\n",
    "#pip install janome\n",
    "\n",
    "# Word Cloudのライブラリ\n",
    "#pip install wordcloud\n",
    "\n",
    "#  絵文字を扱うためのライブラリ\n",
    "#pip install emoji --upgrade\n",
    "\n",
    "import json\n",
    "import config\n",
    "from requests_oauthlib import OAuth1Session\n",
    "from time import sleep\n",
    "import emoji\n",
    "\n",
    "# 絵文字を除去する\n",
    "def remove_emoji(src_str):\n",
    "    return ''.join(c for c in src_str if c not in emoji.UNICODE_EMOJI)\n",
    "\n",
    "# APIキー設定(別ファイルのconfig.pyで定義しています)\n",
    "CK = config.CONSUMER_KEY\n",
    "CS = config.CONSUMER_SECRET\n",
    "AT = config.ACCESS_TOKEN\n",
    "ATS = config.ACCESS_TOKEN_SECRET\n",
    "\n",
    "# 認証処理\n",
    "twitter = OAuth1Session(CK, CS, AT, ATS)  \n",
    "\n",
    "# タイムライン取得エンドポイント\n",
    "url = \"https://api.twitter.com/1.1/statuses/user_timeline.json\"  \n",
    "\n",
    "# パラメータの定義\n",
    "params = {'screen_name': 'actor_satojiro',\n",
    "          'exclude_replies': True,\n",
    "          'include_rts': False,\n",
    "          'count': 200}\n",
    "\n",
    "# 出力先ファイル\n",
    "f_out = open('./output/tweet_data', 'w')\n",
    "\n",
    "for j in range(100):\n",
    "    res = twitter.get(url, params=params)\n",
    "    if res.status_code == 200:\n",
    "        # API残り回数\n",
    "        limit = res.headers['x-rate-limit-remaining']\n",
    "        print(\"API remain: \" + limit)\n",
    "        if limit == 1:\n",
    "            sleep(60*15)\n",
    "\n",
    "        n = 0\n",
    "        timeline = json.loads(res.text)\n",
    "        # 各ツイートの本文を表示\n",
    "        for i in range(len(timeline)):\n",
    "            if i != len(timeline)-1:\n",
    "                # 絵文字があると、wordcloudが使用できないため、削除する\n",
    "                f_out.write(remove_emoji(timeline[i]['text']) + '\\n')\n",
    "            else:\n",
    "                # 絵文字があると、wordcloudが使用できないため、削除する\n",
    "                f_out.write(remove_emoji(timeline[i]['text']) + '\\n')\n",
    "                # 一番最後のツイートIDをパラメータmax_idに追加\n",
    "                params['max_id'] = timeline[i]['id']-1\n",
    "\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x11bcee940>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#　名詞だけ抽出してワードクラウド表示\n",
    "\n",
    "# coding:utf-8\n",
    "import csv\n",
    "from janome.tokenizer import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# 名詞だけ抽出、単語をカウント\n",
    "def counter(texts):\n",
    "    t = Tokenizer()\n",
    "    words_count = defaultdict(int)\n",
    "    words = []\n",
    "    for text in texts:\n",
    "        tokens = t.tokenize(text)\n",
    "        for token in tokens:\n",
    "            # 品詞から名詞だけ抽出\n",
    "            pos = token.part_of_speech.split(',')[0]\n",
    "            if pos in ['名詞']:\n",
    "                # 必要ない単語を省く(実際の結果を見てから不必要そうな単語を記載しました)\n",
    "                if token.base_form not in [\"こと\", \"よう\", \"そう\", \"これ\", \"それ\"]:\n",
    "                    words_count[token.base_form] += 1\n",
    "                    words.append(token.base_form)\n",
    "    return words_count, words\n",
    "\n",
    "with open('./output/tweet_data', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    texts = []\n",
    "    for row in reader:\n",
    "        if(len(row) > 0):\n",
    "            text = row[0].split('http')\n",
    "            texts.append(text[0])\n",
    "\n",
    "words_count, words = counter(texts)\n",
    "text = ' '.join(words)\n",
    "\n",
    "# fontは自分の端末にあるものを使用する\n",
    "fpath = \"./font/NotoSansCJKjp-Regular.otf\"\n",
    "wordcloud = WordCloud(background_color=\"white\",\n",
    "                      font_path=fpath, width=900, height=500).generate(text)\n",
    "\n",
    "wordcloud.to_file(\"./output/wordcloud_sample.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
